{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyN0Q7oDHoKUvBtfynUFNzI3","include_colab_link":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12620520,"sourceType":"datasetVersion","datasetId":7973641},{"sourceId":12681452,"sourceType":"datasetVersion","datasetId":8014077},{"sourceId":12688949,"sourceType":"datasetVersion","datasetId":8018708},{"sourceId":12689371,"sourceType":"datasetVersion","datasetId":8019025},{"sourceId":12689384,"sourceType":"datasetVersion","datasetId":8019036}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/shrav-jally/ML_lab_sem5/blob/main/sem5_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"code","source":"#tinyurl.com/ML-2025-26\n\n''' THIS SEM\nnumpy\npandas\nscikitlearn\nmatplotlib\ntensorflow\nkeras\n'''","metadata":{"id":"2kKoBdcTeRYz","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:24:32.844655Z","iopub.execute_input":"2025-07-30T10:24:32.844954Z","iopub.status.idle":"2025-07-30T10:24:32.850618Z","shell.execute_reply.started":"2025-07-30T10:24:32.844931Z","shell.execute_reply":"2025-07-30T10:24:32.849632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\ncreating dataframe\ninserting, deleting, modifying data\nbasic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\nmissing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\nworking with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\ndata visualization with pandas - line, bar, scatter, histograms\nadvaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\nperformance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\n'''","metadata":{"id":"UlspcwvGgQfd","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:24:35.643919Z","iopub.execute_input":"2025-07-30T10:24:35.644538Z","iopub.status.idle":"2025-07-30T10:24:35.649907Z","shell.execute_reply.started":"2025-07-30T10:24:35.644513Z","shell.execute_reply":"2025-07-30T10:24:35.649138Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#working with dataframes\n#https://colab.research.google.com/gist/shrav-jally/640d061d3ffa5b830ee2e4ecb291fb2a/sem2oops.ipynb#scrollTo=IMtntOpz8hqo\n#this is the link to oops, numpy, pandas, matplotlib etc. from sem2\n\n'''\ndf = pd.read_csv('your_dataset.csv') #csv (comma separated vectors)\nprint(df.head())\nprint(df.describe()) #summary of statistics\nprint(df.info())\ndf.sample() #gives random 5 rows of dataframe\ndf.columns\ndf[['Age', 'BMI']].head() #selecting multiple columns (passing array of feature names)\ndf[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\ndf.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\n\n#handling missing values\nprint(df.isnull().sum())  #check for missing values\ndf_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\ndf_filled=df.fillna(df.mean())\n#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\n\ndf.fillna(df.mean(), inplace=True)\nprint(df.duplicated().sum()) #identifies duplicate values\ndf_no_duplicates = df.drop_duplicates()\ndf[\"column1\"]=df[\"column1\"].astype(float)\n\n#encode categorical variables\ndf_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\n#dummy value is assigned to label name\n#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\n'''","metadata":{"id":"FJzt_9akjels","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:25:08.789549Z","iopub.execute_input":"2025-07-30T10:25:08.789839Z","iopub.status.idle":"2025-07-30T10:25:08.796217Z","shell.execute_reply.started":"2025-07-30T10:25:08.789818Z","shell.execute_reply":"2025-07-30T10:25:08.795486Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Scikit_learn (estimator api) - pip install scikitlearn\n#uci ml repository - all types of machine learning datasets available\n#it provides various tools for - classification, regression, clustering, dimensionality reduction, feature selection\n#sklearn is used to build ml models, for data reading and manipulation and summarizing use pandas, numpy etc.\n\nimport pandas as pd\nfrom sklearn import datasets\n\niris = datasets.load_iris()\ndf = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n\nprint(df.shape)\nprint(df.head())\ndf['target'] = iris.target\nprint(df.head())","metadata":{"id":"7ZVAkw8enU2N","outputId":"6c5abe3a-2758-4f39-9f1a-7e759d003660","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T15:20:21.472604Z","iopub.execute_input":"2025-08-06T15:20:21.472951Z","iopub.status.idle":"2025-08-06T15:20:22.009414Z","shell.execute_reply.started":"2025-08-06T15:20:21.472925Z","shell.execute_reply":"2025-08-06T15:20:22.008345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from re import X\nfrom inspect import isroutine\n'''\ncomponents of sklearn:\n\nsupervised learning algorithms: SVM, DT\ncross validation: to check accuracy of supervised models on unseen data using sklearn\nunsupervised learning algorithms: clustering, factor analysis, principal component analysis to unsupervised neural networks\nvarious datasets: IRIS\nfeature extraction: sklearn for extracting images and text features\nmodel selection: comparing, validating parameters and models\n'''\n\n#most common steps involved in machine learning\n\n# Import necessary libraries\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load dataset\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Create and train the model\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\n\n# Make predictions\ny_pred = clf.predict(X_test)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy * 100:.2f}%')","metadata":{"id":"IgI5rXZCqKWE","outputId":"102812bf-577f-40a9-9415-d2346083d875","trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:21:42.945794Z","iopub.execute_input":"2025-08-06T16:21:42.946020Z","iopub.status.idle":"2025-08-06T16:21:45.325995Z","shell.execute_reply.started":"2025-08-06T16:21:42.946000Z","shell.execute_reply":"2025-08-06T16:21:45.324952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''\n1. **Series vs 1-D array, List, Dictionary:**\n   - **Series:** A 1-D labeled array in pandas with index labels for each element.\n   - **1-D Array:** Homogeneous, no labels (NumPy array).\n   - **List:** A Python built-in collection, ordered, but no indexing.\n   - **Dictionary:** Key-value pair mapping, unordered, no numerical operations like Series.\n\n2. **DataFrame vs 2-D array:**\n   - **DataFrame:** 2-D labeled table with heterogeneous data types (columns can have different types).\n   - **2-D Array:** Homogeneous (same data type), no labels, and lacks advanced functionality like a DataFrame.\n\n3. **DataFrames and Series:**\n   - A **DataFrame** is a collection of **Series** (each column in a DataFrame is a Series).\n\n4. **Size of (i) Series, (ii) DataFrame:**\n   - **Series Size:** Number of elements (length of the Series).\n   - **DataFrame Size:** Total elements (rows Ã— columns).\n'''\n\n'''\n# 5. Create the following Series and do the specified operations:\n\n# a) EngAlph with 26 alphabet elements and default indices\nimport pandas as pd\nimport string\n\nEngAlph = pd.Series(list(string.ascii_uppercase))\nprint(EngAlph)\n\n# b) Vowels with specific index labels and all values set to zero\nVowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\nprint(Vowels.empty)  # Check if it is empty\n\n# c) Friends from a dictionary\nFriends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\nprint(Friends)\n\n# d) MTseries as an empty series\nMTseries = pd.Series()\nprint(MTseries.empty)  # Check if it is empty\n\n# e) MonthDays from a numpy array\nimport numpy as np\nMonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\nprint(MonthDays)\n\n# 6. Using the Series created in Question 5:\n\n# a) Set all values of Vowels to 10\nVowels[:] = 10\nprint(Vowels)\n\n# b) Divide all values of Vowels by 2\nVowels /= 2\nprint(Vowels)\n\n# c) Create another series Vowels1\nVowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\nprint(Vowels1)\n\n# d) Add Vowels and Vowels1 and assign to Vowels3\nVowels3 = Vowels + Vowels1\nprint(Vowels3)\n\n# e) Subtract, multiply, and divide Vowels by Vowels1\nVowels_sub = Vowels - Vowels1\nVowels_mul = Vowels * Vowels1\nVowels_div = Vowels / Vowels1\nprint(Vowels_sub, Vowels_mul, Vowels_div)\n\n# f) Alter labels of Vowels1\nVowels1.index = ['A', 'E', 'I', 'O', 'U']\nprint(Vowels1)\n\n# 7. Using the Series created in Question 5:\n\n# a) Find dimensions, size, and values\nprint(EngAlph.shape, Vowels.size, Friends.values)\n\n# b) Rename MTseries as SeriesEmpty\nSeriesEmpty = MTseries\nprint(SeriesEmpty)\n\n# c) Name the index of MonthDays and Friends\nMonthDays.index.name = 'monthno'\nFriends.index.name = 'Fname'\n\n# d) Display the 3rd and 2nd value of Friends\nprint(Friends.iloc[2], Friends.iloc[1])\n\n# e) Display alphabets 'e' to 'p' from EngAlph\nprint(EngAlph['E':'P'])\n\n# f) Display the first 10 values of EngAlph\nprint(EngAlph.head(10))\n\n# g) Display the last 10 values of EngAlph\nprint(EngAlph.tail(10))\n\n# h) Display MTseries\nprint(MTseries)\n\n# 8. Using the Series created in Question 5:\n\n# a) Display months 3 through 7 from MonthDays\nprint(MonthDays[3:8])\n\n# b) Display MonthDays in reverse order\nprint(MonthDays[::-1])\n\n# 9. Create the following DataFrame Sales:\nSales = pd.DataFrame({\n    '2014': [100.5, 150.8, 200.9, 30000, 40000],\n    '2015': [12000, 18000, 22000, 30000, 45000],\n    '2016': [20000, 50000, 70000, 100000, 125000],\n    '2017': [50000, 60000, 70000, 80000, 90000]\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\nprint(Sales)\n\n# 10. Use the DataFrame Sales to do the following:\n\n# a) Display row labels\nprint(Sales.index)\n\n# b) Display column labels\nprint(Sales.columns)\n\n# c) Display data types of each column\nprint(Sales.dtypes)\n\n# d) Display dimensions, shape, size, and values\nprint(Sales.shape, Sales.size, Sales.values)\n\n# e) Display the last two rows\nprint(Sales.tail(2))\n\n# f) Display the first two columns\nprint(Sales.iloc[:, :2])\n\n# g) Create Sales2 DataFrame from dictionary\nSales2 = pd.DataFrame({\n    '2018': [160000, 110000, 500000, 340000, 900000]\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\nprint(Sales2)\n\n# h) Check if Sales2 is empty\nprint(Sales2.empty)\n\n# 11. Use the DataFrame Sales to do the following:\n\n# a) Append Sales2 to Sales\nSales = Sales.append(Sales2)\nprint(Sales)\n\n# b) Transpose the Sales DataFrame\nSales = Sales.T\nprint(Sales)\n\n# c) Display sales by all salespersons in 2017\nprint(Sales['2017'])\n\n# d) Display sales by Madhu and Ankit in 2017 and 2018\nprint(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\n\n# e) Display Shruti's sales in 2016\nprint(Sales.loc['Shruti', '2016'])\n\n# f) Add data for Sumeet\nSales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\nprint(Sales)\n\n# g) Delete 2014 data from Sales\nSales.drop('2014', axis=1, inplace=True)\nprint(Sales)\n\n# h) Delete data for Kinshuk\nSales.drop('Kinshuk', axis=0, inplace=True)\nprint(Sales)\n\n# i) Change salesperson names\nSales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\nprint(Sales)\n\n# j) Update sales made by Shailesh in 2018\nSales.loc['Shailesh', '2018'] = 100000\nprint(Sales)\n\n# k) Write Sales DataFrame to a CSV file\nSales.to_csv('SalesFigures.csv', header=False, index=False)\n\n# l) Read the CSV file and update row/column labels\nSalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\nSalesRetrieved.columns = Sales.columns\nSalesRetrieved.index = Sales.index\nprint(SalesRetrieved)\n'''","metadata":{"id":"4DlWrtN7w7N0","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:25:49.613970Z","iopub.execute_input":"2025-07-30T10:25:49.614569Z","iopub.status.idle":"2025-07-30T10:25:49.623678Z","shell.execute_reply.started":"2025-07-30T10:25:49.614542Z","shell.execute_reply":"2025-07-30T10:25:49.622801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Install pymysql\n# pip install pymysql\n\n# 2. Pivot vs Pivot_table\n# pivot():\n# Used for reshaping data where you specify the index, columns, and values.\n# It requires unique index/column combinations. If there are duplicates, it throws an error.\n\n# pivot_table():\n# Similar to pivot(), but it can handle duplicate entries. It allows you to specify an aggregation function (e.g., sum, mean) to aggregate data when duplicates exist.\n# Syntax: pivot_table(index=..., columns=..., values=..., aggfunc='sum')\n\n# 3. SQLAlchemy\n# SQLAlchemy is a Python library that provides an Object Relational Mapping (ORM) interface for interacting with databases.\n\n'''\n# 4. Sort DataFrame by multiple columns\nimport pandas as pd\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\ndf.sort_values(by=['col1', 'col2'], ascending=[True, False])\n\n# 5. Handling missing values\ndf.fillna(value=0)  # Fill with 0\ndf.dropna()         # Drop rows with missing values\n\n# 6. Median, Standard Deviation, and Variance\nimport numpy as np\ndata = [1, 2, 3, 4, 5]\nmedian = np.median(data)\nstd_dev = np.std(data)\nvariance = np.var(data)\n\n# 7. Mode\nmode_value = df['column'].mode()\n\n# 8. Data Aggregation\n# Example of aggregation using groupby\ndf.groupby('column_name').agg({'other_column': 'mean'})\n\n# 9. GROUP BY in SQL\n# Example using pandas\ndf.groupby('column_name').agg({'other_column': 'mean'})\n\n# 10. Read data from MySQL to DataFrame\nimport pymysql\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\ndf = pd.read_sql('SELECT * FROM table_name', conn)\nconn.close()\n\n# 11. Reshaping data example\nreshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\n\n# 12. Estimation\n# Helps make informed decisions, calculate confidence intervals, and understand reliability.\n\n# 13. Product Table operations\nimport pandas as pd\n# a) Create DataFrame\ndata = {\n    'Item': ['TV', 'TV', 'TV', 'AC'],\n    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\n    'Rupees': [12000, 10000, 15000, 14000],\n    'USD': [700, 650, 800, 750]\n}\ndf = pd.DataFrame(data)\nprint(df)\n\n# b) Add new rows\nnew_data = pd.DataFrame({\n    'Item': ['AC', 'TV'],\n    'Company': ['SAMSUNG', 'SAMSUNG'],\n    'Rupees': [16000, 12000],\n    'USD': [850, 700]\n})\ndf = df.append(new_data, ignore_index=True)\nprint(df)\n\n# c) Max price of LG TV\nmax_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\nprint(max_price_LG)\n\n# d) Sum of all products\ntotal_sum = df['Rupees'].sum()\nprint(total_sum)\n\n# e) Median USD of Sony products\nmedian_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\nprint(median_usd_sony)\n\n# f) Sort data by Rupees\ndf_sorted = df.sort_values(by='Rupees')\nprint(df_sorted)\n\n# g) Transfer DataFrame to MySQL\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\ndf.to_sql('product_table', conn, if_exists='replace', index=False)\nconn.close()\n\n# 14. Student dataset operations\n# a) Create DataFrame\ndata = {\n    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\n    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\n    'Marks': [76, 89, 45, 88]\n}\ndf = pd.DataFrame(data)\n\n# b) Degree and max marks in each stream\ndf.groupby('Degree')['Marks'].max()\n\n# c) Fill NaN with 76\ndf.fillna(76, inplace=True)\n\n# d) Set index to Name\ndf.set_index('Name', inplace=True)\n\n# e) Name and degree-wise average marks of each student\ndf.groupby(['Degree']).agg({'Marks': 'mean'})\n\n# f) Count number of students in MBA\nmba_count = df[df['Degree'] == 'MBA'].shape[0]\nprint(mba_count)\n\n# g) Mode of marks in BCA\nbca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\nprint(bca_mode)\n\n# UCI Dataset (auto-mpg) operations\n# 1) Load the dataset\nautodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\n\n# 2) Describe the DataFrame\nprint(autodf.describe())\n\n# 3) Display first 10 rows\nprint(autodf.head(10))\n\n# 4) Handle missing values\nautodf.fillna(method='ffill', inplace=True)\nautodf.dropna(inplace=True)\n\n# 5) Car with max mileage\nmax_mileage_car = autodf.loc[autodf[0].idxmax()]\nprint(max_mileage_car)\n\n# 6) Average displacement based on cylinders\navg_displacement = autodf.groupby(1)[3].mean()\nprint(avg_displacement)\n\n# 7) Average number of cylinders\navg_cylinders = autodf[1].mean()\nprint(avg_cylinders)\n'''","metadata":{"id":"Mdho8ROkw6f0","trusted":true,"execution":{"iopub.status.busy":"2025-07-30T10:26:15.189952Z","iopub.execute_input":"2025-07-30T10:26:15.190470Z","iopub.status.idle":"2025-07-30T10:26:15.198494Z","shell.execute_reply.started":"2025-07-30T10:26:15.190444Z","shell.execute_reply":"2025-07-30T10:26:15.197619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Week 3,4: Perform all the below feature selection methods on minimum three datasets\n1. Filter Methods: \n\ni) Basic Methods\nConstant Features\nQuasi-constant features\nDuplicate features\n\n\nii) Statistical Methods\nCorrelation\nMutual Information  (Regression and Classification)\nChi-Square | Fischer Square\nUnivariate feature selection (Or) ANOVA (Regression and Classification)\nUnivariate roc-auc | mse  (Regression and Classification)\n\n\n\n2. Wrapper Methods\nForward Selection - Add one feature at a time recursively\nBackward Selection - Removes one feature at a time recursively\nExhaustive Search - searches across all possible feature combinations\nRecursive Feature Elimination (RFE) - i) Ranking Features ii) Iterative Removal  iii) Stopping Criterion\n\n\n3. Embeded Methods\nThe L1 Regularization (also called as LASSO)\nThe L2 Regularization (also called as Ridge)\nThe L1/L2 Regularization (also called Elastic Net)","metadata":{}},{"cell_type":"markdown","source":"# **DATASET 1 FEATURE ENGINEERING**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:36:03.088183Z","iopub.execute_input":"2025-08-06T03:36:03.088567Z","iopub.status.idle":"2025-08-06T03:36:09.612125Z","shell.execute_reply.started":"2025-08-06T03:36:03.088539Z","shell.execute_reply":"2025-08-06T03:36:09.610011Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Load the Dataset ---\n\ndf = pd.read_csv('/kaggle/input/student-performance/student/student-mat.csv')\nprint(f\"Original dataset shape: {df.shape}\\n\")\n\ndf_original = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:36:41.495617Z","iopub.execute_input":"2025-08-06T03:36:41.496061Z","iopub.status.idle":"2025-08-06T03:36:41.541964Z","shell.execute_reply.started":"2025-08-06T03:36:41.496033Z","shell.execute_reply":"2025-08-06T03:36:41.538811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Constant Method Filtering ---\n\nconstant_columns = [col for col in df_original.columns if df_original[col].nunique() == 1]\ndf_constant_filtered = df_original.drop(columns=constant_columns)\nprint(f\"Columns removed by Constant Method: {constant_columns}\")\nprint(f\"Dataset shape after Constant Method filtering: {df_constant_filtered.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:40:51.706198Z","iopub.execute_input":"2025-08-06T03:40:51.706581Z","iopub.status.idle":"2025-08-06T03:40:51.733488Z","shell.execute_reply.started":"2025-08-06T03:40:51.706557Z","shell.execute_reply":"2025-08-06T03:40:51.731962Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Quasi-Constant Method Filtering ---\n\nthreshold = 0.99\nquasi_constant_columns = []\nfor col in df_original.columns:\n    # Calculate the most frequent value's proportion\n    most_frequent_proportion = df_original[col].value_counts(normalize=True).max()\n    if most_frequent_proportion > threshold:\n        quasi_constant_columns.append(col)\n\ndf_quasi_constant_filtered = df_original.drop(columns=quasi_constant_columns)\nprint(f\"Columns removed by Quasi-Constant Method (threshold={threshold*100}%): {quasi_constant_columns}\")\nprint(f\"Dataset shape after Quasi-Constant Method filtering: {df_quasi_constant_filtered.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:40:54.072610Z","iopub.execute_input":"2025-08-06T03:40:54.072995Z","iopub.status.idle":"2025-08-06T03:40:54.088804Z","shell.execute_reply.started":"2025-08-06T03:40:54.072969Z","shell.execute_reply":"2025-08-06T03:40:54.086798Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Correlation Filtering ---\n\n# Select only numerical columns for correlation calculation\nnumerical_df = df_original.select_dtypes(include=np.number)\ncorrelation_matrix = numerical_df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\n# Find features with correlation greater than a threshold (e.g., 0.9)\ncorrelation_threshold = 0.9\nto_drop_high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n\n# Create a new DataFrame with non-numerical columns and the filtered numerical columns\ndf_correlation_filtered = df_original.drop(columns=to_drop_high_corr)\nprint(f\"Columns removed by Correlation Method (threshold={correlation_threshold}): {to_drop_high_corr}\")\nprint(f\"Dataset shape after Correlation Method filtering: {df_correlation_filtered.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:40:58.135051Z","iopub.execute_input":"2025-08-06T03:40:58.135447Z","iopub.status.idle":"2025-08-06T03:40:58.154225Z","shell.execute_reply.started":"2025-08-06T03:40:58.135421Z","shell.execute_reply":"2025-08-06T03:40:58.151068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Mutual Information Filtering ---\n\n# For demonstration, let's assume 'G3' (final grade) is the target variable\n# If 'G3' is not present, or if it's a regression task, adjust accordingly.\n# For classification, use mutual_info_classif; for regression, use mutual_info_regression.\n\n# Identify numerical and categorical columns\nnumerical_cols = df_original.select_dtypes(include=np.number).columns.tolist()\ncategorical_cols = df_original.select_dtypes(include='object').columns.tolist()\n\n# The 'G3' column is typically numerical in this dataset, so it can be a target for regression.\n# If you want to treat it as classification (e.g., pass/fail), you might need to discretize it.\ntarget_column = 'G3'\n\nif target_column in df_original.columns:\n    X = df_original.drop(columns=[target_column])\n    y = df_original[target_column]\n\n    # Encode categorical features for mutual information calculation\n    X_encoded = X.copy()\n    for col in categorical_cols:\n        if col in X_encoded.columns:\n            le = LabelEncoder()\n            X_encoded[col] = le.fit_transform(X_encoded[col])\n\n    # Calculate mutual information. Use mutual_info_regression for numerical target.\n    # If your target is categorical, use mutual_info_classif.\n    mi_scores = mutual_info_regression(X_encoded, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n\n    print(\"Mutual Information Scores:\")\n    print(mi_scores)\n\n    # You can set a threshold to select features, e.g., keep features with MI score > 0.05\n    mi_threshold = 0.05\n    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n    df_mi_filtered = df_original[selected_features_mi + [target_column]] # Keep target column\n    print(f\"\\nFeatures selected by Mutual Information (threshold={mi_threshold}): {selected_features_mi}\")\n    print(f\"Dataset shape after Mutual Information filtering: {df_mi_filtered.shape}\\n\")\nelse:\n    print(f\"Target column '{target_column}' not found. Skipping Mutual Information filtering.\\n\")\n    df_mi_filtered = df_original.copy() # No filtering applied","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:41:01.443393Z","iopub.execute_input":"2025-08-06T03:41:01.443888Z","iopub.status.idle":"2025-08-06T03:41:01.458986Z","shell.execute_reply.started":"2025-08-06T03:41:01.443851Z","shell.execute_reply":"2025-08-06T03:41:01.457433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. Duplicate Methods Filtering ---\n\ndf_no_duplicate_rows = df_original.drop_duplicates()\nprint(f\"Original number of rows: {df_original.shape[0]}\")\nprint(f\"Number of rows after removing duplicate rows: {df_no_duplicate_rows.shape[0]}\")\nprint(f\"Rows removed: {df_original.shape[0] - df_no_duplicate_rows.shape[0]}\\n\")\n\n# Remove duplicate columns\n# Transpose the DataFrame to treat columns as rows, then drop duplicates, then transpose back.\ndf_transposed = df_original.T\ndf_no_duplicate_cols_transposed = df_transposed.drop_duplicates()\ndf_duplicate_filtered = df_no_duplicate_cols_transposed.T\n\n# Identify columns that were duplicates and removed\noriginal_cols = set(df_original.columns)\nfiltered_cols = set(df_duplicate_filtered.columns)\nremoved_duplicate_columns = list(original_cols - filtered_cols)\n\nprint(f\"Columns removed by Duplicate Columns Method: {removed_duplicate_columns}\")\nprint(f\"Dataset shape after Duplicate Columns filtering: {df_duplicate_filtered.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T03:41:04.864305Z","iopub.execute_input":"2025-08-06T03:41:04.864686Z","iopub.status.idle":"2025-08-06T03:41:04.949271Z","shell.execute_reply.started":"2025-08-06T03:41:04.864660Z","shell.execute_reply":"2025-08-06T03:41:04.946705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **DATASET 2 FEATURE ENGINEERING**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:33:10.579387Z","iopub.execute_input":"2025-08-06T16:33:10.579703Z","iopub.status.idle":"2025-08-06T16:33:10.956497Z","shell.execute_reply.started":"2025-08-06T16:33:10.579679Z","shell.execute_reply":"2025-08-06T16:33:10.955601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 1. Load the Dataset ---\n    \ndf = pd.read_csv('/kaggle/input/drug-induced-autoimmunity-prediction/DIA_trainingset_RDKit_descriptors.csv')\nprint(f\"Original dataset shape: {df.shape}\\n\")\n\ndf_original = df.copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:33:12.505565Z","iopub.execute_input":"2025-08-06T16:33:12.505993Z","iopub.status.idle":"2025-08-06T16:33:12.677369Z","shell.execute_reply.started":"2025-08-06T16:33:12.505971Z","shell.execute_reply":"2025-08-06T16:33:12.676515Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Constant Method Filtering ---\n\nconstant_columns = [col for col in df.columns if df[col].nunique() == 1]\ndf = df.drop(columns=constant_columns)\nprint(f\"Dataset shape after Constant Method filtering: {df.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:33:17.810199Z","iopub.execute_input":"2025-08-06T16:33:17.810514Z","iopub.status.idle":"2025-08-06T16:33:17.848635Z","shell.execute_reply.started":"2025-08-06T16:33:17.810492Z","shell.execute_reply":"2025-08-06T16:33:17.847682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 3. Quasi-Constant Method Filtering ---\n\nthreshold = 0.99\nquasi_constant_columns = [col for col in df.columns \n                          if df[col].value_counts(normalize=True).max() > threshold]\ndf = df.drop(columns=quasi_constant_columns)\nprint(f\"Dataset shape after Quasi-Constant Method filtering: {df.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:51:37.145895Z","iopub.execute_input":"2025-08-06T16:51:37.146280Z","iopub.status.idle":"2025-08-06T16:51:37.162882Z","shell.execute_reply.started":"2025-08-06T16:51:37.146253Z","shell.execute_reply":"2025-08-06T16:51:37.161813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Correlation Filtering ---\n\nnumerical_df = df.select_dtypes(include=np.number)\ncorrelation_matrix = numerical_df.corr().abs()\nupper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n\ncorrelation_threshold = 0.9\nto_drop_high_corr = [\n    column for column in upper_tri.columns\n    if any(upper_tri[column].fillna(0) > correlation_threshold)  # Avoid NaN warning\n]\ndf = df.drop(columns=to_drop_high_corr)\n\nprint(f\"Dataset shape after Correlation Method filtering: {df.shape}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:52:04.037742Z","iopub.execute_input":"2025-08-06T16:52:04.038145Z","iopub.status.idle":"2025-08-06T16:52:04.050822Z","shell.execute_reply.started":"2025-08-06T16:52:04.038084Z","shell.execute_reply":"2025-08-06T16:52:04.049957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 5. Mutual Information Filtering ---\n\n# Replace 'Target' with your actual target column name (e.g., 'label', 'class', etc.)\ntarget_column = 'Label'  # UPDATE THIS LINE if your column has a different name\n\nif target_column in df.columns:\n    X = df.drop(columns=[target_column])\n    y = df[target_column]\n\n    categorical_cols = X.select_dtypes(include='object').columns.tolist()\n    X_encoded = X.copy()\n\n    for col in categorical_cols:\n        le = LabelEncoder()\n        X_encoded[col] = le.fit_transform(X_encoded[col])\n\n    # Use classification or regression based on data type of target\n    if y.nunique() <= 10 and y.dtype in ['int', 'object']:  # assume classification\n        from sklearn.feature_selection import mutual_info_classif\n        mi_scores = mutual_info_classif(X_encoded, y, discrete_features='auto')\n    else:  # assume regression\n        mi_scores = mutual_info_regression(X_encoded, y)\n\n    mi_scores = pd.Series(mi_scores, index=X.columns, name=\"MI Scores\").sort_values(ascending=False)\n    print(\"Mutual Information Scores:\")\n    print(mi_scores)\n\n    mi_threshold = 0.05\n    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n    df = df[selected_features_mi + [target_column]]\n\n    print(f\"\\nFeatures selected by Mutual Information (>{mi_threshold}): {selected_features_mi}\")\n    print(f\"Dataset shape after Mutual Information filtering: {df.shape}\\n\")\nelse:\n    print(f\"Target column '{target_column}' not found. Skipping MI filtering.\\n\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:33:28.522655Z","iopub.execute_input":"2025-08-06T16:33:28.522944Z","iopub.status.idle":"2025-08-06T16:33:28.936410Z","shell.execute_reply.started":"2025-08-06T16:33:28.522923Z","shell.execute_reply":"2025-08-06T16:33:28.935465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. Duplicate Filtering ---\n\nrows_before = df.shape[0]\ndf = df.drop_duplicates()\nrows_after = df.shape[0]\n\nprint(f\"Removed {rows_before - rows_after} duplicate rows.\")\n\ncols_before = df.shape[1]\ndf = df.T.drop_duplicates().T\ncols_after = df.shape[1]\n\nprint(f\"Removed {cols_before - cols_after} duplicate columns.\")\nprint(f\"Final dataset shape: {df.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T16:33:34.433290Z","iopub.execute_input":"2025-08-06T16:33:34.433621Z","iopub.status.idle":"2025-08-06T16:33:34.598678Z","shell.execute_reply.started":"2025-08-06T16:33:34.433594Z","shell.execute_reply":"2025-08-06T16:33:34.597405Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Titanic Dataset Feature Engineering**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.feature_selection import mutual_info_classif, chi2, f_classif","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:48:04.977285Z","iopub.execute_input":"2025-08-12T16:48:04.977724Z","iopub.status.idle":"2025-08-12T16:48:06.595144Z","shell.execute_reply.started":"2025-08-12T16:48:04.977689Z","shell.execute_reply":"2025-08-12T16:48:06.593877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load dataset\n\ndf = pd.read_csv('/kaggle/input/titanic/titanic_train.csv')\n\n# categorical encoding\nfor col in df.select_dtypes(include='object').columns:\n    df[col] = LabelEncoder().fit_transform(df[col])\n\nprint(\"Initial shape:\", df.shape)\n\ntarget = 'Survived'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:48:12.364971Z","iopub.execute_input":"2025-08-12T16:48:12.365434Z","iopub.status.idle":"2025-08-12T16:48:12.414450Z","shell.execute_reply.started":"2025-08-12T16:48:12.365409Z","shell.execute_reply":"2025-08-12T16:48:12.413341Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T13:25:01.137389Z","iopub.execute_input":"2025-08-12T13:25:01.138262Z","iopub.status.idle":"2025-08-12T13:25:01.164495Z","shell.execute_reply.started":"2025-08-12T13:25:01.138222Z","shell.execute_reply":"2025-08-12T13:25:01.163600Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Constant Features\nconstant_features = [col for col in df.columns if df[col].nunique() == 1]\ndf = df.drop(columns=constant_features)\nprint(\"After removing constant features:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:48:20.963991Z","iopub.execute_input":"2025-08-12T16:48:20.964836Z","iopub.status.idle":"2025-08-12T16:48:20.986615Z","shell.execute_reply.started":"2025-08-12T16:48:20.964799Z","shell.execute_reply":"2025-08-12T16:48:20.985131Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Quasi-Constant Features\nquasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\ndf = df.drop(columns=quasi_constant)\nprint(\"After removing quasi-constant features:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:16:17.490442Z","iopub.execute_input":"2025-08-06T17:16:17.490713Z","iopub.status.idle":"2025-08-06T17:16:17.505365Z","shell.execute_reply.started":"2025-08-06T17:16:17.490694Z","shell.execute_reply":"2025-08-06T17:16:17.503992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Duplicate Features\ndf = df.T.drop_duplicates().T\nprint(\"After removing duplicate features:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:16:19.182232Z","iopub.execute_input":"2025-08-06T17:16:19.182553Z","iopub.status.idle":"2025-08-06T17:16:19.206810Z","shell.execute_reply.started":"2025-08-06T17:16:19.182531Z","shell.execute_reply":"2025-08-06T17:16:19.205722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Correlation\ncorr_matrix = df.corr().abs()\nupper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\ndf = df.drop(columns=to_drop_corr)\nprint(\"After removing highly correlated features:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:16:21.291678Z","iopub.execute_input":"2025-08-06T17:16:21.292004Z","iopub.status.idle":"2025-08-06T17:16:21.302947Z","shell.execute_reply.started":"2025-08-06T17:16:21.291983Z","shell.execute_reply":"2025-08-06T17:16:21.302054Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Mutual Information (Classification)\nX = df.drop(columns=[target])\ny = df[target]\nmi_scores = mutual_info_classif(X, y)\nprint(\"\\nTop MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:16:22.776871Z","iopub.execute_input":"2025-08-06T17:16:22.777190Z","iopub.status.idle":"2025-08-06T17:16:22.814172Z","shell.execute_reply.started":"2025-08-06T17:16:22.777166Z","shell.execute_reply":"2025-08-06T17:16:22.813237Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Chi-Square\nX_chi = MinMaxScaler().fit_transform(X)\nchi2_scores = chi2(X_chi, y)[0]\nprint(\"\\nTop Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:16:25.220509Z","iopub.execute_input":"2025-08-06T17:16:25.220784Z","iopub.status.idle":"2025-08-06T17:16:25.287788Z","shell.execute_reply.started":"2025-08-06T17:16:25.220765Z","shell.execute_reply":"2025-08-06T17:16:25.286739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. ANOVA (Classification)\nanova = f_classif(X, y)\nprint(\"\\nTop ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:16:27.352266Z","iopub.execute_input":"2025-08-06T17:16:27.352580Z","iopub.status.idle":"2025-08-06T17:16:27.361881Z","shell.execute_reply.started":"2025-08-06T17:16:27.352559Z","shell.execute_reply":"2025-08-06T17:16:27.360924Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **House Price Dataset Feature Engineering**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nfrom sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression, chi2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:49:39.949088Z","iopub.execute_input":"2025-08-12T16:49:39.949418Z","iopub.status.idle":"2025-08-12T16:49:39.954619Z","shell.execute_reply.started":"2025-08-12T16:49:39.949395Z","shell.execute_reply":"2025-08-12T16:49:39.953386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/house-price-prediction/house_price_train.csv')\n\n#df = df.dropna(axis=1, how='any') - drops empty columns\n\n#df = df.select_dtypes(include=[np.number]) - keeps only numeric columns\n\ntarget = 'SalePrice' if 'SalePrice' in df.columns else df.columns[-1]  \nprint(\"Before removing features:\", df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:54:38.761804Z","iopub.execute_input":"2025-08-12T16:54:38.762116Z","iopub.status.idle":"2025-08-12T16:54:38.795846Z","shell.execute_reply.started":"2025-08-12T16:54:38.762094Z","shell.execute_reply":"2025-08-12T16:54:38.794972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:55:07.761480Z","iopub.execute_input":"2025-08-12T16:55:07.761793Z","iopub.status.idle":"2025-08-12T16:55:07.793417Z","shell.execute_reply.started":"2025-08-12T16:55:07.761772Z","shell.execute_reply":"2025-08-12T16:55:07.792503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Constant Features\nconstant_features = [col for col in df.columns if df[col].nunique() == 1]\ndf = df.drop(columns=constant_features)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:54:50.774011Z","iopub.execute_input":"2025-08-12T16:54:50.774340Z","iopub.status.idle":"2025-08-12T16:54:50.795845Z","shell.execute_reply.started":"2025-08-12T16:54:50.774317Z","shell.execute_reply":"2025-08-12T16:54:50.794760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Quasi-Constant Features\nquasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\ndf = df.drop(columns=quasi_constant)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:17:29.649010Z","iopub.execute_input":"2025-08-06T17:17:29.649339Z","iopub.status.idle":"2025-08-06T17:17:29.672190Z","shell.execute_reply.started":"2025-08-06T17:17:29.649318Z","shell.execute_reply":"2025-08-06T17:17:29.671294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Duplicate Features\ndf = df.T.drop_duplicates().T\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:17:32.121216Z","iopub.execute_input":"2025-08-06T17:17:32.121514Z","iopub.status.idle":"2025-08-06T17:17:32.270243Z","shell.execute_reply.started":"2025-08-06T17:17:32.121493Z","shell.execute_reply":"2025-08-06T17:17:32.269487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Correlation\ncorr_matrix = df.corr().abs()\nupper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\ndf = df.drop(columns=to_drop_corr)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:17:34.153617Z","iopub.execute_input":"2025-08-06T17:17:34.153892Z","iopub.status.idle":"2025-08-06T17:17:34.174878Z","shell.execute_reply.started":"2025-08-06T17:17:34.153873Z","shell.execute_reply":"2025-08-06T17:17:34.173895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set target column\ntarget = 'SalePrice' if 'SalePrice' in df.columns else df.columns[-1]\n\n# Make sure target is still in the dataframe after cleaning\nif target not in df.columns:\n    print(f\"Target column '{target}' not found in DataFrame after preprocessing.\")\n    print(\"Available columns:\", df.columns.tolist())\nelse:\n    X = df.drop(columns=[target])\n    y = df[target]\n\n    # Mutual Information\n    from sklearn.feature_selection import mutual_info_regression, f_regression\n    mi_scores = mutual_info_regression(X, y)\n    print(\"Top MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:17:36.896623Z","iopub.execute_input":"2025-08-06T17:17:36.896929Z","iopub.status.idle":"2025-08-06T17:17:37.195057Z","shell.execute_reply.started":"2025-08-06T17:17:36.896905Z","shell.execute_reply":"2025-08-06T17:17:37.194351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Chi-Square (scaled)\nX_chi = MinMaxScaler().fit_transform(X)\nchi2_scores = chi2(X_chi, y)[0]\nprint(\"Top Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:17:39.664249Z","iopub.execute_input":"2025-08-06T17:17:39.664548Z","iopub.status.idle":"2025-08-06T17:17:39.728656Z","shell.execute_reply.started":"2025-08-06T17:17:39.664528Z","shell.execute_reply":"2025-08-06T17:17:39.727663Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. ANOVA (Regression)\nanova = f_regression(X, y)\nprint(\"Top ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-06T17:17:42.094575Z","iopub.execute_input":"2025-08-06T17:17:42.094854Z","iopub.status.idle":"2025-08-06T17:17:42.115426Z","shell.execute_reply.started":"2025-08-06T17:17:42.094834Z","shell.execute_reply":"2025-08-06T17:17:42.114417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Santander Customer Satisfaction Dataset Feature Engineering**","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif, f_classif\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/santander-customer-satisfaction-prediction/Santander Customer Satisfaction_train.csv')\n#df = df.dropna()\ntarget = 'TARGET' if 'TARGET' in df.columns else df.columns[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:57:07.382781Z","iopub.execute_input":"2025-08-12T16:57:07.383197Z","iopub.status.idle":"2025-08-12T16:57:10.358425Z","shell.execute_reply.started":"2025-08-12T16:57:07.383169Z","shell.execute_reply":"2025-08-12T16:57:10.357604Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T16:57:17.193440Z","iopub.execute_input":"2025-08-12T16:57:17.193742Z","iopub.status.idle":"2025-08-12T16:57:17.218973Z","shell.execute_reply.started":"2025-08-12T16:57:17.193722Z","shell.execute_reply":"2025-08-12T16:57:17.218024Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Constant Features\nconstant_features = [col for col in df.columns if df[col].nunique() == 1]\ndf = df.drop(columns=constant_features)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:03:02.294393Z","iopub.execute_input":"2025-08-12T17:03:02.294693Z","iopub.status.idle":"2025-08-12T17:03:02.599815Z","shell.execute_reply.started":"2025-08-12T17:03:02.294674Z","shell.execute_reply":"2025-08-12T17:03:02.598740Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Quasi-Constant Features\nquasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\ndf = df.drop(columns=quasi_constant)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:09:50.987123Z","iopub.execute_input":"2025-08-12T17:09:50.987482Z","iopub.status.idle":"2025-08-12T17:09:51.191085Z","shell.execute_reply.started":"2025-08-12T17:09:50.987459Z","shell.execute_reply":"2025-08-12T17:09:51.190053Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 3. Duplicate Features\ndf = df.T.drop_duplicates().T\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:10:06.795655Z","iopub.execute_input":"2025-08-12T17:10:06.796242Z","iopub.status.idle":"2025-08-12T17:10:20.590845Z","shell.execute_reply.started":"2025-08-12T17:10:06.796208Z","shell.execute_reply":"2025-08-12T17:10:20.589840Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 4. Correlation\ncorr_matrix = df.corr().abs()\nupper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\ndf = df.drop(columns=to_drop_corr)\nprint(df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:11:44.802773Z","iopub.execute_input":"2025-08-12T17:11:44.803229Z","iopub.status.idle":"2025-08-12T17:11:48.604094Z","shell.execute_reply.started":"2025-08-12T17:11:44.803207Z","shell.execute_reply":"2025-08-12T17:11:48.603106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Mutual Information (Classification)\nX = df.drop(columns=[target])\ny = df[target]\nmi_scores = mutual_info_classif(X, y)\nprint(\"Top MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:12:01.433442Z","iopub.execute_input":"2025-08-12T17:12:01.433752Z","iopub.status.idle":"2025-08-12T17:12:31.800933Z","shell.execute_reply.started":"2025-08-12T17:12:01.433732Z","shell.execute_reply":"2025-08-12T17:12:31.800058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 6. Chi-Square\nX_chi = MinMaxScaler().fit_transform(X)\nchi2_scores = chi2(X_chi, y)[0]\nprint(\"Top Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:13:26.075134Z","iopub.execute_input":"2025-08-12T17:13:26.075470Z","iopub.status.idle":"2025-08-12T17:13:26.227212Z","shell.execute_reply.started":"2025-08-12T17:13:26.075448Z","shell.execute_reply":"2025-08-12T17:13:26.226311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. ANOVA (Classification)\nanova = f_classif(X, y)\nprint(\"Top ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-12T17:13:29.154540Z","iopub.execute_input":"2025-08-12T17:13:29.154887Z","iopub.status.idle":"2025-08-12T17:13:29.257970Z","shell.execute_reply.started":"2025-08-12T17:13:29.154864Z","shell.execute_reply":"2025-08-12T17:13:29.256818Z"}},"outputs":[],"execution_count":null}]}