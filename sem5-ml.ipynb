{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "519618e3",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.004723,
     "end_time": "2025-07-30T10:26:44.438163",
     "exception": false,
     "start_time": "2025-07-30T10:26:44.433440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shrav-jally/ML_lab_sem5/blob/main/sem5_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4acb05aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:44.447078Z",
     "iopub.status.busy": "2025-07-30T10:26:44.446774Z",
     "iopub.status.idle": "2025-07-30T10:26:44.455885Z",
     "shell.execute_reply": "2025-07-30T10:26:44.455179Z"
    },
    "id": "2kKoBdcTeRYz",
    "papermill": {
     "duration": 0.015101,
     "end_time": "2025-07-30T10:26:44.457219",
     "exception": false,
     "start_time": "2025-07-30T10:26:44.442118",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumpy\\npandas\\nscikitlearn\\nmatplotlib\\ntensorflow\\nkeras\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tinyurl.com/ML-2025-26\n",
    "\n",
    "'''\n",
    "numpy\n",
    "pandas\n",
    "scikitlearn\n",
    "matplotlib\n",
    "tensorflow\n",
    "keras\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "351b91b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:44.466220Z",
     "iopub.status.busy": "2025-07-30T10:26:44.465600Z",
     "iopub.status.idle": "2025-07-30T10:26:44.470889Z",
     "shell.execute_reply": "2025-07-30T10:26:44.470210Z"
    },
    "id": "UlspcwvGgQfd",
    "papermill": {
     "duration": 0.010908,
     "end_time": "2025-07-30T10:26:44.472066",
     "exception": false,
     "start_time": "2025-07-30T10:26:44.461158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncreating dataframe\\ninserting, deleting, modifying data\\nbasic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\\nmissing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\\nworking with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\\ndata visualization with pandas - line, bar, scatter, histograms\\nadvaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\\nperformance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "creating dataframe\n",
    "inserting, deleting, modifying data\n",
    "basic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\n",
    "missing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\n",
    "working with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\n",
    "data visualization with pandas - line, bar, scatter, histograms\n",
    "advaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\n",
    "performance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d1853ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:44.480652Z",
     "iopub.status.busy": "2025-07-30T10:26:44.480362Z",
     "iopub.status.idle": "2025-07-30T10:26:44.486096Z",
     "shell.execute_reply": "2025-07-30T10:26:44.485455Z"
    },
    "id": "FJzt_9akjels",
    "papermill": {
     "duration": 0.011486,
     "end_time": "2025-07-30T10:26:44.487365",
     "exception": false,
     "start_time": "2025-07-30T10:26:44.475879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\\'your_dataset.csv\\') #csv (comma separated vectors)\\nprint(df.head())\\nprint(df.describe()) #summary of statistics\\nprint(df.info())\\ndf.sample() #gives random 5 rows of dataframe\\ndf.columns\\ndf[[\\'Age\\', \\'BMI\\']].head() #selecting multiple columns (passing array of feature names)\\ndf[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\\ndf.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\\n\\n#handling missing values\\nprint(df.isnull().sum())  #check for missing values\\ndf_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\\ndf_filled=df.fillna(df.mean())\\n#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\\n\\ndf.fillna(df.mean(), inplace=True)\\nprint(df.duplicated().sum()) #identifies duplicate values\\ndf_no_duplicates = df.drop_duplicates()\\ndf[\"column1\"]=df[\"column1\"].astype(float)\\n\\n#encode categorical variables\\ndf_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\\n#dummy value is assigned to label name\\n#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working with dataframes\n",
    "\n",
    "'''\n",
    "df = pd.read_csv('your_dataset.csv') #csv (comma separated vectors)\n",
    "print(df.head())\n",
    "print(df.describe()) #summary of statistics\n",
    "print(df.info())\n",
    "df.sample() #gives random 5 rows of dataframe\n",
    "df.columns\n",
    "df[['Age', 'BMI']].head() #selecting multiple columns (passing array of feature names)\n",
    "df[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\n",
    "df.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\n",
    "\n",
    "#handling missing values\n",
    "print(df.isnull().sum())  #check for missing values\n",
    "df_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\n",
    "df_filled=df.fillna(df.mean())\n",
    "#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df.duplicated().sum()) #identifies duplicate values\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "df[\"column1\"]=df[\"column1\"].astype(float)\n",
    "\n",
    "#encode categorical variables\n",
    "df_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\n",
    "#dummy value is assigned to label name\n",
    "#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a8c2158",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:44.496255Z",
     "iopub.status.busy": "2025-07-30T10:26:44.495975Z",
     "iopub.status.idle": "2025-07-30T10:26:45.967326Z",
     "shell.execute_reply": "2025-07-30T10:26:45.966373Z"
    },
    "id": "7ZVAkw8enU2N",
    "outputId": "6c5abe3a-2758-4f39-9f1a-7e759d003660",
    "papermill": {
     "duration": 1.47747,
     "end_time": "2025-07-30T10:26:45.968854",
     "exception": false,
     "start_time": "2025-07-30T10:26:44.491384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#Scikit_learn (estimator api) - pip install scikitlearn\n",
    "#uci ml repository - all types of machine learning datasets available\n",
    "#it provides various tools for - classification, regression, clustering, dimensionality reduction, feature selection\n",
    "#sklearn is used to build ml models, for data reading and manipulation and summarizing use pandas, numpy etc.\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5efff222",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:45.978730Z",
     "iopub.status.busy": "2025-07-30T10:26:45.977877Z",
     "iopub.status.idle": "2025-07-30T10:26:46.649352Z",
     "shell.execute_reply": "2025-07-30T10:26:46.648417Z"
    },
    "id": "IgI5rXZCqKWE",
    "outputId": "102812bf-577f-40a9-9415-d2346083d875",
    "papermill": {
     "duration": 0.677684,
     "end_time": "2025-07-30T10:26:46.650754",
     "exception": false,
     "start_time": "2025-07-30T10:26:45.973070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from re import X\n",
    "from inspect import isroutine\n",
    "'''\n",
    "components of sklearn:\n",
    "\n",
    "supervised learning algorithms: SVM, DT\n",
    "cross validation: to check accuracy of supervised models on unseen data using sklearn\n",
    "unsupervised learning algorithms: clustering, factor analysis, principal component analysis to undupervised neural networks\n",
    "various datasets: IRIS\n",
    "feature extraction: sklearn for extracting images and text features\n",
    "model selection: comparing, validating parameters and models\n",
    "'''\n",
    "\n",
    "#most common steps involved in machine learning\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44bb8299",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:46.660312Z",
     "iopub.status.busy": "2025-07-30T10:26:46.659873Z",
     "iopub.status.idle": "2025-07-30T10:26:46.668394Z",
     "shell.execute_reply": "2025-07-30T10:26:46.667653Z"
    },
    "id": "4DlWrtN7w7N0",
    "papermill": {
     "duration": 0.014864,
     "end_time": "2025-07-30T10:26:46.669734",
     "exception": false,
     "start_time": "2025-07-30T10:26:46.654870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 5. Create the following Series and do the specified operations:\\n\\n# a) EngAlph with 26 alphabet elements and default indices\\nimport pandas as pd\\nimport string\\n\\nEngAlph = pd.Series(list(string.ascii_uppercase))\\nprint(EngAlph)\\n\\n# b) Vowels with specific index labels and all values set to zero\\nVowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\\nprint(Vowels.empty)  # Check if it is empty\\n\\n# c) Friends from a dictionary\\nFriends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\\nprint(Friends)\\n\\n# d) MTseries as an empty series\\nMTseries = pd.Series()\\nprint(MTseries.empty)  # Check if it is empty\\n\\n# e) MonthDays from a numpy array\\nimport numpy as np\\nMonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\\nprint(MonthDays)\\n\\n# 6. Using the Series created in Question 5:\\n\\n# a) Set all values of Vowels to 10\\nVowels[:] = 10\\nprint(Vowels)\\n\\n# b) Divide all values of Vowels by 2\\nVowels /= 2\\nprint(Vowels)\\n\\n# c) Create another series Vowels1\\nVowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\\nprint(Vowels1)\\n\\n# d) Add Vowels and Vowels1 and assign to Vowels3\\nVowels3 = Vowels + Vowels1\\nprint(Vowels3)\\n\\n# e) Subtract, multiply, and divide Vowels by Vowels1\\nVowels_sub = Vowels - Vowels1\\nVowels_mul = Vowels * Vowels1\\nVowels_div = Vowels / Vowels1\\nprint(Vowels_sub, Vowels_mul, Vowels_div)\\n\\n# f) Alter labels of Vowels1\\nVowels1.index = ['A', 'E', 'I', 'O', 'U']\\nprint(Vowels1)\\n\\n# 7. Using the Series created in Question 5:\\n\\n# a) Find dimensions, size, and values\\nprint(EngAlph.shape, Vowels.size, Friends.values)\\n\\n# b) Rename MTseries as SeriesEmpty\\nSeriesEmpty = MTseries\\nprint(SeriesEmpty)\\n\\n# c) Name the index of MonthDays and Friends\\nMonthDays.index.name = 'monthno'\\nFriends.index.name = 'Fname'\\n\\n# d) Display the 3rd and 2nd value of Friends\\nprint(Friends.iloc[2], Friends.iloc[1])\\n\\n# e) Display alphabets 'e' to 'p' from EngAlph\\nprint(EngAlph['E':'P'])\\n\\n# f) Display the first 10 values of EngAlph\\nprint(EngAlph.head(10))\\n\\n# g) Display the last 10 values of EngAlph\\nprint(EngAlph.tail(10))\\n\\n# h) Display MTseries\\nprint(MTseries)\\n\\n# 8. Using the Series created in Question 5:\\n\\n# a) Display months 3 through 7 from MonthDays\\nprint(MonthDays[3:8])\\n\\n# b) Display MonthDays in reverse order\\nprint(MonthDays[::-1])\\n\\n# 9. Create the following DataFrame Sales:\\nSales = pd.DataFrame({\\n    '2014': [100.5, 150.8, 200.9, 30000, 40000],\\n    '2015': [12000, 18000, 22000, 30000, 45000],\\n    '2016': [20000, 50000, 70000, 100000, 125000],\\n    '2017': [50000, 60000, 70000, 80000, 90000]\\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\\nprint(Sales)\\n\\n# 10. Use the DataFrame Sales to do the following:\\n\\n# a) Display row labels\\nprint(Sales.index)\\n\\n# b) Display column labels\\nprint(Sales.columns)\\n\\n# c) Display data types of each column\\nprint(Sales.dtypes)\\n\\n# d) Display dimensions, shape, size, and values\\nprint(Sales.shape, Sales.size, Sales.values)\\n\\n# e) Display the last two rows\\nprint(Sales.tail(2))\\n\\n# f) Display the first two columns\\nprint(Sales.iloc[:, :2])\\n\\n# g) Create Sales2 DataFrame from dictionary\\nSales2 = pd.DataFrame({\\n    '2018': [160000, 110000, 500000, 340000, 900000]\\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\\nprint(Sales2)\\n\\n# h) Check if Sales2 is empty\\nprint(Sales2.empty)\\n\\n# 11. Use the DataFrame Sales to do the following:\\n\\n# a) Append Sales2 to Sales\\nSales = Sales.append(Sales2)\\nprint(Sales)\\n\\n# b) Transpose the Sales DataFrame\\nSales = Sales.T\\nprint(Sales)\\n\\n# c) Display sales by all salespersons in 2017\\nprint(Sales['2017'])\\n\\n# d) Display sales by Madhu and Ankit in 2017 and 2018\\nprint(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\\n\\n# e) Display Shruti's sales in 2016\\nprint(Sales.loc['Shruti', '2016'])\\n\\n# f) Add data for Sumeet\\nSales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\\nprint(Sales)\\n\\n# g) Delete 2014 data from Sales\\nSales.drop('2014', axis=1, inplace=True)\\nprint(Sales)\\n\\n# h) Delete data for Kinshuk\\nSales.drop('Kinshuk', axis=0, inplace=True)\\nprint(Sales)\\n\\n# i) Change salesperson names\\nSales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\\nprint(Sales)\\n\\n# j) Update sales made by Shailesh in 2018\\nSales.loc['Shailesh', '2018'] = 100000\\nprint(Sales)\\n\\n# k) Write Sales DataFrame to a CSV file\\nSales.to_csv('SalesFigures.csv', header=False, index=False)\\n\\n# l) Read the CSV file and update row/column labels\\nSalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\\nSalesRetrieved.columns = Sales.columns\\nSalesRetrieved.index = Sales.index\\nprint(SalesRetrieved)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. **Series vs 1-D array, List, Dictionary:**\n",
    "   - **Series:** A 1-D labeled array in pandas with index labels for each element.\n",
    "   - **1-D Array:** Homogeneous, no labels (NumPy array).\n",
    "   - **List:** A Python built-in collection, ordered, but no indexing.\n",
    "   - **Dictionary:** Key-value pair mapping, unordered, no numerical operations like Series.\n",
    "\n",
    "2. **DataFrame vs 2-D array:**\n",
    "   - **DataFrame:** 2-D labeled table with heterogeneous data types (columns can have different types).\n",
    "   - **2-D Array:** Homogeneous (same data type), no labels, and lacks advanced functionality like a DataFrame.\n",
    "\n",
    "3. **DataFrames and Series:**\n",
    "   - A **DataFrame** is a collection of **Series** (each column in a DataFrame is a Series).\n",
    "\n",
    "4. **Size of (i) Series, (ii) DataFrame:**\n",
    "   - **Series Size:** Number of elements (length of the Series).\n",
    "   - **DataFrame Size:** Total elements (rows × columns).\n",
    "'''\n",
    "\n",
    "'''\n",
    "# 5. Create the following Series and do the specified operations:\n",
    "\n",
    "# a) EngAlph with 26 alphabet elements and default indices\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "EngAlph = pd.Series(list(string.ascii_uppercase))\n",
    "print(EngAlph)\n",
    "\n",
    "# b) Vowels with specific index labels and all values set to zero\n",
    "Vowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\n",
    "print(Vowels.empty)  # Check if it is empty\n",
    "\n",
    "# c) Friends from a dictionary\n",
    "Friends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\n",
    "print(Friends)\n",
    "\n",
    "# d) MTseries as an empty series\n",
    "MTseries = pd.Series()\n",
    "print(MTseries.empty)  # Check if it is empty\n",
    "\n",
    "# e) MonthDays from a numpy array\n",
    "import numpy as np\n",
    "MonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\n",
    "print(MonthDays)\n",
    "\n",
    "# 6. Using the Series created in Question 5:\n",
    "\n",
    "# a) Set all values of Vowels to 10\n",
    "Vowels[:] = 10\n",
    "print(Vowels)\n",
    "\n",
    "# b) Divide all values of Vowels by 2\n",
    "Vowels /= 2\n",
    "print(Vowels)\n",
    "\n",
    "# c) Create another series Vowels1\n",
    "Vowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\n",
    "print(Vowels1)\n",
    "\n",
    "# d) Add Vowels and Vowels1 and assign to Vowels3\n",
    "Vowels3 = Vowels + Vowels1\n",
    "print(Vowels3)\n",
    "\n",
    "# e) Subtract, multiply, and divide Vowels by Vowels1\n",
    "Vowels_sub = Vowels - Vowels1\n",
    "Vowels_mul = Vowels * Vowels1\n",
    "Vowels_div = Vowels / Vowels1\n",
    "print(Vowels_sub, Vowels_mul, Vowels_div)\n",
    "\n",
    "# f) Alter labels of Vowels1\n",
    "Vowels1.index = ['A', 'E', 'I', 'O', 'U']\n",
    "print(Vowels1)\n",
    "\n",
    "# 7. Using the Series created in Question 5:\n",
    "\n",
    "# a) Find dimensions, size, and values\n",
    "print(EngAlph.shape, Vowels.size, Friends.values)\n",
    "\n",
    "# b) Rename MTseries as SeriesEmpty\n",
    "SeriesEmpty = MTseries\n",
    "print(SeriesEmpty)\n",
    "\n",
    "# c) Name the index of MonthDays and Friends\n",
    "MonthDays.index.name = 'monthno'\n",
    "Friends.index.name = 'Fname'\n",
    "\n",
    "# d) Display the 3rd and 2nd value of Friends\n",
    "print(Friends.iloc[2], Friends.iloc[1])\n",
    "\n",
    "# e) Display alphabets 'e' to 'p' from EngAlph\n",
    "print(EngAlph['E':'P'])\n",
    "\n",
    "# f) Display the first 10 values of EngAlph\n",
    "print(EngAlph.head(10))\n",
    "\n",
    "# g) Display the last 10 values of EngAlph\n",
    "print(EngAlph.tail(10))\n",
    "\n",
    "# h) Display MTseries\n",
    "print(MTseries)\n",
    "\n",
    "# 8. Using the Series created in Question 5:\n",
    "\n",
    "# a) Display months 3 through 7 from MonthDays\n",
    "print(MonthDays[3:8])\n",
    "\n",
    "# b) Display MonthDays in reverse order\n",
    "print(MonthDays[::-1])\n",
    "\n",
    "# 9. Create the following DataFrame Sales:\n",
    "Sales = pd.DataFrame({\n",
    "    '2014': [100.5, 150.8, 200.9, 30000, 40000],\n",
    "    '2015': [12000, 18000, 22000, 30000, 45000],\n",
    "    '2016': [20000, 50000, 70000, 100000, 125000],\n",
    "    '2017': [50000, 60000, 70000, 80000, 90000]\n",
    "}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\n",
    "print(Sales)\n",
    "\n",
    "# 10. Use the DataFrame Sales to do the following:\n",
    "\n",
    "# a) Display row labels\n",
    "print(Sales.index)\n",
    "\n",
    "# b) Display column labels\n",
    "print(Sales.columns)\n",
    "\n",
    "# c) Display data types of each column\n",
    "print(Sales.dtypes)\n",
    "\n",
    "# d) Display dimensions, shape, size, and values\n",
    "print(Sales.shape, Sales.size, Sales.values)\n",
    "\n",
    "# e) Display the last two rows\n",
    "print(Sales.tail(2))\n",
    "\n",
    "# f) Display the first two columns\n",
    "print(Sales.iloc[:, :2])\n",
    "\n",
    "# g) Create Sales2 DataFrame from dictionary\n",
    "Sales2 = pd.DataFrame({\n",
    "    '2018': [160000, 110000, 500000, 340000, 900000]\n",
    "}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\n",
    "print(Sales2)\n",
    "\n",
    "# h) Check if Sales2 is empty\n",
    "print(Sales2.empty)\n",
    "\n",
    "# 11. Use the DataFrame Sales to do the following:\n",
    "\n",
    "# a) Append Sales2 to Sales\n",
    "Sales = Sales.append(Sales2)\n",
    "print(Sales)\n",
    "\n",
    "# b) Transpose the Sales DataFrame\n",
    "Sales = Sales.T\n",
    "print(Sales)\n",
    "\n",
    "# c) Display sales by all salespersons in 2017\n",
    "print(Sales['2017'])\n",
    "\n",
    "# d) Display sales by Madhu and Ankit in 2017 and 2018\n",
    "print(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\n",
    "\n",
    "# e) Display Shruti's sales in 2016\n",
    "print(Sales.loc['Shruti', '2016'])\n",
    "\n",
    "# f) Add data for Sumeet\n",
    "Sales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\n",
    "print(Sales)\n",
    "\n",
    "# g) Delete 2014 data from Sales\n",
    "Sales.drop('2014', axis=1, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# h) Delete data for Kinshuk\n",
    "Sales.drop('Kinshuk', axis=0, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# i) Change salesperson names\n",
    "Sales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# j) Update sales made by Shailesh in 2018\n",
    "Sales.loc['Shailesh', '2018'] = 100000\n",
    "print(Sales)\n",
    "\n",
    "# k) Write Sales DataFrame to a CSV file\n",
    "Sales.to_csv('SalesFigures.csv', header=False, index=False)\n",
    "\n",
    "# l) Read the CSV file and update row/column labels\n",
    "SalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\n",
    "SalesRetrieved.columns = Sales.columns\n",
    "SalesRetrieved.index = Sales.index\n",
    "print(SalesRetrieved)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c47d90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:46.679444Z",
     "iopub.status.busy": "2025-07-30T10:26:46.679161Z",
     "iopub.status.idle": "2025-07-30T10:26:46.687047Z",
     "shell.execute_reply": "2025-07-30T10:26:46.686237Z"
    },
    "id": "Mdho8ROkw6f0",
    "papermill": {
     "duration": 0.014386,
     "end_time": "2025-07-30T10:26:46.688325",
     "exception": false,
     "start_time": "2025-07-30T10:26:46.673939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 4. Sort DataFrame by multiple columns\\nimport pandas as pd\\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\\ndf.sort_values(by=['col1', 'col2'], ascending=[True, False])\\n\\n# 5. Handling missing values\\ndf.fillna(value=0)  # Fill with 0\\ndf.dropna()         # Drop rows with missing values\\n\\n# 6. Median, Standard Deviation, and Variance\\nimport numpy as np\\ndata = [1, 2, 3, 4, 5]\\nmedian = np.median(data)\\nstd_dev = np.std(data)\\nvariance = np.var(data)\\n\\n# 7. Mode\\nmode_value = df['column'].mode()\\n\\n# 8. Data Aggregation\\n# Example of aggregation using groupby\\ndf.groupby('column_name').agg({'other_column': 'mean'})\\n\\n# 9. GROUP BY in SQL\\n# Example using pandas\\ndf.groupby('column_name').agg({'other_column': 'mean'})\\n\\n# 10. Read data from MySQL to DataFrame\\nimport pymysql\\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\\ndf = pd.read_sql('SELECT * FROM table_name', conn)\\nconn.close()\\n\\n# 11. Reshaping data example\\nreshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\\n\\n# 12. Estimation\\n# Helps make informed decisions, calculate confidence intervals, and understand reliability.\\n\\n# 13. Product Table operations\\nimport pandas as pd\\n# a) Create DataFrame\\ndata = {\\n    'Item': ['TV', 'TV', 'TV', 'AC'],\\n    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\\n    'Rupees': [12000, 10000, 15000, 14000],\\n    'USD': [700, 650, 800, 750]\\n}\\ndf = pd.DataFrame(data)\\nprint(df)\\n\\n# b) Add new rows\\nnew_data = pd.DataFrame({\\n    'Item': ['AC', 'TV'],\\n    'Company': ['SAMSUNG', 'SAMSUNG'],\\n    'Rupees': [16000, 12000],\\n    'USD': [850, 700]\\n})\\ndf = df.append(new_data, ignore_index=True)\\nprint(df)\\n\\n# c) Max price of LG TV\\nmax_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\\nprint(max_price_LG)\\n\\n# d) Sum of all products\\ntotal_sum = df['Rupees'].sum()\\nprint(total_sum)\\n\\n# e) Median USD of Sony products\\nmedian_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\\nprint(median_usd_sony)\\n\\n# f) Sort data by Rupees\\ndf_sorted = df.sort_values(by='Rupees')\\nprint(df_sorted)\\n\\n# g) Transfer DataFrame to MySQL\\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\\ndf.to_sql('product_table', conn, if_exists='replace', index=False)\\nconn.close()\\n\\n# 14. Student dataset operations\\n# a) Create DataFrame\\ndata = {\\n    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\\n    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\\n    'Marks': [76, 89, 45, 88]\\n}\\ndf = pd.DataFrame(data)\\n\\n# b) Degree and max marks in each stream\\ndf.groupby('Degree')['Marks'].max()\\n\\n# c) Fill NaN with 76\\ndf.fillna(76, inplace=True)\\n\\n# d) Set index to Name\\ndf.set_index('Name', inplace=True)\\n\\n# e) Name and degree-wise average marks of each student\\ndf.groupby(['Degree']).agg({'Marks': 'mean'})\\n\\n# f) Count number of students in MBA\\nmba_count = df[df['Degree'] == 'MBA'].shape[0]\\nprint(mba_count)\\n\\n# g) Mode of marks in BCA\\nbca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\\nprint(bca_mode)\\n\\n# UCI Dataset (auto-mpg) operations\\n# 1) Load the dataset\\nautodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\\n\\n# 2) Describe the DataFrame\\nprint(autodf.describe())\\n\\n# 3) Display first 10 rows\\nprint(autodf.head(10))\\n\\n# 4) Handle missing values\\nautodf.fillna(method='ffill', inplace=True)\\nautodf.dropna(inplace=True)\\n\\n# 5) Car with max mileage\\nmax_mileage_car = autodf.loc[autodf[0].idxmax()]\\nprint(max_mileage_car)\\n\\n# 6) Average displacement based on cylinders\\navg_displacement = autodf.groupby(1)[3].mean()\\nprint(avg_displacement)\\n\\n# 7) Average number of cylinders\\navg_cylinders = autodf[1].mean()\\nprint(avg_cylinders)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Install pymysql\n",
    "# pip install pymysql\n",
    "\n",
    "# 2. Pivot vs Pivot_table\n",
    "# pivot():\n",
    "# Used for reshaping data where you specify the index, columns, and values.\n",
    "# It requires unique index/column combinations. If there are duplicates, it throws an error.\n",
    "\n",
    "# pivot_table():\n",
    "# Similar to pivot(), but it can handle duplicate entries. It allows you to specify an aggregation function (e.g., sum, mean) to aggregate data when duplicates exist.\n",
    "# Syntax: pivot_table(index=..., columns=..., values=..., aggfunc='sum')\n",
    "\n",
    "# 3. SQLAlchemy\n",
    "# SQLAlchemy is a Python library that provides an Object Relational Mapping (ORM) interface for interacting with databases.\n",
    "\n",
    "'''\n",
    "# 4. Sort DataFrame by multiple columns\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\n",
    "df.sort_values(by=['col1', 'col2'], ascending=[True, False])\n",
    "\n",
    "# 5. Handling missing values\n",
    "df.fillna(value=0)  # Fill with 0\n",
    "df.dropna()         # Drop rows with missing values\n",
    "\n",
    "# 6. Median, Standard Deviation, and Variance\n",
    "import numpy as np\n",
    "data = [1, 2, 3, 4, 5]\n",
    "median = np.median(data)\n",
    "std_dev = np.std(data)\n",
    "variance = np.var(data)\n",
    "\n",
    "# 7. Mode\n",
    "mode_value = df['column'].mode()\n",
    "\n",
    "# 8. Data Aggregation\n",
    "# Example of aggregation using groupby\n",
    "df.groupby('column_name').agg({'other_column': 'mean'})\n",
    "\n",
    "# 9. GROUP BY in SQL\n",
    "# Example using pandas\n",
    "df.groupby('column_name').agg({'other_column': 'mean'})\n",
    "\n",
    "# 10. Read data from MySQL to DataFrame\n",
    "import pymysql\n",
    "conn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\n",
    "df = pd.read_sql('SELECT * FROM table_name', conn)\n",
    "conn.close()\n",
    "\n",
    "# 11. Reshaping data example\n",
    "reshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\n",
    "\n",
    "# 12. Estimation\n",
    "# Helps make informed decisions, calculate confidence intervals, and understand reliability.\n",
    "\n",
    "# 13. Product Table operations\n",
    "import pandas as pd\n",
    "# a) Create DataFrame\n",
    "data = {\n",
    "    'Item': ['TV', 'TV', 'TV', 'AC'],\n",
    "    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\n",
    "    'Rupees': [12000, 10000, 15000, 14000],\n",
    "    'USD': [700, 650, 800, 750]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# b) Add new rows\n",
    "new_data = pd.DataFrame({\n",
    "    'Item': ['AC', 'TV'],\n",
    "    'Company': ['SAMSUNG', 'SAMSUNG'],\n",
    "    'Rupees': [16000, 12000],\n",
    "    'USD': [850, 700]\n",
    "})\n",
    "df = df.append(new_data, ignore_index=True)\n",
    "print(df)\n",
    "\n",
    "# c) Max price of LG TV\n",
    "max_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\n",
    "print(max_price_LG)\n",
    "\n",
    "# d) Sum of all products\n",
    "total_sum = df['Rupees'].sum()\n",
    "print(total_sum)\n",
    "\n",
    "# e) Median USD of Sony products\n",
    "median_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\n",
    "print(median_usd_sony)\n",
    "\n",
    "# f) Sort data by Rupees\n",
    "df_sorted = df.sort_values(by='Rupees')\n",
    "print(df_sorted)\n",
    "\n",
    "# g) Transfer DataFrame to MySQL\n",
    "conn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\n",
    "df.to_sql('product_table', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "# 14. Student dataset operations\n",
    "# a) Create DataFrame\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\n",
    "    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\n",
    "    'Marks': [76, 89, 45, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# b) Degree and max marks in each stream\n",
    "df.groupby('Degree')['Marks'].max()\n",
    "\n",
    "# c) Fill NaN with 76\n",
    "df.fillna(76, inplace=True)\n",
    "\n",
    "# d) Set index to Name\n",
    "df.set_index('Name', inplace=True)\n",
    "\n",
    "# e) Name and degree-wise average marks of each student\n",
    "df.groupby(['Degree']).agg({'Marks': 'mean'})\n",
    "\n",
    "# f) Count number of students in MBA\n",
    "mba_count = df[df['Degree'] == 'MBA'].shape[0]\n",
    "print(mba_count)\n",
    "\n",
    "# g) Mode of marks in BCA\n",
    "bca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\n",
    "print(bca_mode)\n",
    "\n",
    "# UCI Dataset (auto-mpg) operations\n",
    "# 1) Load the dataset\n",
    "autodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\n",
    "\n",
    "# 2) Describe the DataFrame\n",
    "print(autodf.describe())\n",
    "\n",
    "# 3) Display first 10 rows\n",
    "print(autodf.head(10))\n",
    "\n",
    "# 4) Handle missing values\n",
    "autodf.fillna(method='ffill', inplace=True)\n",
    "autodf.dropna(inplace=True)\n",
    "\n",
    "# 5) Car with max mileage\n",
    "max_mileage_car = autodf.loc[autodf[0].idxmax()]\n",
    "print(max_mileage_car)\n",
    "\n",
    "# 6) Average displacement based on cylinders\n",
    "avg_displacement = autodf.groupby(1)[3].mean()\n",
    "print(avg_displacement)\n",
    "\n",
    "# 7) Average number of cylinders\n",
    "avg_cylinders = autodf[1].mean()\n",
    "print(avg_cylinders)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e8531",
   "metadata": {
    "papermill": {
     "duration": 0.004104,
     "end_time": "2025-07-30T10:26:46.696990",
     "exception": false,
     "start_time": "2025-07-30T10:26:46.692886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##DRUG REVIEWS FEATURE CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91cb3db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:46.706845Z",
     "iopub.status.busy": "2025-07-30T10:26:46.706508Z",
     "iopub.status.idle": "2025-07-30T10:26:48.733524Z",
     "shell.execute_reply": "2025-07-30T10:26:48.732751Z"
    },
    "papermill": {
     "duration": 2.033981,
     "end_time": "2025-07-30T10:26:48.735228",
     "exception": false,
     "start_time": "2025-07-30T10:26:46.701247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7e76853",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:48.746247Z",
     "iopub.status.busy": "2025-07-30T10:26:48.745793Z",
     "iopub.status.idle": "2025-07-30T10:26:48.771078Z",
     "shell.execute_reply": "2025-07-30T10:26:48.769798Z"
    },
    "papermill": {
     "duration": 0.0319,
     "end_time": "2025-07-30T10:26:48.772515",
     "exception": false,
     "start_time": "2025-07-30T10:26:48.740615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Dataset ---\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/student-performance/student/student-mat.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\\n\")\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6a3c5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:48.782700Z",
     "iopub.status.busy": "2025-07-30T10:26:48.782351Z",
     "iopub.status.idle": "2025-07-30T10:26:48.799201Z",
     "shell.execute_reply": "2025-07-30T10:26:48.798438Z"
    },
    "papermill": {
     "duration": 0.023515,
     "end_time": "2025-07-30T10:26:48.800580",
     "exception": false,
     "start_time": "2025-07-30T10:26:48.777065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Constant Method: []\n",
      "Dataset shape after Constant Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Constant Method Filtering ---\n",
    "\n",
    "constant_columns = [col for col in df_original.columns if df_original[col].nunique() == 1]\n",
    "df_constant_filtered = df_original.drop(columns=constant_columns)\n",
    "print(f\"Columns removed by Constant Method: {constant_columns}\")\n",
    "print(f\"Dataset shape after Constant Method filtering: {df_constant_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2390d6b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:48.810894Z",
     "iopub.status.busy": "2025-07-30T10:26:48.810559Z",
     "iopub.status.idle": "2025-07-30T10:26:48.820526Z",
     "shell.execute_reply": "2025-07-30T10:26:48.819580Z"
    },
    "papermill": {
     "duration": 0.016738,
     "end_time": "2025-07-30T10:26:48.821918",
     "exception": false,
     "start_time": "2025-07-30T10:26:48.805180",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Quasi-Constant Method (threshold=99.0%): []\n",
      "Dataset shape after Quasi-Constant Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Quasi-Constant Method Filtering ---\n",
    "\n",
    "threshold = 0.99\n",
    "quasi_constant_columns = []\n",
    "for col in df_original.columns:\n",
    "    # Calculate the most frequent value's proportion\n",
    "    most_frequent_proportion = df_original[col].value_counts(normalize=True).max()\n",
    "    if most_frequent_proportion > threshold:\n",
    "        quasi_constant_columns.append(col)\n",
    "\n",
    "df_quasi_constant_filtered = df_original.drop(columns=quasi_constant_columns)\n",
    "print(f\"Columns removed by Quasi-Constant Method (threshold={threshold*100}%): {quasi_constant_columns}\")\n",
    "print(f\"Dataset shape after Quasi-Constant Method filtering: {df_quasi_constant_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd31d5d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:48.832069Z",
     "iopub.status.busy": "2025-07-30T10:26:48.831749Z",
     "iopub.status.idle": "2025-07-30T10:26:48.841748Z",
     "shell.execute_reply": "2025-07-30T10:26:48.840882Z"
    },
    "papermill": {
     "duration": 0.016792,
     "end_time": "2025-07-30T10:26:48.843211",
     "exception": false,
     "start_time": "2025-07-30T10:26:48.826419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Correlation Method (threshold=0.9): []\n",
      "Dataset shape after Correlation Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Correlation Filtering ---\n",
    "\n",
    "# Select only numerical columns for correlation calculation\n",
    "numerical_df = df_original.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than a threshold (e.g., 0.9)\n",
    "correlation_threshold = 0.9\n",
    "to_drop_high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "# Create a new DataFrame with non-numerical columns and the filtered numerical columns\n",
    "df_correlation_filtered = df_original.drop(columns=to_drop_high_corr)\n",
    "print(f\"Columns removed by Correlation Method (threshold={correlation_threshold}): {to_drop_high_corr}\")\n",
    "print(f\"Dataset shape after Correlation Method filtering: {df_correlation_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355d9ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:48.853636Z",
     "iopub.status.busy": "2025-07-30T10:26:48.853311Z",
     "iopub.status.idle": "2025-07-30T10:26:48.862567Z",
     "shell.execute_reply": "2025-07-30T10:26:48.861751Z"
    },
    "papermill": {
     "duration": 0.016076,
     "end_time": "2025-07-30T10:26:48.863841",
     "exception": false,
     "start_time": "2025-07-30T10:26:48.847765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Applying Mutual Information Filtering ---\n",
      "Target column 'G3' not found. Skipping Mutual Information filtering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Mutual Information Filtering ---\n",
    "# Select features based on their mutual information with the target variable.\n",
    "# Mutual information measures the dependency between two variables.\n",
    "print(\"--- Applying Mutual Information Filtering ---\")\n",
    "\n",
    "# For demonstration, let's assume 'G3' (final grade) is the target variable\n",
    "# If 'G3' is not present, or if it's a regression task, adjust accordingly.\n",
    "# For classification, use mutual_info_classif; for regression, use mutual_info_regression.\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_original.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df_original.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# The 'G3' column is typically numerical in this dataset, so it can be a target for regression.\n",
    "# If you want to treat it as classification (e.g., pass/fail), you might need to discretize it.\n",
    "target_column = 'G3'\n",
    "\n",
    "if target_column in df_original.columns:\n",
    "    X = df_original.drop(columns=[target_column])\n",
    "    y = df_original[target_column]\n",
    "\n",
    "    # Encode categorical features for mutual information calculation\n",
    "    X_encoded = X.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in X_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "\n",
    "    # Calculate mutual information. Use mutual_info_regression for numerical target.\n",
    "    # If your target is categorical, use mutual_info_classif.\n",
    "    mi_scores = mutual_info_regression(X_encoded, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    print(\"Mutual Information Scores:\")\n",
    "    print(mi_scores)\n",
    "\n",
    "    # You can set a threshold to select features, e.g., keep features with MI score > 0.05\n",
    "    mi_threshold = 0.05\n",
    "    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n",
    "    df_mi_filtered = df_original[selected_features_mi + [target_column]] # Keep target column\n",
    "    print(f\"\\nFeatures selected by Mutual Information (threshold={mi_threshold}): {selected_features_mi}\")\n",
    "    print(f\"Dataset shape after Mutual Information filtering: {df_mi_filtered.shape}\\n\")\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' not found. Skipping Mutual Information filtering.\\n\")\n",
    "    df_mi_filtered = df_original.copy() # No filtering applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a563fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-30T10:26:48.874136Z",
     "iopub.status.busy": "2025-07-30T10:26:48.873830Z",
     "iopub.status.idle": "2025-07-30T10:26:48.911181Z",
     "shell.execute_reply": "2025-07-30T10:26:48.910091Z"
    },
    "papermill": {
     "duration": 0.044168,
     "end_time": "2025-07-30T10:26:48.912655",
     "exception": false,
     "start_time": "2025-07-30T10:26:48.868487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 395\n",
      "Number of rows after removing duplicate rows: 395\n",
      "Rows removed: 0\n",
      "\n",
      "Columns removed by Duplicate Columns Method: []\n",
      "Dataset shape after Duplicate Columns filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Duplicate Methods Filtering ---\n",
    "\n",
    "df_no_duplicate_rows = df_original.drop_duplicates()\n",
    "print(f\"Original number of rows: {df_original.shape[0]}\")\n",
    "print(f\"Number of rows after removing duplicate rows: {df_no_duplicate_rows.shape[0]}\")\n",
    "print(f\"Rows removed: {df_original.shape[0] - df_no_duplicate_rows.shape[0]}\\n\")\n",
    "\n",
    "# Remove duplicate columns\n",
    "# Transpose the DataFrame to treat columns as rows, then drop duplicates, then transpose back.\n",
    "df_transposed = df_original.T\n",
    "df_no_duplicate_cols_transposed = df_transposed.drop_duplicates()\n",
    "df_duplicate_filtered = df_no_duplicate_cols_transposed.T\n",
    "\n",
    "# Identify columns that were duplicates and removed\n",
    "original_cols = set(df_original.columns)\n",
    "filtered_cols = set(df_duplicate_filtered.columns)\n",
    "removed_duplicate_columns = list(original_cols - filtered_cols)\n",
    "\n",
    "print(f\"Columns removed by Duplicate Columns Method: {removed_duplicate_columns}\")\n",
    "print(f\"Dataset shape after Duplicate Columns filtering: {df_duplicate_filtered.shape}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0Q7oDHoKUvBtfynUFNzI3",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7973641,
     "sourceId": 12620520,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.841891,
   "end_time": "2025-07-30T10:26:49.637560",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-30T10:26:39.795669",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
