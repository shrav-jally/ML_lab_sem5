{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc5d4e2",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.009573,
     "end_time": "2025-08-06T10:25:49.034385",
     "exception": false,
     "start_time": "2025-08-06T10:25:49.024812",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shrav-jally/ML_lab_sem5/blob/main/sem5_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a3b621b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:49.052971Z",
     "iopub.status.busy": "2025-08-06T10:25:49.052654Z",
     "iopub.status.idle": "2025-08-06T10:25:49.062774Z",
     "shell.execute_reply": "2025-08-06T10:25:49.061871Z"
    },
    "id": "2kKoBdcTeRYz",
    "papermill": {
     "duration": 0.021118,
     "end_time": "2025-08-06T10:25:49.064279",
     "exception": false,
     "start_time": "2025-08-06T10:25:49.043161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumpy\\npandas\\nscikitlearn\\nmatplotlib\\ntensorflow\\nkeras\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tinyurl.com/ML-2025-26\n",
    "\n",
    "'''\n",
    "numpy\n",
    "pandas\n",
    "scikitlearn\n",
    "matplotlib\n",
    "tensorflow\n",
    "keras\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f915968",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:49.084960Z",
     "iopub.status.busy": "2025-08-06T10:25:49.084635Z",
     "iopub.status.idle": "2025-08-06T10:25:49.090929Z",
     "shell.execute_reply": "2025-08-06T10:25:49.090052Z"
    },
    "id": "UlspcwvGgQfd",
    "papermill": {
     "duration": 0.019172,
     "end_time": "2025-08-06T10:25:49.092297",
     "exception": false,
     "start_time": "2025-08-06T10:25:49.073125",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncreating dataframe\\ninserting, deleting, modifying data\\nbasic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\\nmissing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\\nworking with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\\ndata visualization with pandas - line, bar, scatter, histograms\\nadvaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\\nperformance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "creating dataframe\n",
    "inserting, deleting, modifying data\n",
    "basic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\n",
    "missing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\n",
    "working with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\n",
    "data visualization with pandas - line, bar, scatter, histograms\n",
    "advaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\n",
    "performance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb3c932",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:49.111248Z",
     "iopub.status.busy": "2025-08-06T10:25:49.110906Z",
     "iopub.status.idle": "2025-08-06T10:25:49.117402Z",
     "shell.execute_reply": "2025-08-06T10:25:49.116371Z"
    },
    "id": "FJzt_9akjels",
    "papermill": {
     "duration": 0.018038,
     "end_time": "2025-08-06T10:25:49.119038",
     "exception": false,
     "start_time": "2025-08-06T10:25:49.101000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\\'your_dataset.csv\\') #csv (comma separated vectors)\\nprint(df.head())\\nprint(df.describe()) #summary of statistics\\nprint(df.info())\\ndf.sample() #gives random 5 rows of dataframe\\ndf.columns\\ndf[[\\'Age\\', \\'BMI\\']].head() #selecting multiple columns (passing array of feature names)\\ndf[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\\ndf.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\\n\\n#handling missing values\\nprint(df.isnull().sum())  #check for missing values\\ndf_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\\ndf_filled=df.fillna(df.mean())\\n#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\\n\\ndf.fillna(df.mean(), inplace=True)\\nprint(df.duplicated().sum()) #identifies duplicate values\\ndf_no_duplicates = df.drop_duplicates()\\ndf[\"column1\"]=df[\"column1\"].astype(float)\\n\\n#encode categorical variables\\ndf_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\\n#dummy value is assigned to label name\\n#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working with dataframes\n",
    "\n",
    "'''\n",
    "df = pd.read_csv('your_dataset.csv') #csv (comma separated vectors)\n",
    "print(df.head())\n",
    "print(df.describe()) #summary of statistics\n",
    "print(df.info())\n",
    "df.sample() #gives random 5 rows of dataframe\n",
    "df.columns\n",
    "df[['Age', 'BMI']].head() #selecting multiple columns (passing array of feature names)\n",
    "df[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\n",
    "df.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\n",
    "\n",
    "#handling missing values\n",
    "print(df.isnull().sum())  #check for missing values\n",
    "df_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\n",
    "df_filled=df.fillna(df.mean())\n",
    "#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df.duplicated().sum()) #identifies duplicate values\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "df[\"column1\"]=df[\"column1\"].astype(float)\n",
    "\n",
    "#encode categorical variables\n",
    "df_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\n",
    "#dummy value is assigned to label name\n",
    "#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a060f52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:49.138023Z",
     "iopub.status.busy": "2025-08-06T10:25:49.137715Z",
     "iopub.status.idle": "2025-08-06T10:25:50.779060Z",
     "shell.execute_reply": "2025-08-06T10:25:50.778041Z"
    },
    "id": "7ZVAkw8enU2N",
    "outputId": "6c5abe3a-2758-4f39-9f1a-7e759d003660",
    "papermill": {
     "duration": 1.653064,
     "end_time": "2025-08-06T10:25:50.781072",
     "exception": false,
     "start_time": "2025-08-06T10:25:49.128008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#Scikit_learn (estimator api) - pip install scikitlearn\n",
    "#uci ml repository - all types of machine learning datasets available\n",
    "#it provides various tools for - classification, regression, clustering, dimensionality reduction, feature selection\n",
    "#sklearn is used to build ml models, for data reading and manipulation and summarizing use pandas, numpy etc.\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91cec9b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:50.812136Z",
     "iopub.status.busy": "2025-08-06T10:25:50.811537Z",
     "iopub.status.idle": "2025-08-06T10:25:51.600338Z",
     "shell.execute_reply": "2025-08-06T10:25:51.599144Z"
    },
    "id": "IgI5rXZCqKWE",
    "outputId": "102812bf-577f-40a9-9415-d2346083d875",
    "papermill": {
     "duration": 0.804966,
     "end_time": "2025-08-06T10:25:51.601885",
     "exception": false,
     "start_time": "2025-08-06T10:25:50.796919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from re import X\n",
    "from inspect import isroutine\n",
    "'''\n",
    "components of sklearn:\n",
    "\n",
    "supervised learning algorithms: SVM, DT\n",
    "cross validation: to check accuracy of supervised models on unseen data using sklearn\n",
    "unsupervised learning algorithms: clustering, factor analysis, principal component analysis to undupervised neural networks\n",
    "various datasets: IRIS\n",
    "feature extraction: sklearn for extracting images and text features\n",
    "model selection: comparing, validating parameters and models\n",
    "'''\n",
    "\n",
    "#most common steps involved in machine learning\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3418a182",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:51.621141Z",
     "iopub.status.busy": "2025-08-06T10:25:51.620828Z",
     "iopub.status.idle": "2025-08-06T10:25:51.630583Z",
     "shell.execute_reply": "2025-08-06T10:25:51.629659Z"
    },
    "id": "4DlWrtN7w7N0",
    "papermill": {
     "duration": 0.021213,
     "end_time": "2025-08-06T10:25:51.632071",
     "exception": false,
     "start_time": "2025-08-06T10:25:51.610858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 5. Create the following Series and do the specified operations:\\n\\n# a) EngAlph with 26 alphabet elements and default indices\\nimport pandas as pd\\nimport string\\n\\nEngAlph = pd.Series(list(string.ascii_uppercase))\\nprint(EngAlph)\\n\\n# b) Vowels with specific index labels and all values set to zero\\nVowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\\nprint(Vowels.empty)  # Check if it is empty\\n\\n# c) Friends from a dictionary\\nFriends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\\nprint(Friends)\\n\\n# d) MTseries as an empty series\\nMTseries = pd.Series()\\nprint(MTseries.empty)  # Check if it is empty\\n\\n# e) MonthDays from a numpy array\\nimport numpy as np\\nMonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\\nprint(MonthDays)\\n\\n# 6. Using the Series created in Question 5:\\n\\n# a) Set all values of Vowels to 10\\nVowels[:] = 10\\nprint(Vowels)\\n\\n# b) Divide all values of Vowels by 2\\nVowels /= 2\\nprint(Vowels)\\n\\n# c) Create another series Vowels1\\nVowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\\nprint(Vowels1)\\n\\n# d) Add Vowels and Vowels1 and assign to Vowels3\\nVowels3 = Vowels + Vowels1\\nprint(Vowels3)\\n\\n# e) Subtract, multiply, and divide Vowels by Vowels1\\nVowels_sub = Vowels - Vowels1\\nVowels_mul = Vowels * Vowels1\\nVowels_div = Vowels / Vowels1\\nprint(Vowels_sub, Vowels_mul, Vowels_div)\\n\\n# f) Alter labels of Vowels1\\nVowels1.index = ['A', 'E', 'I', 'O', 'U']\\nprint(Vowels1)\\n\\n# 7. Using the Series created in Question 5:\\n\\n# a) Find dimensions, size, and values\\nprint(EngAlph.shape, Vowels.size, Friends.values)\\n\\n# b) Rename MTseries as SeriesEmpty\\nSeriesEmpty = MTseries\\nprint(SeriesEmpty)\\n\\n# c) Name the index of MonthDays and Friends\\nMonthDays.index.name = 'monthno'\\nFriends.index.name = 'Fname'\\n\\n# d) Display the 3rd and 2nd value of Friends\\nprint(Friends.iloc[2], Friends.iloc[1])\\n\\n# e) Display alphabets 'e' to 'p' from EngAlph\\nprint(EngAlph['E':'P'])\\n\\n# f) Display the first 10 values of EngAlph\\nprint(EngAlph.head(10))\\n\\n# g) Display the last 10 values of EngAlph\\nprint(EngAlph.tail(10))\\n\\n# h) Display MTseries\\nprint(MTseries)\\n\\n# 8. Using the Series created in Question 5:\\n\\n# a) Display months 3 through 7 from MonthDays\\nprint(MonthDays[3:8])\\n\\n# b) Display MonthDays in reverse order\\nprint(MonthDays[::-1])\\n\\n# 9. Create the following DataFrame Sales:\\nSales = pd.DataFrame({\\n    '2014': [100.5, 150.8, 200.9, 30000, 40000],\\n    '2015': [12000, 18000, 22000, 30000, 45000],\\n    '2016': [20000, 50000, 70000, 100000, 125000],\\n    '2017': [50000, 60000, 70000, 80000, 90000]\\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\\nprint(Sales)\\n\\n# 10. Use the DataFrame Sales to do the following:\\n\\n# a) Display row labels\\nprint(Sales.index)\\n\\n# b) Display column labels\\nprint(Sales.columns)\\n\\n# c) Display data types of each column\\nprint(Sales.dtypes)\\n\\n# d) Display dimensions, shape, size, and values\\nprint(Sales.shape, Sales.size, Sales.values)\\n\\n# e) Display the last two rows\\nprint(Sales.tail(2))\\n\\n# f) Display the first two columns\\nprint(Sales.iloc[:, :2])\\n\\n# g) Create Sales2 DataFrame from dictionary\\nSales2 = pd.DataFrame({\\n    '2018': [160000, 110000, 500000, 340000, 900000]\\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\\nprint(Sales2)\\n\\n# h) Check if Sales2 is empty\\nprint(Sales2.empty)\\n\\n# 11. Use the DataFrame Sales to do the following:\\n\\n# a) Append Sales2 to Sales\\nSales = Sales.append(Sales2)\\nprint(Sales)\\n\\n# b) Transpose the Sales DataFrame\\nSales = Sales.T\\nprint(Sales)\\n\\n# c) Display sales by all salespersons in 2017\\nprint(Sales['2017'])\\n\\n# d) Display sales by Madhu and Ankit in 2017 and 2018\\nprint(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\\n\\n# e) Display Shruti's sales in 2016\\nprint(Sales.loc['Shruti', '2016'])\\n\\n# f) Add data for Sumeet\\nSales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\\nprint(Sales)\\n\\n# g) Delete 2014 data from Sales\\nSales.drop('2014', axis=1, inplace=True)\\nprint(Sales)\\n\\n# h) Delete data for Kinshuk\\nSales.drop('Kinshuk', axis=0, inplace=True)\\nprint(Sales)\\n\\n# i) Change salesperson names\\nSales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\\nprint(Sales)\\n\\n# j) Update sales made by Shailesh in 2018\\nSales.loc['Shailesh', '2018'] = 100000\\nprint(Sales)\\n\\n# k) Write Sales DataFrame to a CSV file\\nSales.to_csv('SalesFigures.csv', header=False, index=False)\\n\\n# l) Read the CSV file and update row/column labels\\nSalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\\nSalesRetrieved.columns = Sales.columns\\nSalesRetrieved.index = Sales.index\\nprint(SalesRetrieved)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. **Series vs 1-D array, List, Dictionary:**\n",
    "   - **Series:** A 1-D labeled array in pandas with index labels for each element.\n",
    "   - **1-D Array:** Homogeneous, no labels (NumPy array).\n",
    "   - **List:** A Python built-in collection, ordered, but no indexing.\n",
    "   - **Dictionary:** Key-value pair mapping, unordered, no numerical operations like Series.\n",
    "\n",
    "2. **DataFrame vs 2-D array:**\n",
    "   - **DataFrame:** 2-D labeled table with heterogeneous data types (columns can have different types).\n",
    "   - **2-D Array:** Homogeneous (same data type), no labels, and lacks advanced functionality like a DataFrame.\n",
    "\n",
    "3. **DataFrames and Series:**\n",
    "   - A **DataFrame** is a collection of **Series** (each column in a DataFrame is a Series).\n",
    "\n",
    "4. **Size of (i) Series, (ii) DataFrame:**\n",
    "   - **Series Size:** Number of elements (length of the Series).\n",
    "   - **DataFrame Size:** Total elements (rows Ã— columns).\n",
    "'''\n",
    "\n",
    "'''\n",
    "# 5. Create the following Series and do the specified operations:\n",
    "\n",
    "# a) EngAlph with 26 alphabet elements and default indices\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "EngAlph = pd.Series(list(string.ascii_uppercase))\n",
    "print(EngAlph)\n",
    "\n",
    "# b) Vowels with specific index labels and all values set to zero\n",
    "Vowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\n",
    "print(Vowels.empty)  # Check if it is empty\n",
    "\n",
    "# c) Friends from a dictionary\n",
    "Friends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\n",
    "print(Friends)\n",
    "\n",
    "# d) MTseries as an empty series\n",
    "MTseries = pd.Series()\n",
    "print(MTseries.empty)  # Check if it is empty\n",
    "\n",
    "# e) MonthDays from a numpy array\n",
    "import numpy as np\n",
    "MonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\n",
    "print(MonthDays)\n",
    "\n",
    "# 6. Using the Series created in Question 5:\n",
    "\n",
    "# a) Set all values of Vowels to 10\n",
    "Vowels[:] = 10\n",
    "print(Vowels)\n",
    "\n",
    "# b) Divide all values of Vowels by 2\n",
    "Vowels /= 2\n",
    "print(Vowels)\n",
    "\n",
    "# c) Create another series Vowels1\n",
    "Vowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\n",
    "print(Vowels1)\n",
    "\n",
    "# d) Add Vowels and Vowels1 and assign to Vowels3\n",
    "Vowels3 = Vowels + Vowels1\n",
    "print(Vowels3)\n",
    "\n",
    "# e) Subtract, multiply, and divide Vowels by Vowels1\n",
    "Vowels_sub = Vowels - Vowels1\n",
    "Vowels_mul = Vowels * Vowels1\n",
    "Vowels_div = Vowels / Vowels1\n",
    "print(Vowels_sub, Vowels_mul, Vowels_div)\n",
    "\n",
    "# f) Alter labels of Vowels1\n",
    "Vowels1.index = ['A', 'E', 'I', 'O', 'U']\n",
    "print(Vowels1)\n",
    "\n",
    "# 7. Using the Series created in Question 5:\n",
    "\n",
    "# a) Find dimensions, size, and values\n",
    "print(EngAlph.shape, Vowels.size, Friends.values)\n",
    "\n",
    "# b) Rename MTseries as SeriesEmpty\n",
    "SeriesEmpty = MTseries\n",
    "print(SeriesEmpty)\n",
    "\n",
    "# c) Name the index of MonthDays and Friends\n",
    "MonthDays.index.name = 'monthno'\n",
    "Friends.index.name = 'Fname'\n",
    "\n",
    "# d) Display the 3rd and 2nd value of Friends\n",
    "print(Friends.iloc[2], Friends.iloc[1])\n",
    "\n",
    "# e) Display alphabets 'e' to 'p' from EngAlph\n",
    "print(EngAlph['E':'P'])\n",
    "\n",
    "# f) Display the first 10 values of EngAlph\n",
    "print(EngAlph.head(10))\n",
    "\n",
    "# g) Display the last 10 values of EngAlph\n",
    "print(EngAlph.tail(10))\n",
    "\n",
    "# h) Display MTseries\n",
    "print(MTseries)\n",
    "\n",
    "# 8. Using the Series created in Question 5:\n",
    "\n",
    "# a) Display months 3 through 7 from MonthDays\n",
    "print(MonthDays[3:8])\n",
    "\n",
    "# b) Display MonthDays in reverse order\n",
    "print(MonthDays[::-1])\n",
    "\n",
    "# 9. Create the following DataFrame Sales:\n",
    "Sales = pd.DataFrame({\n",
    "    '2014': [100.5, 150.8, 200.9, 30000, 40000],\n",
    "    '2015': [12000, 18000, 22000, 30000, 45000],\n",
    "    '2016': [20000, 50000, 70000, 100000, 125000],\n",
    "    '2017': [50000, 60000, 70000, 80000, 90000]\n",
    "}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\n",
    "print(Sales)\n",
    "\n",
    "# 10. Use the DataFrame Sales to do the following:\n",
    "\n",
    "# a) Display row labels\n",
    "print(Sales.index)\n",
    "\n",
    "# b) Display column labels\n",
    "print(Sales.columns)\n",
    "\n",
    "# c) Display data types of each column\n",
    "print(Sales.dtypes)\n",
    "\n",
    "# d) Display dimensions, shape, size, and values\n",
    "print(Sales.shape, Sales.size, Sales.values)\n",
    "\n",
    "# e) Display the last two rows\n",
    "print(Sales.tail(2))\n",
    "\n",
    "# f) Display the first two columns\n",
    "print(Sales.iloc[:, :2])\n",
    "\n",
    "# g) Create Sales2 DataFrame from dictionary\n",
    "Sales2 = pd.DataFrame({\n",
    "    '2018': [160000, 110000, 500000, 340000, 900000]\n",
    "}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\n",
    "print(Sales2)\n",
    "\n",
    "# h) Check if Sales2 is empty\n",
    "print(Sales2.empty)\n",
    "\n",
    "# 11. Use the DataFrame Sales to do the following:\n",
    "\n",
    "# a) Append Sales2 to Sales\n",
    "Sales = Sales.append(Sales2)\n",
    "print(Sales)\n",
    "\n",
    "# b) Transpose the Sales DataFrame\n",
    "Sales = Sales.T\n",
    "print(Sales)\n",
    "\n",
    "# c) Display sales by all salespersons in 2017\n",
    "print(Sales['2017'])\n",
    "\n",
    "# d) Display sales by Madhu and Ankit in 2017 and 2018\n",
    "print(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\n",
    "\n",
    "# e) Display Shruti's sales in 2016\n",
    "print(Sales.loc['Shruti', '2016'])\n",
    "\n",
    "# f) Add data for Sumeet\n",
    "Sales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\n",
    "print(Sales)\n",
    "\n",
    "# g) Delete 2014 data from Sales\n",
    "Sales.drop('2014', axis=1, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# h) Delete data for Kinshuk\n",
    "Sales.drop('Kinshuk', axis=0, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# i) Change salesperson names\n",
    "Sales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# j) Update sales made by Shailesh in 2018\n",
    "Sales.loc['Shailesh', '2018'] = 100000\n",
    "print(Sales)\n",
    "\n",
    "# k) Write Sales DataFrame to a CSV file\n",
    "Sales.to_csv('SalesFigures.csv', header=False, index=False)\n",
    "\n",
    "# l) Read the CSV file and update row/column labels\n",
    "SalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\n",
    "SalesRetrieved.columns = Sales.columns\n",
    "SalesRetrieved.index = Sales.index\n",
    "print(SalesRetrieved)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbd8ffd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:51.651988Z",
     "iopub.status.busy": "2025-08-06T10:25:51.651656Z",
     "iopub.status.idle": "2025-08-06T10:25:51.660132Z",
     "shell.execute_reply": "2025-08-06T10:25:51.659201Z"
    },
    "id": "Mdho8ROkw6f0",
    "papermill": {
     "duration": 0.020535,
     "end_time": "2025-08-06T10:25:51.661873",
     "exception": false,
     "start_time": "2025-08-06T10:25:51.641338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 4. Sort DataFrame by multiple columns\\nimport pandas as pd\\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\\ndf.sort_values(by=['col1', 'col2'], ascending=[True, False])\\n\\n# 5. Handling missing values\\ndf.fillna(value=0)  # Fill with 0\\ndf.dropna()         # Drop rows with missing values\\n\\n# 6. Median, Standard Deviation, and Variance\\nimport numpy as np\\ndata = [1, 2, 3, 4, 5]\\nmedian = np.median(data)\\nstd_dev = np.std(data)\\nvariance = np.var(data)\\n\\n# 7. Mode\\nmode_value = df['column'].mode()\\n\\n# 8. Data Aggregation\\n# Example of aggregation using groupby\\ndf.groupby('column_name').agg({'other_column': 'mean'})\\n\\n# 9. GROUP BY in SQL\\n# Example using pandas\\ndf.groupby('column_name').agg({'other_column': 'mean'})\\n\\n# 10. Read data from MySQL to DataFrame\\nimport pymysql\\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\\ndf = pd.read_sql('SELECT * FROM table_name', conn)\\nconn.close()\\n\\n# 11. Reshaping data example\\nreshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\\n\\n# 12. Estimation\\n# Helps make informed decisions, calculate confidence intervals, and understand reliability.\\n\\n# 13. Product Table operations\\nimport pandas as pd\\n# a) Create DataFrame\\ndata = {\\n    'Item': ['TV', 'TV', 'TV', 'AC'],\\n    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\\n    'Rupees': [12000, 10000, 15000, 14000],\\n    'USD': [700, 650, 800, 750]\\n}\\ndf = pd.DataFrame(data)\\nprint(df)\\n\\n# b) Add new rows\\nnew_data = pd.DataFrame({\\n    'Item': ['AC', 'TV'],\\n    'Company': ['SAMSUNG', 'SAMSUNG'],\\n    'Rupees': [16000, 12000],\\n    'USD': [850, 700]\\n})\\ndf = df.append(new_data, ignore_index=True)\\nprint(df)\\n\\n# c) Max price of LG TV\\nmax_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\\nprint(max_price_LG)\\n\\n# d) Sum of all products\\ntotal_sum = df['Rupees'].sum()\\nprint(total_sum)\\n\\n# e) Median USD of Sony products\\nmedian_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\\nprint(median_usd_sony)\\n\\n# f) Sort data by Rupees\\ndf_sorted = df.sort_values(by='Rupees')\\nprint(df_sorted)\\n\\n# g) Transfer DataFrame to MySQL\\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\\ndf.to_sql('product_table', conn, if_exists='replace', index=False)\\nconn.close()\\n\\n# 14. Student dataset operations\\n# a) Create DataFrame\\ndata = {\\n    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\\n    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\\n    'Marks': [76, 89, 45, 88]\\n}\\ndf = pd.DataFrame(data)\\n\\n# b) Degree and max marks in each stream\\ndf.groupby('Degree')['Marks'].max()\\n\\n# c) Fill NaN with 76\\ndf.fillna(76, inplace=True)\\n\\n# d) Set index to Name\\ndf.set_index('Name', inplace=True)\\n\\n# e) Name and degree-wise average marks of each student\\ndf.groupby(['Degree']).agg({'Marks': 'mean'})\\n\\n# f) Count number of students in MBA\\nmba_count = df[df['Degree'] == 'MBA'].shape[0]\\nprint(mba_count)\\n\\n# g) Mode of marks in BCA\\nbca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\\nprint(bca_mode)\\n\\n# UCI Dataset (auto-mpg) operations\\n# 1) Load the dataset\\nautodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\\n\\n# 2) Describe the DataFrame\\nprint(autodf.describe())\\n\\n# 3) Display first 10 rows\\nprint(autodf.head(10))\\n\\n# 4) Handle missing values\\nautodf.fillna(method='ffill', inplace=True)\\nautodf.dropna(inplace=True)\\n\\n# 5) Car with max mileage\\nmax_mileage_car = autodf.loc[autodf[0].idxmax()]\\nprint(max_mileage_car)\\n\\n# 6) Average displacement based on cylinders\\navg_displacement = autodf.groupby(1)[3].mean()\\nprint(avg_displacement)\\n\\n# 7) Average number of cylinders\\navg_cylinders = autodf[1].mean()\\nprint(avg_cylinders)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Install pymysql\n",
    "# pip install pymysql\n",
    "\n",
    "# 2. Pivot vs Pivot_table\n",
    "# pivot():\n",
    "# Used for reshaping data where you specify the index, columns, and values.\n",
    "# It requires unique index/column combinations. If there are duplicates, it throws an error.\n",
    "\n",
    "# pivot_table():\n",
    "# Similar to pivot(), but it can handle duplicate entries. It allows you to specify an aggregation function (e.g., sum, mean) to aggregate data when duplicates exist.\n",
    "# Syntax: pivot_table(index=..., columns=..., values=..., aggfunc='sum')\n",
    "\n",
    "# 3. SQLAlchemy\n",
    "# SQLAlchemy is a Python library that provides an Object Relational Mapping (ORM) interface for interacting with databases.\n",
    "\n",
    "'''\n",
    "# 4. Sort DataFrame by multiple columns\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\n",
    "df.sort_values(by=['col1', 'col2'], ascending=[True, False])\n",
    "\n",
    "# 5. Handling missing values\n",
    "df.fillna(value=0)  # Fill with 0\n",
    "df.dropna()         # Drop rows with missing values\n",
    "\n",
    "# 6. Median, Standard Deviation, and Variance\n",
    "import numpy as np\n",
    "data = [1, 2, 3, 4, 5]\n",
    "median = np.median(data)\n",
    "std_dev = np.std(data)\n",
    "variance = np.var(data)\n",
    "\n",
    "# 7. Mode\n",
    "mode_value = df['column'].mode()\n",
    "\n",
    "# 8. Data Aggregation\n",
    "# Example of aggregation using groupby\n",
    "df.groupby('column_name').agg({'other_column': 'mean'})\n",
    "\n",
    "# 9. GROUP BY in SQL\n",
    "# Example using pandas\n",
    "df.groupby('column_name').agg({'other_column': 'mean'})\n",
    "\n",
    "# 10. Read data from MySQL to DataFrame\n",
    "import pymysql\n",
    "conn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\n",
    "df = pd.read_sql('SELECT * FROM table_name', conn)\n",
    "conn.close()\n",
    "\n",
    "# 11. Reshaping data example\n",
    "reshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\n",
    "\n",
    "# 12. Estimation\n",
    "# Helps make informed decisions, calculate confidence intervals, and understand reliability.\n",
    "\n",
    "# 13. Product Table operations\n",
    "import pandas as pd\n",
    "# a) Create DataFrame\n",
    "data = {\n",
    "    'Item': ['TV', 'TV', 'TV', 'AC'],\n",
    "    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\n",
    "    'Rupees': [12000, 10000, 15000, 14000],\n",
    "    'USD': [700, 650, 800, 750]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# b) Add new rows\n",
    "new_data = pd.DataFrame({\n",
    "    'Item': ['AC', 'TV'],\n",
    "    'Company': ['SAMSUNG', 'SAMSUNG'],\n",
    "    'Rupees': [16000, 12000],\n",
    "    'USD': [850, 700]\n",
    "})\n",
    "df = df.append(new_data, ignore_index=True)\n",
    "print(df)\n",
    "\n",
    "# c) Max price of LG TV\n",
    "max_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\n",
    "print(max_price_LG)\n",
    "\n",
    "# d) Sum of all products\n",
    "total_sum = df['Rupees'].sum()\n",
    "print(total_sum)\n",
    "\n",
    "# e) Median USD of Sony products\n",
    "median_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\n",
    "print(median_usd_sony)\n",
    "\n",
    "# f) Sort data by Rupees\n",
    "df_sorted = df.sort_values(by='Rupees')\n",
    "print(df_sorted)\n",
    "\n",
    "# g) Transfer DataFrame to MySQL\n",
    "conn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\n",
    "df.to_sql('product_table', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "# 14. Student dataset operations\n",
    "# a) Create DataFrame\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\n",
    "    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\n",
    "    'Marks': [76, 89, 45, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# b) Degree and max marks in each stream\n",
    "df.groupby('Degree')['Marks'].max()\n",
    "\n",
    "# c) Fill NaN with 76\n",
    "df.fillna(76, inplace=True)\n",
    "\n",
    "# d) Set index to Name\n",
    "df.set_index('Name', inplace=True)\n",
    "\n",
    "# e) Name and degree-wise average marks of each student\n",
    "df.groupby(['Degree']).agg({'Marks': 'mean'})\n",
    "\n",
    "# f) Count number of students in MBA\n",
    "mba_count = df[df['Degree'] == 'MBA'].shape[0]\n",
    "print(mba_count)\n",
    "\n",
    "# g) Mode of marks in BCA\n",
    "bca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\n",
    "print(bca_mode)\n",
    "\n",
    "# UCI Dataset (auto-mpg) operations\n",
    "# 1) Load the dataset\n",
    "autodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\n",
    "\n",
    "# 2) Describe the DataFrame\n",
    "print(autodf.describe())\n",
    "\n",
    "# 3) Display first 10 rows\n",
    "print(autodf.head(10))\n",
    "\n",
    "# 4) Handle missing values\n",
    "autodf.fillna(method='ffill', inplace=True)\n",
    "autodf.dropna(inplace=True)\n",
    "\n",
    "# 5) Car with max mileage\n",
    "max_mileage_car = autodf.loc[autodf[0].idxmax()]\n",
    "print(max_mileage_car)\n",
    "\n",
    "# 6) Average displacement based on cylinders\n",
    "avg_displacement = autodf.groupby(1)[3].mean()\n",
    "print(avg_displacement)\n",
    "\n",
    "# 7) Average number of cylinders\n",
    "avg_cylinders = autodf[1].mean()\n",
    "print(avg_cylinders)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe44b2",
   "metadata": {
    "papermill": {
     "duration": 0.009065,
     "end_time": "2025-08-06T10:25:51.681699",
     "exception": false,
     "start_time": "2025-08-06T10:25:51.672634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATASET 1 FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88902340",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:51.701165Z",
     "iopub.status.busy": "2025-08-06T10:25:51.700849Z",
     "iopub.status.idle": "2025-08-06T10:25:53.749966Z",
     "shell.execute_reply": "2025-08-06T10:25:53.748754Z"
    },
    "papermill": {
     "duration": 2.061081,
     "end_time": "2025-08-06T10:25:53.751789",
     "exception": false,
     "start_time": "2025-08-06T10:25:51.690708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfe9fbdb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:53.771258Z",
     "iopub.status.busy": "2025-08-06T10:25:53.770806Z",
     "iopub.status.idle": "2025-08-06T10:25:53.799415Z",
     "shell.execute_reply": "2025-08-06T10:25:53.798438Z"
    },
    "papermill": {
     "duration": 0.040654,
     "end_time": "2025-08-06T10:25:53.801624",
     "exception": false,
     "start_time": "2025-08-06T10:25:53.760970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Dataset ---\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/student-performance/student/student-mat.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\\n\")\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eac86ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:53.821934Z",
     "iopub.status.busy": "2025-08-06T10:25:53.821614Z",
     "iopub.status.idle": "2025-08-06T10:25:53.838606Z",
     "shell.execute_reply": "2025-08-06T10:25:53.837739Z"
    },
    "papermill": {
     "duration": 0.028361,
     "end_time": "2025-08-06T10:25:53.839896",
     "exception": false,
     "start_time": "2025-08-06T10:25:53.811535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Constant Method: []\n",
      "Dataset shape after Constant Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Constant Method Filtering ---\n",
    "\n",
    "constant_columns = [col for col in df_original.columns if df_original[col].nunique() == 1]\n",
    "df_constant_filtered = df_original.drop(columns=constant_columns)\n",
    "print(f\"Columns removed by Constant Method: {constant_columns}\")\n",
    "print(f\"Dataset shape after Constant Method filtering: {df_constant_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "800ca8df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:53.859937Z",
     "iopub.status.busy": "2025-08-06T10:25:53.859594Z",
     "iopub.status.idle": "2025-08-06T10:25:53.869922Z",
     "shell.execute_reply": "2025-08-06T10:25:53.868746Z"
    },
    "papermill": {
     "duration": 0.022155,
     "end_time": "2025-08-06T10:25:53.871630",
     "exception": false,
     "start_time": "2025-08-06T10:25:53.849475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Quasi-Constant Method (threshold=99.0%): []\n",
      "Dataset shape after Quasi-Constant Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Quasi-Constant Method Filtering ---\n",
    "\n",
    "threshold = 0.99\n",
    "quasi_constant_columns = []\n",
    "for col in df_original.columns:\n",
    "    # Calculate the most frequent value's proportion\n",
    "    most_frequent_proportion = df_original[col].value_counts(normalize=True).max()\n",
    "    if most_frequent_proportion > threshold:\n",
    "        quasi_constant_columns.append(col)\n",
    "\n",
    "df_quasi_constant_filtered = df_original.drop(columns=quasi_constant_columns)\n",
    "print(f\"Columns removed by Quasi-Constant Method (threshold={threshold*100}%): {quasi_constant_columns}\")\n",
    "print(f\"Dataset shape after Quasi-Constant Method filtering: {df_quasi_constant_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f3d87c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:53.891670Z",
     "iopub.status.busy": "2025-08-06T10:25:53.891329Z",
     "iopub.status.idle": "2025-08-06T10:25:53.901850Z",
     "shell.execute_reply": "2025-08-06T10:25:53.900828Z"
    },
    "papermill": {
     "duration": 0.022212,
     "end_time": "2025-08-06T10:25:53.903239",
     "exception": false,
     "start_time": "2025-08-06T10:25:53.881027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Correlation Method (threshold=0.9): []\n",
      "Dataset shape after Correlation Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Correlation Filtering ---\n",
    "\n",
    "# Select only numerical columns for correlation calculation\n",
    "numerical_df = df_original.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than a threshold (e.g., 0.9)\n",
    "correlation_threshold = 0.9\n",
    "to_drop_high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "# Create a new DataFrame with non-numerical columns and the filtered numerical columns\n",
    "df_correlation_filtered = df_original.drop(columns=to_drop_high_corr)\n",
    "print(f\"Columns removed by Correlation Method (threshold={correlation_threshold}): {to_drop_high_corr}\")\n",
    "print(f\"Dataset shape after Correlation Method filtering: {df_correlation_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b9aac5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:53.923216Z",
     "iopub.status.busy": "2025-08-06T10:25:53.922889Z",
     "iopub.status.idle": "2025-08-06T10:25:53.931965Z",
     "shell.execute_reply": "2025-08-06T10:25:53.931019Z"
    },
    "papermill": {
     "duration": 0.020843,
     "end_time": "2025-08-06T10:25:53.933566",
     "exception": false,
     "start_time": "2025-08-06T10:25:53.912723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column 'G3' not found. Skipping Mutual Information filtering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Mutual Information Filtering ---\n",
    "\n",
    "# For demonstration, let's assume 'G3' (final grade) is the target variable\n",
    "# If 'G3' is not present, or if it's a regression task, adjust accordingly.\n",
    "# For classification, use mutual_info_classif; for regression, use mutual_info_regression.\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_original.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df_original.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# The 'G3' column is typically numerical in this dataset, so it can be a target for regression.\n",
    "# If you want to treat it as classification (e.g., pass/fail), you might need to discretize it.\n",
    "target_column = 'G3'\n",
    "\n",
    "if target_column in df_original.columns:\n",
    "    X = df_original.drop(columns=[target_column])\n",
    "    y = df_original[target_column]\n",
    "\n",
    "    # Encode categorical features for mutual information calculation\n",
    "    X_encoded = X.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in X_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "\n",
    "    # Calculate mutual information. Use mutual_info_regression for numerical target.\n",
    "    # If your target is categorical, use mutual_info_classif.\n",
    "    mi_scores = mutual_info_regression(X_encoded, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    print(\"Mutual Information Scores:\")\n",
    "    print(mi_scores)\n",
    "\n",
    "    # You can set a threshold to select features, e.g., keep features with MI score > 0.05\n",
    "    mi_threshold = 0.05\n",
    "    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n",
    "    df_mi_filtered = df_original[selected_features_mi + [target_column]] # Keep target column\n",
    "    print(f\"\\nFeatures selected by Mutual Information (threshold={mi_threshold}): {selected_features_mi}\")\n",
    "    print(f\"Dataset shape after Mutual Information filtering: {df_mi_filtered.shape}\\n\")\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' not found. Skipping Mutual Information filtering.\\n\")\n",
    "    df_mi_filtered = df_original.copy() # No filtering applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df063fe9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.023221Z",
     "iopub.status.busy": "2025-08-06T10:25:54.022879Z",
     "iopub.status.idle": "2025-08-06T10:25:54.073397Z",
     "shell.execute_reply": "2025-08-06T10:25:54.072403Z"
    },
    "papermill": {
     "duration": 0.062673,
     "end_time": "2025-08-06T10:25:54.075111",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.012438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 395\n",
      "Number of rows after removing duplicate rows: 395\n",
      "Rows removed: 0\n",
      "\n",
      "Columns removed by Duplicate Columns Method: []\n",
      "Dataset shape after Duplicate Columns filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Duplicate Methods Filtering ---\n",
    "\n",
    "df_no_duplicate_rows = df_original.drop_duplicates()\n",
    "print(f\"Original number of rows: {df_original.shape[0]}\")\n",
    "print(f\"Number of rows after removing duplicate rows: {df_no_duplicate_rows.shape[0]}\")\n",
    "print(f\"Rows removed: {df_original.shape[0] - df_no_duplicate_rows.shape[0]}\\n\")\n",
    "\n",
    "# Remove duplicate columns\n",
    "# Transpose the DataFrame to treat columns as rows, then drop duplicates, then transpose back.\n",
    "df_transposed = df_original.T\n",
    "df_no_duplicate_cols_transposed = df_transposed.drop_duplicates()\n",
    "df_duplicate_filtered = df_no_duplicate_cols_transposed.T\n",
    "\n",
    "# Identify columns that were duplicates and removed\n",
    "original_cols = set(df_original.columns)\n",
    "filtered_cols = set(df_duplicate_filtered.columns)\n",
    "removed_duplicate_columns = list(original_cols - filtered_cols)\n",
    "\n",
    "print(f\"Columns removed by Duplicate Columns Method: {removed_duplicate_columns}\")\n",
    "print(f\"Dataset shape after Duplicate Columns filtering: {df_duplicate_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7c251a",
   "metadata": {
    "papermill": {
     "duration": 0.010413,
     "end_time": "2025-08-06T10:25:54.095403",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.084990",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATASET 2 FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa2bbc26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.116729Z",
     "iopub.status.busy": "2025-08-06T10:25:54.116415Z",
     "iopub.status.idle": "2025-08-06T10:25:54.120733Z",
     "shell.execute_reply": "2025-08-06T10:25:54.120017Z"
    },
    "papermill": {
     "duration": 0.016328,
     "end_time": "2025-08-06T10:25:54.122148",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.105820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85840ea3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.144118Z",
     "iopub.status.busy": "2025-08-06T10:25:54.143785Z",
     "iopub.status.idle": "2025-08-06T10:25:54.175714Z",
     "shell.execute_reply": "2025-08-06T10:25:54.174698Z"
    },
    "papermill": {
     "duration": 0.045339,
     "end_time": "2025-08-06T10:25:54.177303",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.131964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (477, 198)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Dataset ---\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/drug-induced-autoimmunity-prediction/DIA_trainingset_RDKit_descriptors.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\\n\")\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53812213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.199031Z",
     "iopub.status.busy": "2025-08-06T10:25:54.198679Z",
     "iopub.status.idle": "2025-08-06T10:25:54.225136Z",
     "shell.execute_reply": "2025-08-06T10:25:54.224069Z"
    },
    "papermill": {
     "duration": 0.039092,
     "end_time": "2025-08-06T10:25:54.226914",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.187822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after Constant Method filtering: (477, 181)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Constant Method Filtering ---\n",
    "\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_columns)\n",
    "print(f\"Dataset shape after Constant Method filtering: {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40959d2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.254060Z",
     "iopub.status.busy": "2025-08-06T10:25:54.253518Z",
     "iopub.status.idle": "2025-08-06T10:25:54.343028Z",
     "shell.execute_reply": "2025-08-06T10:25:54.341883Z"
    },
    "papermill": {
     "duration": 0.10812,
     "end_time": "2025-08-06T10:25:54.344872",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.236752",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Quasi-Constant Method (>99.0% same value): ['fr_C_S', 'fr_aldehyde', 'fr_amidine', 'fr_azo', 'fr_hdrzone', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_term_acetylene', 'fr_tetrazole']\n",
      "Dataset shape after Quasi-Constant Method filtering: (477, 171)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Quasi-Constant Method Filtering ---\n",
    "\n",
    "threshold = 0.99\n",
    "quasi_constant_columns = [col for col in df.columns \n",
    "                          if df[col].value_counts(normalize=True).max() > threshold]\n",
    "df = df.drop(columns=quasi_constant_columns)\n",
    "print(f\"Columns removed by Quasi-Constant Method (>{threshold*100}% same value): {quasi_constant_columns}\")\n",
    "print(f\"Dataset shape after Quasi-Constant Method filtering: {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7479c399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.366867Z",
     "iopub.status.busy": "2025-08-06T10:25:54.366428Z",
     "iopub.status.idle": "2025-08-06T10:25:54.472412Z",
     "shell.execute_reply": "2025-08-06T10:25:54.471083Z"
    },
    "papermill": {
     "duration": 0.11913,
     "end_time": "2025-08-06T10:25:54.474213",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.355083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Correlation Method (>0.9): ['Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'ExactMolWt', 'HeavyAtomCount', 'HeavyAtomMolWt', 'Kappa1', 'Kappa2', 'LabuteASA', 'MaxEStateIndex', 'MinAbsPartialCharge', 'MinPartialCharge', 'MolMR', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumSaturatedCarbocycles', 'NumSaturatedRings', 'NumValenceElectrons', 'SlogP_VSA6', 'TPSA', 'fr_Al_OH_noTert', 'fr_COO', 'fr_COO2', 'fr_C_O_noCOO', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitrile', 'fr_phenol', 'fr_phenol_noOrthoHbond']\n",
      "Dataset shape after Correlation Method filtering: (477, 132)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Correlation Filtering ---\n",
    "\n",
    "numerical_df = df.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "correlation_threshold = 0.9\n",
    "to_drop_high_corr = [\n",
    "    column for column in upper_tri.columns\n",
    "    if any(upper_tri[column].fillna(0) > correlation_threshold)  # Avoid NaN warning\n",
    "]\n",
    "df = df.drop(columns=to_drop_high_corr)\n",
    "\n",
    "print(f\"Columns removed by Correlation Method (>{correlation_threshold}): {to_drop_high_corr}\")\n",
    "print(f\"Dataset shape after Correlation Method filtering: {df.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb0c5bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.495830Z",
     "iopub.status.busy": "2025-08-06T10:25:54.495439Z",
     "iopub.status.idle": "2025-08-06T10:25:54.946319Z",
     "shell.execute_reply": "2025-08-06T10:25:54.945176Z"
    },
    "papermill": {
     "duration": 0.463898,
     "end_time": "2025-08-06T10:25:54.948314",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.484416",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores:\n",
      "SMR_VSA10            0.099493\n",
      "MaxPartialCharge     0.089871\n",
      "EState_VSA5          0.080970\n",
      "PEOE_VSA7            0.065955\n",
      "fr_C_O               0.058564\n",
      "                       ...   \n",
      "fr_alkyl_halide      0.000000\n",
      "fr_allylic_oxid      0.000000\n",
      "fr_epoxide           0.000000\n",
      "fr_ester             0.000000\n",
      "fr_Ndealkylation1    0.000000\n",
      "Name: MI Scores, Length: 131, dtype: float64\n",
      "\n",
      "Features selected by Mutual Information (>0.05): ['SMR_VSA10', 'MaxPartialCharge', 'EState_VSA5', 'PEOE_VSA7', 'fr_C_O', 'SMR_VSA6', 'PEOE_VSA9', 'fr_ketone', 'EState_VSA3', 'SlogP_VSA3', 'SMR_VSA5']\n",
      "Dataset shape after Mutual Information filtering: (477, 12)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Mutual Information Filtering ---\n",
    "\n",
    "# Replace 'Target' with your actual target column name (e.g., 'label', 'class', etc.)\n",
    "target_column = 'Label'  # UPDATE THIS LINE if your column has a different name\n",
    "\n",
    "if target_column in df.columns:\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "    X_encoded = X.copy()\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "\n",
    "    # Use classification or regression based on data type of target\n",
    "    if y.nunique() <= 10 and y.dtype in ['int', 'object']:  # assume classification\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        mi_scores = mutual_info_classif(X_encoded, y, discrete_features='auto')\n",
    "    else:  # assume regression\n",
    "        mi_scores = mutual_info_regression(X_encoded, y)\n",
    "\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns, name=\"MI Scores\").sort_values(ascending=False)\n",
    "    print(\"Mutual Information Scores:\")\n",
    "    print(mi_scores)\n",
    "\n",
    "    mi_threshold = 0.05\n",
    "    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n",
    "    df = df[selected_features_mi + [target_column]]\n",
    "\n",
    "    print(f\"\\nFeatures selected by Mutual Information (>{mi_threshold}): {selected_features_mi}\")\n",
    "    print(f\"Dataset shape after Mutual Information filtering: {df.shape}\\n\")\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' not found. Skipping MI filtering.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9e9c03d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:54.979687Z",
     "iopub.status.busy": "2025-08-06T10:25:54.979078Z",
     "iopub.status.idle": "2025-08-06T10:25:55.031902Z",
     "shell.execute_reply": "2025-08-06T10:25:55.029172Z"
    },
    "papermill": {
     "duration": 0.066822,
     "end_time": "2025-08-06T10:25:55.034143",
     "exception": false,
     "start_time": "2025-08-06T10:25:54.967321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 4 duplicate rows.\n",
      "Removed 0 duplicate columns.\n",
      "Final dataset shape: (473, 12)\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Duplicate Filtering ---\n",
    "\n",
    "rows_before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "rows_after = df.shape[0]\n",
    "\n",
    "print(f\"Removed {rows_before - rows_after} duplicate rows.\")\n",
    "\n",
    "cols_before = df.shape[1]\n",
    "df = df.T.drop_duplicates().T\n",
    "cols_after = df.shape[1]\n",
    "\n",
    "print(f\"Removed {cols_before - cols_after} duplicate columns.\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8209ad8",
   "metadata": {
    "papermill": {
     "duration": 0.010035,
     "end_time": "2025-08-06T10:25:55.055017",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.044982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Titanic Dataset Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27caee37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.080128Z",
     "iopub.status.busy": "2025-08-06T10:25:55.079684Z",
     "iopub.status.idle": "2025-08-06T10:25:55.085794Z",
     "shell.execute_reply": "2025-08-06T10:25:55.084445Z"
    },
    "papermill": {
     "duration": 0.020118,
     "end_time": "2025-08-06T10:25:55.087747",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.067629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6adae1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.110036Z",
     "iopub.status.busy": "2025-08-06T10:25:55.109622Z",
     "iopub.status.idle": "2025-08-06T10:25:55.140070Z",
     "shell.execute_reply": "2025-08-06T10:25:55.138985Z"
    },
    "papermill": {
     "duration": 0.043169,
     "end_time": "2025-08-06T10:25:55.141826",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.098657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (183, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 183 entries, 1 to 889\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  183 non-null    int64  \n",
      " 1   Survived     183 non-null    int64  \n",
      " 2   Pclass       183 non-null    int64  \n",
      " 3   Name         183 non-null    int64  \n",
      " 4   Sex          183 non-null    int64  \n",
      " 5   Age          183 non-null    float64\n",
      " 6   SibSp        183 non-null    int64  \n",
      " 7   Parch        183 non-null    int64  \n",
      " 8   Ticket       183 non-null    int64  \n",
      " 9   Fare         183 non-null    float64\n",
      " 10  Cabin        183 non-null    int64  \n",
      " 11  Embarked     183 non-null    int64  \n",
      "dtypes: float64(2), int64(10)\n",
      "memory usage: 18.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('/kaggle/input/titanic/titanic_train.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "target = 'Survived'\n",
    "print(df.info())  # Shows column names, data types, and non-null counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dcc1201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.163994Z",
     "iopub.status.busy": "2025-08-06T10:25:55.163707Z",
     "iopub.status.idle": "2025-08-06T10:25:55.172132Z",
     "shell.execute_reply": "2025-08-06T10:25:55.170865Z"
    },
    "papermill": {
     "duration": 0.021597,
     "end_time": "2025-08-06T10:25:55.173698",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.152101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing constant features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 1. Constant Features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_features)\n",
    "print(\"After removing constant features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e5738f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.195568Z",
     "iopub.status.busy": "2025-08-06T10:25:55.195137Z",
     "iopub.status.idle": "2025-08-06T10:25:55.213224Z",
     "shell.execute_reply": "2025-08-06T10:25:55.212058Z"
    },
    "papermill": {
     "duration": 0.031659,
     "end_time": "2025-08-06T10:25:55.215700",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.184041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing quasi-constant features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 2. Quasi-Constant Features\n",
    "quasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\n",
    "df = df.drop(columns=quasi_constant)\n",
    "print(\"After removing quasi-constant features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "195452f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.243485Z",
     "iopub.status.busy": "2025-08-06T10:25:55.243115Z",
     "iopub.status.idle": "2025-08-06T10:25:55.271922Z",
     "shell.execute_reply": "2025-08-06T10:25:55.270902Z"
    },
    "papermill": {
     "duration": 0.041727,
     "end_time": "2025-08-06T10:25:55.273630",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.231903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicate features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 3. Duplicate Features\n",
    "df = df.T.drop_duplicates().T\n",
    "print(\"After removing duplicate features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "476d84ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.302372Z",
     "iopub.status.busy": "2025-08-06T10:25:55.301358Z",
     "iopub.status.idle": "2025-08-06T10:25:55.314036Z",
     "shell.execute_reply": "2025-08-06T10:25:55.312675Z"
    },
    "papermill": {
     "duration": 0.025883,
     "end_time": "2025-08-06T10:25:55.316061",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.290178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing highly correlated features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(\"After removing highly correlated features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f23777f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.341377Z",
     "iopub.status.busy": "2025-08-06T10:25:55.340966Z",
     "iopub.status.idle": "2025-08-06T10:25:55.382726Z",
     "shell.execute_reply": "2025-08-06T10:25:55.381446Z"
    },
    "papermill": {
     "duration": 0.056749,
     "end_time": "2025-08-06T10:25:55.384301",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.327552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top MI Features:\n",
      " Sex            0.118070\n",
      "Age            0.065896\n",
      "Ticket         0.045218\n",
      "Pclass         0.037608\n",
      "Parch          0.032625\n",
      "Embarked       0.026973\n",
      "Fare           0.024043\n",
      "SibSp          0.023452\n",
      "PassengerId    0.000000\n",
      "Name           0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 5. Mutual Information (Classification)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "print(\"\\nTop MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a293ec6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.408025Z",
     "iopub.status.busy": "2025-08-06T10:25:55.407118Z",
     "iopub.status.idle": "2025-08-06T10:25:55.448775Z",
     "shell.execute_reply": "2025-08-06T10:25:55.447436Z"
    },
    "papermill": {
     "duration": 0.055378,
     "end_time": "2025-08-06T10:25:55.450666",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.395288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Chi2 Features:\n",
      " Sex            24.945263\n",
      "Age             1.046261\n",
      "Name            0.690274\n",
      "Embarked        0.664543\n",
      "SibSp           0.612932\n",
      "PassengerId     0.608432\n",
      "Fare            0.474251\n",
      "Pclass          0.150679\n",
      "Parch           0.030309\n",
      "Ticket          0.015566\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Chi-Square\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi2_scores = chi2(X_chi, y)[0]\n",
    "print(\"\\nTop Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c9000c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.473534Z",
     "iopub.status.busy": "2025-08-06T10:25:55.473212Z",
     "iopub.status.idle": "2025-08-06T10:25:55.485041Z",
     "shell.execute_reply": "2025-08-06T10:25:55.483785Z"
    },
    "papermill": {
     "duration": 0.025449,
     "end_time": "2025-08-06T10:25:55.486670",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.461221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ANOVA Features:\n",
      " Sex            71.605923\n",
      "Age            12.491639\n",
      "Name            4.144636\n",
      "PassengerId     4.081191\n",
      "Fare            3.321597\n",
      "SibSp           2.070439\n",
      "Embarked        1.863299\n",
      "Pclass          0.216220\n",
      "Parch           0.100716\n",
      "Ticket          0.093873\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. ANOVA (Classification)\n",
    "anova = f_classif(X, y)\n",
    "print(\"\\nTop ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c8ea92",
   "metadata": {
    "papermill": {
     "duration": 0.010024,
     "end_time": "2025-08-06T10:25:55.508082",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.498058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **House Price Dataset Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "91d4c078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.530481Z",
     "iopub.status.busy": "2025-08-06T10:25:55.530062Z",
     "iopub.status.idle": "2025-08-06T10:25:55.535954Z",
     "shell.execute_reply": "2025-08-06T10:25:55.534930Z"
    },
    "papermill": {
     "duration": 0.019363,
     "end_time": "2025-08-06T10:25:55.537871",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.518508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1af2dc6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.560494Z",
     "iopub.status.busy": "2025-08-06T10:25:55.560126Z",
     "iopub.status.idle": "2025-08-06T10:25:55.603094Z",
     "shell.execute_reply": "2025-08-06T10:25:55.602008Z"
    },
    "papermill": {
     "duration": 0.055911,
     "end_time": "2025-08-06T10:25:55.604856",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.548945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing features: (1460, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/house-price-prediction/house_price_train.csv')\n",
    "\n",
    "df = df.dropna(axis=1, how='any')\n",
    "\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "target = 'SalePrice' if 'SalePrice' in df.columns else df.columns[-1]  \n",
    "print(\"Before removing features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f9f0194",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.628520Z",
     "iopub.status.busy": "2025-08-06T10:25:55.627616Z",
     "iopub.status.idle": "2025-08-06T10:25:55.639946Z",
     "shell.execute_reply": "2025-08-06T10:25:55.638966Z"
    },
    "papermill": {
     "duration": 0.025984,
     "end_time": "2025-08-06T10:25:55.641682",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.615698",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 35)\n"
     ]
    }
   ],
   "source": [
    "# 1. Constant Features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_features)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f78bf210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.670136Z",
     "iopub.status.busy": "2025-08-06T10:25:55.669756Z",
     "iopub.status.idle": "2025-08-06T10:25:55.692991Z",
     "shell.execute_reply": "2025-08-06T10:25:55.691815Z"
    },
    "papermill": {
     "duration": 0.036607,
     "end_time": "2025-08-06T10:25:55.694628",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.658021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 32)\n"
     ]
    }
   ],
   "source": [
    "# 2. Quasi-Constant Features\n",
    "quasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\n",
    "df = df.drop(columns=quasi_constant)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4d0a0feb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.718550Z",
     "iopub.status.busy": "2025-08-06T10:25:55.718115Z",
     "iopub.status.idle": "2025-08-06T10:25:55.884252Z",
     "shell.execute_reply": "2025-08-06T10:25:55.883272Z"
    },
    "papermill": {
     "duration": 0.179504,
     "end_time": "2025-08-06T10:25:55.886003",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.706499",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 32)\n"
     ]
    }
   ],
   "source": [
    "# 3. Duplicate Features\n",
    "df = df.T.drop_duplicates().T\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df3d0df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.911288Z",
     "iopub.status.busy": "2025-08-06T10:25:55.910951Z",
     "iopub.status.idle": "2025-08-06T10:25:55.935133Z",
     "shell.execute_reply": "2025-08-06T10:25:55.933996Z"
    },
    "papermill": {
     "duration": 0.037647,
     "end_time": "2025-08-06T10:25:55.936749",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.899102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 32)\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b83a5b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:55.959668Z",
     "iopub.status.busy": "2025-08-06T10:25:55.958905Z",
     "iopub.status.idle": "2025-08-06T10:25:56.254542Z",
     "shell.execute_reply": "2025-08-06T10:25:56.253490Z"
    },
    "papermill": {
     "duration": 0.308463,
     "end_time": "2025-08-06T10:25:56.256061",
     "exception": false,
     "start_time": "2025-08-06T10:25:55.947598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top MI Features:\n",
      " OverallQual     0.499675\n",
      "GarageCars      0.383546\n",
      "GrLivArea       0.379345\n",
      "GarageArea      0.282460\n",
      "YearBuilt       0.272426\n",
      "TotalBsmtSF     0.260262\n",
      "FullBath        0.259013\n",
      "1stFlrSF        0.249987\n",
      "YearRemodAdd    0.215437\n",
      "TotRmsAbvGrd    0.178211\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Set target column\n",
    "target = 'SalePrice' if 'SalePrice' in df.columns else df.columns[-1]\n",
    "\n",
    "# Make sure target is still in the dataframe after cleaning\n",
    "if target not in df.columns:\n",
    "    print(f\"Target column '{target}' not found in DataFrame after preprocessing.\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "else:\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Mutual Information\n",
    "    from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    print(\"Top MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "24a679b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:56.278905Z",
     "iopub.status.busy": "2025-08-06T10:25:56.278596Z",
     "iopub.status.idle": "2025-08-06T10:25:56.302642Z",
     "shell.execute_reply": "2025-08-06T10:25:56.301647Z"
    },
    "papermill": {
     "duration": 0.037235,
     "end_time": "2025-08-06T10:25:56.304283",
     "exception": false,
     "start_time": "2025-08-06T10:25:56.267048",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Chi2 Features:\n",
      " MiscVal          403.440783\n",
      "BsmtHalfBath     298.612323\n",
      "ScreenPorch      284.644720\n",
      "HalfBath         260.352308\n",
      "BsmtFinSF2       250.259651\n",
      "2ndFlrSF         225.125456\n",
      "Fireplaces       190.169091\n",
      "YearRemodAdd     179.213938\n",
      "EnclosedPorch    179.142328\n",
      "MSSubClass       174.897301\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Chi-Square (scaled)\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi2_scores = chi2(X_chi, y)[0]\n",
    "print(\"Top Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59d70d56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:56.327583Z",
     "iopub.status.busy": "2025-08-06T10:25:56.327175Z",
     "iopub.status.idle": "2025-08-06T10:25:56.340249Z",
     "shell.execute_reply": "2025-08-06T10:25:56.339364Z"
    },
    "papermill": {
     "duration": 0.026396,
     "end_time": "2025-08-06T10:25:56.341772",
     "exception": false,
     "start_time": "2025-08-06T10:25:56.315376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ANOVA Features:\n",
      " OverallQual     2436.770591\n",
      "GrLivArea       1470.585010\n",
      "GarageCars      1013.705666\n",
      "GarageArea       926.951287\n",
      "TotalBsmtSF      880.341282\n",
      "1stFlrSF         845.524488\n",
      "FullBath         668.430296\n",
      "TotRmsAbvGrd     580.762801\n",
      "YearBuilt        548.665821\n",
      "YearRemodAdd     504.714855\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. ANOVA (Regression)\n",
    "anova = f_regression(X, y)\n",
    "print(\"Top ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ee176",
   "metadata": {
    "papermill": {
     "duration": 0.010579,
     "end_time": "2025-08-06T10:25:56.363476",
     "exception": false,
     "start_time": "2025-08-06T10:25:56.352897",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Santander Customer Satisfaction Dataset Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8921779",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:56.386513Z",
     "iopub.status.busy": "2025-08-06T10:25:56.386211Z",
     "iopub.status.idle": "2025-08-06T10:25:59.220042Z",
     "shell.execute_reply": "2025-08-06T10:25:59.219296Z"
    },
    "papermill": {
     "duration": 2.847409,
     "end_time": "2025-08-06T10:25:59.221679",
     "exception": false,
     "start_time": "2025-08-06T10:25:56.374270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/kaggle/input/santander-customer-satisfaction-prediction/Santander Customer Satisfaction_train.csv')\n",
    "df = df.dropna()\n",
    "target = 'TARGET' if 'TARGET' in df.columns else df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53af8ac5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:59.244925Z",
     "iopub.status.busy": "2025-08-06T10:25:59.244639Z",
     "iopub.status.idle": "2025-08-06T10:25:59.549252Z",
     "shell.execute_reply": "2025-08-06T10:25:59.548116Z"
    },
    "papermill": {
     "duration": 0.3182,
     "end_time": "2025-08-06T10:25:59.550946",
     "exception": false,
     "start_time": "2025-08-06T10:25:59.232746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 337)\n"
     ]
    }
   ],
   "source": [
    "# 1. Constant Features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_features)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "73a1e0ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:59.574282Z",
     "iopub.status.busy": "2025-08-06T10:25:59.573942Z",
     "iopub.status.idle": "2025-08-06T10:25:59.936531Z",
     "shell.execute_reply": "2025-08-06T10:25:59.935110Z"
    },
    "papermill": {
     "duration": 0.375997,
     "end_time": "2025-08-06T10:25:59.938147",
     "exception": false,
     "start_time": "2025-08-06T10:25:59.562150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 138)\n"
     ]
    }
   ],
   "source": [
    "# 2. Quasi-Constant Features\n",
    "quasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\n",
    "df = df.drop(columns=quasi_constant)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a60fa66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:25:59.962030Z",
     "iopub.status.busy": "2025-08-06T10:25:59.961752Z",
     "iopub.status.idle": "2025-08-06T10:26:12.641641Z",
     "shell.execute_reply": "2025-08-06T10:26:12.640517Z"
    },
    "papermill": {
     "duration": 12.693898,
     "end_time": "2025-08-06T10:26:12.643277",
     "exception": false,
     "start_time": "2025-08-06T10:25:59.949379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 132)\n"
     ]
    }
   ],
   "source": [
    "# 3. Duplicate Features\n",
    "df = df.T.drop_duplicates().T\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c641cd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:26:12.666091Z",
     "iopub.status.busy": "2025-08-06T10:26:12.665798Z",
     "iopub.status.idle": "2025-08-06T10:26:16.330583Z",
     "shell.execute_reply": "2025-08-06T10:26:16.329582Z"
    },
    "papermill": {
     "duration": 3.677939,
     "end_time": "2025-08-06T10:26:16.332165",
     "exception": false,
     "start_time": "2025-08-06T10:26:12.654226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 70)\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "090a75d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:26:16.355902Z",
     "iopub.status.busy": "2025-08-06T10:26:16.355586Z",
     "iopub.status.idle": "2025-08-06T10:26:45.933659Z",
     "shell.execute_reply": "2025-08-06T10:26:45.932687Z"
    },
    "papermill": {
     "duration": 29.59175,
     "end_time": "2025-08-06T10:26:45.935381",
     "exception": false,
     "start_time": "2025-08-06T10:26:16.343631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top MI Features:\n",
      " ind_var30      0.025697\n",
      "num_var42      0.024041\n",
      "ind_var5       0.023297\n",
      "ind_var5_0     0.017840\n",
      "num_var30_0    0.016959\n",
      "num_var4       0.016922\n",
      "num_var30      0.016914\n",
      "num_var39_0    0.014889\n",
      "saldo_var30    0.013756\n",
      "var15          0.013634\n",
      "dtype: float64\n",
      "(76020, 70)\n"
     ]
    }
   ],
   "source": [
    "# 5. Mutual Information (Classification)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "print(\"Top MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e26ee814",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:26:45.959400Z",
     "iopub.status.busy": "2025-08-06T10:26:45.958875Z",
     "iopub.status.idle": "2025-08-06T10:26:46.063393Z",
     "shell.execute_reply": "2025-08-06T10:26:46.062444Z"
    },
    "papermill": {
     "duration": 0.118625,
     "end_time": "2025-08-06T10:26:46.065223",
     "exception": false,
     "start_time": "2025-08-06T10:26:45.946598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Chi2 Features:\n",
      " ind_var5       468.257507\n",
      "ind_var30      455.821225\n",
      "var36          451.071536\n",
      "ind_var8_0     160.110889\n",
      "ind_var13_0    112.246032\n",
      "ind_var12_0    103.522051\n",
      "ind_var12       85.145416\n",
      "num_var42       78.642021\n",
      "ind_var24_0     66.836311\n",
      "num_var4        53.527151\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Chi-Square\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi2_scores = chi2(X_chi, y)[0]\n",
    "print(\"Top Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "006088bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:26:46.088874Z",
     "iopub.status.busy": "2025-08-06T10:26:46.088530Z",
     "iopub.status.idle": "2025-08-06T10:26:46.170461Z",
     "shell.execute_reply": "2025-08-06T10:26:46.169281Z"
    },
    "papermill": {
     "duration": 0.095685,
     "end_time": "2025-08-06T10:26:46.172221",
     "exception": false,
     "start_time": "2025-08-06T10:26:46.076536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ANOVA Features:\n",
      " ind_var30      1745.255659\n",
      "num_var30      1482.100843\n",
      "num_var42      1425.940751\n",
      "ind_var5       1418.577470\n",
      "var36           813.832521\n",
      "var15           788.508493\n",
      "num_var4        492.037964\n",
      "ind_var8_0      165.903276\n",
      "ind_var13_0     118.615826\n",
      "ind_var12_0     111.177629\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. ANOVA (Classification)\n",
    "anova = f_classif(X, y)\n",
    "print(\"Top ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0Q7oDHoKUvBtfynUFNzI3",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7973641,
     "sourceId": 12620520,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8014077,
     "sourceId": 12681452,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8018708,
     "sourceId": 12688949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8019025,
     "sourceId": 12689371,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8019036,
     "sourceId": 12689384,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 62.886611,
   "end_time": "2025-08-06T10:26:46.904546",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-06T10:25:44.017935",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
