{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b112b11",
   "metadata": {
    "id": "view-in-github",
    "papermill": {
     "duration": 0.015571,
     "end_time": "2025-08-06T10:23:52.474016",
     "exception": false,
     "start_time": "2025-08-06T10:23:52.458445",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/shrav-jally/ML_lab_sem5/blob/main/sem5_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2f0032c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:52.495626Z",
     "iopub.status.busy": "2025-08-06T10:23:52.495190Z",
     "iopub.status.idle": "2025-08-06T10:23:52.508559Z",
     "shell.execute_reply": "2025-08-06T10:23:52.507421Z"
    },
    "id": "2kKoBdcTeRYz",
    "papermill": {
     "duration": 0.025283,
     "end_time": "2025-08-06T10:23:52.510363",
     "exception": false,
     "start_time": "2025-08-06T10:23:52.485080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnumpy\\npandas\\nscikitlearn\\nmatplotlib\\ntensorflow\\nkeras\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tinyurl.com/ML-2025-26\n",
    "\n",
    "'''\n",
    "numpy\n",
    "pandas\n",
    "scikitlearn\n",
    "matplotlib\n",
    "tensorflow\n",
    "keras\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "114bde39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:52.537778Z",
     "iopub.status.busy": "2025-08-06T10:23:52.537200Z",
     "iopub.status.idle": "2025-08-06T10:23:52.543791Z",
     "shell.execute_reply": "2025-08-06T10:23:52.542867Z"
    },
    "id": "UlspcwvGgQfd",
    "papermill": {
     "duration": 0.026269,
     "end_time": "2025-08-06T10:23:52.545869",
     "exception": false,
     "start_time": "2025-08-06T10:23:52.519600",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncreating dataframe\\ninserting, deleting, modifying data\\nbasic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\\nmissing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\\nworking with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\\ndata visualization with pandas - line, bar, scatter, histograms\\nadvaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\\nperformance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "creating dataframe\n",
    "inserting, deleting, modifying data\n",
    "basic and advanced dataframe ops (head, tail, shape, info, describe, groupby, join, merge, missing data handling)\n",
    "missing data handling - dropping, filling, indentifying missing values (isnull(), notnull())\n",
    "working with categorical data (data that can only take a limited number of values) - converting to categorical data, one-hot encoding, label encoding\n",
    "data visualization with pandas - line, bar, scatter, histograms\n",
    "advaced-data analysis techniques - pandas, time series analysis, statistical modelling, machine learning\n",
    "performance optimisation - vectorised operations, data types, indexing, avoid unnecessary copies\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "474ebb96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:52.571354Z",
     "iopub.status.busy": "2025-08-06T10:23:52.571011Z",
     "iopub.status.idle": "2025-08-06T10:23:52.579139Z",
     "shell.execute_reply": "2025-08-06T10:23:52.578137Z"
    },
    "id": "FJzt_9akjels",
    "papermill": {
     "duration": 0.019801,
     "end_time": "2025-08-06T10:23:52.580774",
     "exception": false,
     "start_time": "2025-08-06T10:23:52.560973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = pd.read_csv(\\'your_dataset.csv\\') #csv (comma separated vectors)\\nprint(df.head())\\nprint(df.describe()) #summary of statistics\\nprint(df.info())\\ndf.sample() #gives random 5 rows of dataframe\\ndf.columns\\ndf[[\\'Age\\', \\'BMI\\']].head() #selecting multiple columns (passing array of feature names)\\ndf[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\\ndf.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\\n\\n#handling missing values\\nprint(df.isnull().sum())  #check for missing values\\ndf_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\\ndf_filled=df.fillna(df.mean())\\n#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\\n\\ndf.fillna(df.mean(), inplace=True)\\nprint(df.duplicated().sum()) #identifies duplicate values\\ndf_no_duplicates = df.drop_duplicates()\\ndf[\"column1\"]=df[\"column1\"].astype(float)\\n\\n#encode categorical variables\\ndf_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\\n#dummy value is assigned to label name\\n#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#working with dataframes\n",
    "\n",
    "'''\n",
    "df = pd.read_csv('your_dataset.csv') #csv (comma separated vectors)\n",
    "print(df.head())\n",
    "print(df.describe()) #summary of statistics\n",
    "print(df.info())\n",
    "df.sample() #gives random 5 rows of dataframe\n",
    "df.columns\n",
    "df[['Age', 'BMI']].head() #selecting multiple columns (passing array of feature names)\n",
    "df[0:4] #0 to 4 rows of dataset (or you can use iloc(), loc() functions)\n",
    "df.rename(columns={\"Age\":\"age\"}).head() #object.method.method - pattern is called \"chain of function calls\"\n",
    "\n",
    "#handling missing values\n",
    "print(df.isnull().sum())  #check for missing values\n",
    "df_cleaned=df.dropna()  #drop rows with missing values and place it in new variable\n",
    "df_filled=df.fillna(df.mean())\n",
    "#drop() parameters - labels (list of columns to drop), axis (default=0, 0=rows, 1=columns), columns, level (if there are multiple rows in dataframe), inplace (if false return copy, otherwise do op in place and return none)\n",
    "\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "print(df.duplicated().sum()) #identifies duplicate values\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "df[\"column1\"]=df[\"column1\"].astype(float)\n",
    "\n",
    "#encode categorical variables\n",
    "df_encode = pd.get_dummies(df, columns=[car_brand]) #to convert categorical data from car_brand to numerical data\n",
    "#dummy value is assigned to label name\n",
    "#this just means that the numbers the brand name is converted to it not interperted as 1 is greater than 0, but it tells the algorithm that both 1 and 0 are placveholders for categorical data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17b444ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:52.604669Z",
     "iopub.status.busy": "2025-08-06T10:23:52.603142Z",
     "iopub.status.idle": "2025-08-06T10:23:54.419746Z",
     "shell.execute_reply": "2025-08-06T10:23:54.418548Z"
    },
    "id": "7ZVAkw8enU2N",
    "outputId": "6c5abe3a-2758-4f39-9f1a-7e759d003660",
    "papermill": {
     "duration": 1.83067,
     "end_time": "2025-08-06T10:23:54.421554",
     "exception": false,
     "start_time": "2025-08-06T10:23:52.590884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "#Scikit_learn (estimator api) - pip install scikitlearn\n",
    "#uci ml repository - all types of machine learning datasets available\n",
    "#it provides various tools for - classification, regression, clustering, dimensionality reduction, feature selection\n",
    "#sklearn is used to build ml models, for data reading and manipulation and summarizing use pandas, numpy etc.\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39091942",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:54.441188Z",
     "iopub.status.busy": "2025-08-06T10:23:54.440721Z",
     "iopub.status.idle": "2025-08-06T10:23:55.313081Z",
     "shell.execute_reply": "2025-08-06T10:23:55.311755Z"
    },
    "id": "IgI5rXZCqKWE",
    "outputId": "102812bf-577f-40a9-9415-d2346083d875",
    "papermill": {
     "duration": 0.884784,
     "end_time": "2025-08-06T10:23:55.315363",
     "exception": false,
     "start_time": "2025-08-06T10:23:54.430579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from re import X\n",
    "from inspect import isroutine\n",
    "'''\n",
    "components of sklearn:\n",
    "\n",
    "supervised learning algorithms: SVM, DT\n",
    "cross validation: to check accuracy of supervised models on unseen data using sklearn\n",
    "unsupervised learning algorithms: clustering, factor analysis, principal component analysis to undupervised neural networks\n",
    "various datasets: IRIS\n",
    "feature extraction: sklearn for extracting images and text features\n",
    "model selection: comparing, validating parameters and models\n",
    "'''\n",
    "\n",
    "#most common steps involved in machine learning\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train the model\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d72026b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:55.336482Z",
     "iopub.status.busy": "2025-08-06T10:23:55.336075Z",
     "iopub.status.idle": "2025-08-06T10:23:55.346521Z",
     "shell.execute_reply": "2025-08-06T10:23:55.345476Z"
    },
    "id": "4DlWrtN7w7N0",
    "papermill": {
     "duration": 0.022614,
     "end_time": "2025-08-06T10:23:55.348253",
     "exception": false,
     "start_time": "2025-08-06T10:23:55.325639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 5. Create the following Series and do the specified operations:\\n\\n# a) EngAlph with 26 alphabet elements and default indices\\nimport pandas as pd\\nimport string\\n\\nEngAlph = pd.Series(list(string.ascii_uppercase))\\nprint(EngAlph)\\n\\n# b) Vowels with specific index labels and all values set to zero\\nVowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\\nprint(Vowels.empty)  # Check if it is empty\\n\\n# c) Friends from a dictionary\\nFriends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\\nprint(Friends)\\n\\n# d) MTseries as an empty series\\nMTseries = pd.Series()\\nprint(MTseries.empty)  # Check if it is empty\\n\\n# e) MonthDays from a numpy array\\nimport numpy as np\\nMonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\\nprint(MonthDays)\\n\\n# 6. Using the Series created in Question 5:\\n\\n# a) Set all values of Vowels to 10\\nVowels[:] = 10\\nprint(Vowels)\\n\\n# b) Divide all values of Vowels by 2\\nVowels /= 2\\nprint(Vowels)\\n\\n# c) Create another series Vowels1\\nVowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\\nprint(Vowels1)\\n\\n# d) Add Vowels and Vowels1 and assign to Vowels3\\nVowels3 = Vowels + Vowels1\\nprint(Vowels3)\\n\\n# e) Subtract, multiply, and divide Vowels by Vowels1\\nVowels_sub = Vowels - Vowels1\\nVowels_mul = Vowels * Vowels1\\nVowels_div = Vowels / Vowels1\\nprint(Vowels_sub, Vowels_mul, Vowels_div)\\n\\n# f) Alter labels of Vowels1\\nVowels1.index = ['A', 'E', 'I', 'O', 'U']\\nprint(Vowels1)\\n\\n# 7. Using the Series created in Question 5:\\n\\n# a) Find dimensions, size, and values\\nprint(EngAlph.shape, Vowels.size, Friends.values)\\n\\n# b) Rename MTseries as SeriesEmpty\\nSeriesEmpty = MTseries\\nprint(SeriesEmpty)\\n\\n# c) Name the index of MonthDays and Friends\\nMonthDays.index.name = 'monthno'\\nFriends.index.name = 'Fname'\\n\\n# d) Display the 3rd and 2nd value of Friends\\nprint(Friends.iloc[2], Friends.iloc[1])\\n\\n# e) Display alphabets 'e' to 'p' from EngAlph\\nprint(EngAlph['E':'P'])\\n\\n# f) Display the first 10 values of EngAlph\\nprint(EngAlph.head(10))\\n\\n# g) Display the last 10 values of EngAlph\\nprint(EngAlph.tail(10))\\n\\n# h) Display MTseries\\nprint(MTseries)\\n\\n# 8. Using the Series created in Question 5:\\n\\n# a) Display months 3 through 7 from MonthDays\\nprint(MonthDays[3:8])\\n\\n# b) Display MonthDays in reverse order\\nprint(MonthDays[::-1])\\n\\n# 9. Create the following DataFrame Sales:\\nSales = pd.DataFrame({\\n    '2014': [100.5, 150.8, 200.9, 30000, 40000],\\n    '2015': [12000, 18000, 22000, 30000, 45000],\\n    '2016': [20000, 50000, 70000, 100000, 125000],\\n    '2017': [50000, 60000, 70000, 80000, 90000]\\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\\nprint(Sales)\\n\\n# 10. Use the DataFrame Sales to do the following:\\n\\n# a) Display row labels\\nprint(Sales.index)\\n\\n# b) Display column labels\\nprint(Sales.columns)\\n\\n# c) Display data types of each column\\nprint(Sales.dtypes)\\n\\n# d) Display dimensions, shape, size, and values\\nprint(Sales.shape, Sales.size, Sales.values)\\n\\n# e) Display the last two rows\\nprint(Sales.tail(2))\\n\\n# f) Display the first two columns\\nprint(Sales.iloc[:, :2])\\n\\n# g) Create Sales2 DataFrame from dictionary\\nSales2 = pd.DataFrame({\\n    '2018': [160000, 110000, 500000, 340000, 900000]\\n}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\\nprint(Sales2)\\n\\n# h) Check if Sales2 is empty\\nprint(Sales2.empty)\\n\\n# 11. Use the DataFrame Sales to do the following:\\n\\n# a) Append Sales2 to Sales\\nSales = Sales.append(Sales2)\\nprint(Sales)\\n\\n# b) Transpose the Sales DataFrame\\nSales = Sales.T\\nprint(Sales)\\n\\n# c) Display sales by all salespersons in 2017\\nprint(Sales['2017'])\\n\\n# d) Display sales by Madhu and Ankit in 2017 and 2018\\nprint(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\\n\\n# e) Display Shruti's sales in 2016\\nprint(Sales.loc['Shruti', '2016'])\\n\\n# f) Add data for Sumeet\\nSales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\\nprint(Sales)\\n\\n# g) Delete 2014 data from Sales\\nSales.drop('2014', axis=1, inplace=True)\\nprint(Sales)\\n\\n# h) Delete data for Kinshuk\\nSales.drop('Kinshuk', axis=0, inplace=True)\\nprint(Sales)\\n\\n# i) Change salesperson names\\nSales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\\nprint(Sales)\\n\\n# j) Update sales made by Shailesh in 2018\\nSales.loc['Shailesh', '2018'] = 100000\\nprint(Sales)\\n\\n# k) Write Sales DataFrame to a CSV file\\nSales.to_csv('SalesFigures.csv', header=False, index=False)\\n\\n# l) Read the CSV file and update row/column labels\\nSalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\\nSalesRetrieved.columns = Sales.columns\\nSalesRetrieved.index = Sales.index\\nprint(SalesRetrieved)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. **Series vs 1-D array, List, Dictionary:**\n",
    "   - **Series:** A 1-D labeled array in pandas with index labels for each element.\n",
    "   - **1-D Array:** Homogeneous, no labels (NumPy array).\n",
    "   - **List:** A Python built-in collection, ordered, but no indexing.\n",
    "   - **Dictionary:** Key-value pair mapping, unordered, no numerical operations like Series.\n",
    "\n",
    "2. **DataFrame vs 2-D array:**\n",
    "   - **DataFrame:** 2-D labeled table with heterogeneous data types (columns can have different types).\n",
    "   - **2-D Array:** Homogeneous (same data type), no labels, and lacks advanced functionality like a DataFrame.\n",
    "\n",
    "3. **DataFrames and Series:**\n",
    "   - A **DataFrame** is a collection of **Series** (each column in a DataFrame is a Series).\n",
    "\n",
    "4. **Size of (i) Series, (ii) DataFrame:**\n",
    "   - **Series Size:** Number of elements (length of the Series).\n",
    "   - **DataFrame Size:** Total elements (rows × columns).\n",
    "'''\n",
    "\n",
    "'''\n",
    "# 5. Create the following Series and do the specified operations:\n",
    "\n",
    "# a) EngAlph with 26 alphabet elements and default indices\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "EngAlph = pd.Series(list(string.ascii_uppercase))\n",
    "print(EngAlph)\n",
    "\n",
    "# b) Vowels with specific index labels and all values set to zero\n",
    "Vowels = pd.Series([0]*5, index=['a', 'e', 'i', 'o', 'u'])\n",
    "print(Vowels.empty)  # Check if it is empty\n",
    "\n",
    "# c) Friends from a dictionary\n",
    "Friends = pd.Series({'John': 1, 'Alice': 2, 'Bob': 3, 'Charlie': 4, 'David': 5})\n",
    "print(Friends)\n",
    "\n",
    "# d) MTseries as an empty series\n",
    "MTseries = pd.Series()\n",
    "print(MTseries.empty)  # Check if it is empty\n",
    "\n",
    "# e) MonthDays from a numpy array\n",
    "import numpy as np\n",
    "MonthDays = pd.Series(np.array([31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]), index=np.arange(1, 13))\n",
    "print(MonthDays)\n",
    "\n",
    "# 6. Using the Series created in Question 5:\n",
    "\n",
    "# a) Set all values of Vowels to 10\n",
    "Vowels[:] = 10\n",
    "print(Vowels)\n",
    "\n",
    "# b) Divide all values of Vowels by 2\n",
    "Vowels /= 2\n",
    "print(Vowels)\n",
    "\n",
    "# c) Create another series Vowels1\n",
    "Vowels1 = pd.Series([2, 5, 6, 3, 8], index=['a', 'e', 'i', 'o', 'u'])\n",
    "print(Vowels1)\n",
    "\n",
    "# d) Add Vowels and Vowels1 and assign to Vowels3\n",
    "Vowels3 = Vowels + Vowels1\n",
    "print(Vowels3)\n",
    "\n",
    "# e) Subtract, multiply, and divide Vowels by Vowels1\n",
    "Vowels_sub = Vowels - Vowels1\n",
    "Vowels_mul = Vowels * Vowels1\n",
    "Vowels_div = Vowels / Vowels1\n",
    "print(Vowels_sub, Vowels_mul, Vowels_div)\n",
    "\n",
    "# f) Alter labels of Vowels1\n",
    "Vowels1.index = ['A', 'E', 'I', 'O', 'U']\n",
    "print(Vowels1)\n",
    "\n",
    "# 7. Using the Series created in Question 5:\n",
    "\n",
    "# a) Find dimensions, size, and values\n",
    "print(EngAlph.shape, Vowels.size, Friends.values)\n",
    "\n",
    "# b) Rename MTseries as SeriesEmpty\n",
    "SeriesEmpty = MTseries\n",
    "print(SeriesEmpty)\n",
    "\n",
    "# c) Name the index of MonthDays and Friends\n",
    "MonthDays.index.name = 'monthno'\n",
    "Friends.index.name = 'Fname'\n",
    "\n",
    "# d) Display the 3rd and 2nd value of Friends\n",
    "print(Friends.iloc[2], Friends.iloc[1])\n",
    "\n",
    "# e) Display alphabets 'e' to 'p' from EngAlph\n",
    "print(EngAlph['E':'P'])\n",
    "\n",
    "# f) Display the first 10 values of EngAlph\n",
    "print(EngAlph.head(10))\n",
    "\n",
    "# g) Display the last 10 values of EngAlph\n",
    "print(EngAlph.tail(10))\n",
    "\n",
    "# h) Display MTseries\n",
    "print(MTseries)\n",
    "\n",
    "# 8. Using the Series created in Question 5:\n",
    "\n",
    "# a) Display months 3 through 7 from MonthDays\n",
    "print(MonthDays[3:8])\n",
    "\n",
    "# b) Display MonthDays in reverse order\n",
    "print(MonthDays[::-1])\n",
    "\n",
    "# 9. Create the following DataFrame Sales:\n",
    "Sales = pd.DataFrame({\n",
    "    '2014': [100.5, 150.8, 200.9, 30000, 40000],\n",
    "    '2015': [12000, 18000, 22000, 30000, 45000],\n",
    "    '2016': [20000, 50000, 70000, 100000, 125000],\n",
    "    '2017': [50000, 60000, 70000, 80000, 90000]\n",
    "}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\n",
    "print(Sales)\n",
    "\n",
    "# 10. Use the DataFrame Sales to do the following:\n",
    "\n",
    "# a) Display row labels\n",
    "print(Sales.index)\n",
    "\n",
    "# b) Display column labels\n",
    "print(Sales.columns)\n",
    "\n",
    "# c) Display data types of each column\n",
    "print(Sales.dtypes)\n",
    "\n",
    "# d) Display dimensions, shape, size, and values\n",
    "print(Sales.shape, Sales.size, Sales.values)\n",
    "\n",
    "# e) Display the last two rows\n",
    "print(Sales.tail(2))\n",
    "\n",
    "# f) Display the first two columns\n",
    "print(Sales.iloc[:, :2])\n",
    "\n",
    "# g) Create Sales2 DataFrame from dictionary\n",
    "Sales2 = pd.DataFrame({\n",
    "    '2018': [160000, 110000, 500000, 340000, 900000]\n",
    "}, index=['Madhu', 'Kusum', 'Kinshuk', 'Ankit', 'Shruti'])\n",
    "print(Sales2)\n",
    "\n",
    "# h) Check if Sales2 is empty\n",
    "print(Sales2.empty)\n",
    "\n",
    "# 11. Use the DataFrame Sales to do the following:\n",
    "\n",
    "# a) Append Sales2 to Sales\n",
    "Sales = Sales.append(Sales2)\n",
    "print(Sales)\n",
    "\n",
    "# b) Transpose the Sales DataFrame\n",
    "Sales = Sales.T\n",
    "print(Sales)\n",
    "\n",
    "# c) Display sales by all salespersons in 2017\n",
    "print(Sales['2017'])\n",
    "\n",
    "# d) Display sales by Madhu and Ankit in 2017 and 2018\n",
    "print(Sales.loc[['Madhu', 'Ankit'], ['2017', '2018']])\n",
    "\n",
    "# e) Display Shruti's sales in 2016\n",
    "print(Sales.loc['Shruti', '2016'])\n",
    "\n",
    "# f) Add data for Sumeet\n",
    "Sales.loc['Sumeet'] = [196.2, 37800, 52000, 78438, 38852]\n",
    "print(Sales)\n",
    "\n",
    "# g) Delete 2014 data from Sales\n",
    "Sales.drop('2014', axis=1, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# h) Delete data for Kinshuk\n",
    "Sales.drop('Kinshuk', axis=0, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# i) Change salesperson names\n",
    "Sales.rename(index={'Ankit': 'Vivaan', 'Madhu': 'Shailesh'}, inplace=True)\n",
    "print(Sales)\n",
    "\n",
    "# j) Update sales made by Shailesh in 2018\n",
    "Sales.loc['Shailesh', '2018'] = 100000\n",
    "print(Sales)\n",
    "\n",
    "# k) Write Sales DataFrame to a CSV file\n",
    "Sales.to_csv('SalesFigures.csv', header=False, index=False)\n",
    "\n",
    "# l) Read the CSV file and update row/column labels\n",
    "SalesRetrieved = pd.read_csv('SalesFigures.csv', header=None)\n",
    "SalesRetrieved.columns = Sales.columns\n",
    "SalesRetrieved.index = Sales.index\n",
    "print(SalesRetrieved)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36a9393c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:55.369947Z",
     "iopub.status.busy": "2025-08-06T10:23:55.369603Z",
     "iopub.status.idle": "2025-08-06T10:23:55.377919Z",
     "shell.execute_reply": "2025-08-06T10:23:55.377185Z"
    },
    "id": "Mdho8ROkw6f0",
    "papermill": {
     "duration": 0.021121,
     "end_time": "2025-08-06T10:23:55.379618",
     "exception": false,
     "start_time": "2025-08-06T10:23:55.358497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 4. Sort DataFrame by multiple columns\\nimport pandas as pd\\ndf = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\\ndf.sort_values(by=['col1', 'col2'], ascending=[True, False])\\n\\n# 5. Handling missing values\\ndf.fillna(value=0)  # Fill with 0\\ndf.dropna()         # Drop rows with missing values\\n\\n# 6. Median, Standard Deviation, and Variance\\nimport numpy as np\\ndata = [1, 2, 3, 4, 5]\\nmedian = np.median(data)\\nstd_dev = np.std(data)\\nvariance = np.var(data)\\n\\n# 7. Mode\\nmode_value = df['column'].mode()\\n\\n# 8. Data Aggregation\\n# Example of aggregation using groupby\\ndf.groupby('column_name').agg({'other_column': 'mean'})\\n\\n# 9. GROUP BY in SQL\\n# Example using pandas\\ndf.groupby('column_name').agg({'other_column': 'mean'})\\n\\n# 10. Read data from MySQL to DataFrame\\nimport pymysql\\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\\ndf = pd.read_sql('SELECT * FROM table_name', conn)\\nconn.close()\\n\\n# 11. Reshaping data example\\nreshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\\n\\n# 12. Estimation\\n# Helps make informed decisions, calculate confidence intervals, and understand reliability.\\n\\n# 13. Product Table operations\\nimport pandas as pd\\n# a) Create DataFrame\\ndata = {\\n    'Item': ['TV', 'TV', 'TV', 'AC'],\\n    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\\n    'Rupees': [12000, 10000, 15000, 14000],\\n    'USD': [700, 650, 800, 750]\\n}\\ndf = pd.DataFrame(data)\\nprint(df)\\n\\n# b) Add new rows\\nnew_data = pd.DataFrame({\\n    'Item': ['AC', 'TV'],\\n    'Company': ['SAMSUNG', 'SAMSUNG'],\\n    'Rupees': [16000, 12000],\\n    'USD': [850, 700]\\n})\\ndf = df.append(new_data, ignore_index=True)\\nprint(df)\\n\\n# c) Max price of LG TV\\nmax_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\\nprint(max_price_LG)\\n\\n# d) Sum of all products\\ntotal_sum = df['Rupees'].sum()\\nprint(total_sum)\\n\\n# e) Median USD of Sony products\\nmedian_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\\nprint(median_usd_sony)\\n\\n# f) Sort data by Rupees\\ndf_sorted = df.sort_values(by='Rupees')\\nprint(df_sorted)\\n\\n# g) Transfer DataFrame to MySQL\\nconn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\\ndf.to_sql('product_table', conn, if_exists='replace', index=False)\\nconn.close()\\n\\n# 14. Student dataset operations\\n# a) Create DataFrame\\ndata = {\\n    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\\n    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\\n    'Marks': [76, 89, 45, 88]\\n}\\ndf = pd.DataFrame(data)\\n\\n# b) Degree and max marks in each stream\\ndf.groupby('Degree')['Marks'].max()\\n\\n# c) Fill NaN with 76\\ndf.fillna(76, inplace=True)\\n\\n# d) Set index to Name\\ndf.set_index('Name', inplace=True)\\n\\n# e) Name and degree-wise average marks of each student\\ndf.groupby(['Degree']).agg({'Marks': 'mean'})\\n\\n# f) Count number of students in MBA\\nmba_count = df[df['Degree'] == 'MBA'].shape[0]\\nprint(mba_count)\\n\\n# g) Mode of marks in BCA\\nbca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\\nprint(bca_mode)\\n\\n# UCI Dataset (auto-mpg) operations\\n# 1) Load the dataset\\nautodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\\n\\n# 2) Describe the DataFrame\\nprint(autodf.describe())\\n\\n# 3) Display first 10 rows\\nprint(autodf.head(10))\\n\\n# 4) Handle missing values\\nautodf.fillna(method='ffill', inplace=True)\\nautodf.dropna(inplace=True)\\n\\n# 5) Car with max mileage\\nmax_mileage_car = autodf.loc[autodf[0].idxmax()]\\nprint(max_mileage_car)\\n\\n# 6) Average displacement based on cylinders\\navg_displacement = autodf.groupby(1)[3].mean()\\nprint(avg_displacement)\\n\\n# 7) Average number of cylinders\\navg_cylinders = autodf[1].mean()\\nprint(avg_cylinders)\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Install pymysql\n",
    "# pip install pymysql\n",
    "\n",
    "# 2. Pivot vs Pivot_table\n",
    "# pivot():\n",
    "# Used for reshaping data where you specify the index, columns, and values.\n",
    "# It requires unique index/column combinations. If there are duplicates, it throws an error.\n",
    "\n",
    "# pivot_table():\n",
    "# Similar to pivot(), but it can handle duplicate entries. It allows you to specify an aggregation function (e.g., sum, mean) to aggregate data when duplicates exist.\n",
    "# Syntax: pivot_table(index=..., columns=..., values=..., aggfunc='sum')\n",
    "\n",
    "# 3. SQLAlchemy\n",
    "# SQLAlchemy is a Python library that provides an Object Relational Mapping (ORM) interface for interacting with databases.\n",
    "\n",
    "'''\n",
    "# 4. Sort DataFrame by multiple columns\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({'col1': [1, 2, 3], 'col2': [3, 2, 1]})\n",
    "df.sort_values(by=['col1', 'col2'], ascending=[True, False])\n",
    "\n",
    "# 5. Handling missing values\n",
    "df.fillna(value=0)  # Fill with 0\n",
    "df.dropna()         # Drop rows with missing values\n",
    "\n",
    "# 6. Median, Standard Deviation, and Variance\n",
    "import numpy as np\n",
    "data = [1, 2, 3, 4, 5]\n",
    "median = np.median(data)\n",
    "std_dev = np.std(data)\n",
    "variance = np.var(data)\n",
    "\n",
    "# 7. Mode\n",
    "mode_value = df['column'].mode()\n",
    "\n",
    "# 8. Data Aggregation\n",
    "# Example of aggregation using groupby\n",
    "df.groupby('column_name').agg({'other_column': 'mean'})\n",
    "\n",
    "# 9. GROUP BY in SQL\n",
    "# Example using pandas\n",
    "df.groupby('column_name').agg({'other_column': 'mean'})\n",
    "\n",
    "# 10. Read data from MySQL to DataFrame\n",
    "import pymysql\n",
    "conn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\n",
    "df = pd.read_sql('SELECT * FROM table_name', conn)\n",
    "conn.close()\n",
    "\n",
    "# 11. Reshaping data example\n",
    "reshaped_df = pd.melt(df, id_vars=['id'], value_vars=['col1', 'col2'])\n",
    "\n",
    "# 12. Estimation\n",
    "# Helps make informed decisions, calculate confidence intervals, and understand reliability.\n",
    "\n",
    "# 13. Product Table operations\n",
    "import pandas as pd\n",
    "# a) Create DataFrame\n",
    "data = {\n",
    "    'Item': ['TV', 'TV', 'TV', 'AC'],\n",
    "    'Company': ['LG', 'VIDEOCON', 'LG', 'SONY'],\n",
    "    'Rupees': [12000, 10000, 15000, 14000],\n",
    "    'USD': [700, 650, 800, 750]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "# b) Add new rows\n",
    "new_data = pd.DataFrame({\n",
    "    'Item': ['AC', 'TV'],\n",
    "    'Company': ['SAMSUNG', 'SAMSUNG'],\n",
    "    'Rupees': [16000, 12000],\n",
    "    'USD': [850, 700]\n",
    "})\n",
    "df = df.append(new_data, ignore_index=True)\n",
    "print(df)\n",
    "\n",
    "# c) Max price of LG TV\n",
    "max_price_LG = df[(df['Item'] == 'TV') & (df['Company'] == 'LG')]['Rupees'].max()\n",
    "print(max_price_LG)\n",
    "\n",
    "# d) Sum of all products\n",
    "total_sum = df['Rupees'].sum()\n",
    "print(total_sum)\n",
    "\n",
    "# e) Median USD of Sony products\n",
    "median_usd_sony = df[df['Company'] == 'SONY']['USD'].median()\n",
    "print(median_usd_sony)\n",
    "\n",
    "# f) Sort data by Rupees\n",
    "df_sorted = df.sort_values(by='Rupees')\n",
    "print(df_sorted)\n",
    "\n",
    "# g) Transfer DataFrame to MySQL\n",
    "conn = pymysql.connect(host='localhost', user='username', password='password', database='database_name')\n",
    "df.to_sql('product_table', conn, if_exists='replace', index=False)\n",
    "conn.close()\n",
    "\n",
    "# 14. Student dataset operations\n",
    "# a) Create DataFrame\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob', 'Charlie'],\n",
    "    'Degree': ['BCA', 'MBA', 'BCA', 'MBA'],\n",
    "    'Marks': [76, 89, 45, 88]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# b) Degree and max marks in each stream\n",
    "df.groupby('Degree')['Marks'].max()\n",
    "\n",
    "# c) Fill NaN with 76\n",
    "df.fillna(76, inplace=True)\n",
    "\n",
    "# d) Set index to Name\n",
    "df.set_index('Name', inplace=True)\n",
    "\n",
    "# e) Name and degree-wise average marks of each student\n",
    "df.groupby(['Degree']).agg({'Marks': 'mean'})\n",
    "\n",
    "# f) Count number of students in MBA\n",
    "mba_count = df[df['Degree'] == 'MBA'].shape[0]\n",
    "print(mba_count)\n",
    "\n",
    "# g) Mode of marks in BCA\n",
    "bca_mode = df[df['Degree'] == 'BCA']['Marks'].mode()\n",
    "print(bca_mode)\n",
    "\n",
    "# UCI Dataset (auto-mpg) operations\n",
    "# 1) Load the dataset\n",
    "autodf = pd.read_csv('auto-mpg.data', delim_whitespace=True, header=None)\n",
    "\n",
    "# 2) Describe the DataFrame\n",
    "print(autodf.describe())\n",
    "\n",
    "# 3) Display first 10 rows\n",
    "print(autodf.head(10))\n",
    "\n",
    "# 4) Handle missing values\n",
    "autodf.fillna(method='ffill', inplace=True)\n",
    "autodf.dropna(inplace=True)\n",
    "\n",
    "# 5) Car with max mileage\n",
    "max_mileage_car = autodf.loc[autodf[0].idxmax()]\n",
    "print(max_mileage_car)\n",
    "\n",
    "# 6) Average displacement based on cylinders\n",
    "avg_displacement = autodf.groupby(1)[3].mean()\n",
    "print(avg_displacement)\n",
    "\n",
    "# 7) Average number of cylinders\n",
    "avg_cylinders = autodf[1].mean()\n",
    "print(avg_cylinders)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612cb7dd",
   "metadata": {
    "papermill": {
     "duration": 0.00893,
     "end_time": "2025-08-06T10:23:55.397871",
     "exception": false,
     "start_time": "2025-08-06T10:23:55.388941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATASET 1 FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c384eefc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:55.417506Z",
     "iopub.status.busy": "2025-08-06T10:23:55.417097Z",
     "iopub.status.idle": "2025-08-06T10:23:57.615220Z",
     "shell.execute_reply": "2025-08-06T10:23:57.614114Z"
    },
    "papermill": {
     "duration": 2.210101,
     "end_time": "2025-08-06T10:23:57.617135",
     "exception": false,
     "start_time": "2025-08-06T10:23:55.407034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b4489c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:57.639681Z",
     "iopub.status.busy": "2025-08-06T10:23:57.638800Z",
     "iopub.status.idle": "2025-08-06T10:23:57.681807Z",
     "shell.execute_reply": "2025-08-06T10:23:57.680685Z"
    },
    "papermill": {
     "duration": 0.056478,
     "end_time": "2025-08-06T10:23:57.683389",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.626911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Dataset ---\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/student-performance/student/student-mat.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\\n\")\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6047c90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:57.703797Z",
     "iopub.status.busy": "2025-08-06T10:23:57.703473Z",
     "iopub.status.idle": "2025-08-06T10:23:57.726673Z",
     "shell.execute_reply": "2025-08-06T10:23:57.724597Z"
    },
    "papermill": {
     "duration": 0.036306,
     "end_time": "2025-08-06T10:23:57.729359",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.693053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Constant Method: []\n",
      "Dataset shape after Constant Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Constant Method Filtering ---\n",
    "\n",
    "constant_columns = [col for col in df_original.columns if df_original[col].nunique() == 1]\n",
    "df_constant_filtered = df_original.drop(columns=constant_columns)\n",
    "print(f\"Columns removed by Constant Method: {constant_columns}\")\n",
    "print(f\"Dataset shape after Constant Method filtering: {df_constant_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d16e2bf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:57.753133Z",
     "iopub.status.busy": "2025-08-06T10:23:57.752810Z",
     "iopub.status.idle": "2025-08-06T10:23:57.764069Z",
     "shell.execute_reply": "2025-08-06T10:23:57.762815Z"
    },
    "papermill": {
     "duration": 0.024625,
     "end_time": "2025-08-06T10:23:57.765827",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.741202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Quasi-Constant Method (threshold=99.0%): []\n",
      "Dataset shape after Quasi-Constant Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Quasi-Constant Method Filtering ---\n",
    "\n",
    "threshold = 0.99\n",
    "quasi_constant_columns = []\n",
    "for col in df_original.columns:\n",
    "    # Calculate the most frequent value's proportion\n",
    "    most_frequent_proportion = df_original[col].value_counts(normalize=True).max()\n",
    "    if most_frequent_proportion > threshold:\n",
    "        quasi_constant_columns.append(col)\n",
    "\n",
    "df_quasi_constant_filtered = df_original.drop(columns=quasi_constant_columns)\n",
    "print(f\"Columns removed by Quasi-Constant Method (threshold={threshold*100}%): {quasi_constant_columns}\")\n",
    "print(f\"Dataset shape after Quasi-Constant Method filtering: {df_quasi_constant_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45ae7d6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:57.786930Z",
     "iopub.status.busy": "2025-08-06T10:23:57.786602Z",
     "iopub.status.idle": "2025-08-06T10:23:57.798141Z",
     "shell.execute_reply": "2025-08-06T10:23:57.796985Z"
    },
    "papermill": {
     "duration": 0.023736,
     "end_time": "2025-08-06T10:23:57.799701",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.775965",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Correlation Method (threshold=0.9): []\n",
      "Dataset shape after Correlation Method filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Correlation Filtering ---\n",
    "\n",
    "# Select only numerical columns for correlation calculation\n",
    "numerical_df = df_original.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than a threshold (e.g., 0.9)\n",
    "correlation_threshold = 0.9\n",
    "to_drop_high_corr = [column for column in upper_tri.columns if any(upper_tri[column] > correlation_threshold)]\n",
    "\n",
    "# Create a new DataFrame with non-numerical columns and the filtered numerical columns\n",
    "df_correlation_filtered = df_original.drop(columns=to_drop_high_corr)\n",
    "print(f\"Columns removed by Correlation Method (threshold={correlation_threshold}): {to_drop_high_corr}\")\n",
    "print(f\"Dataset shape after Correlation Method filtering: {df_correlation_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd4841e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:57.820715Z",
     "iopub.status.busy": "2025-08-06T10:23:57.820048Z",
     "iopub.status.idle": "2025-08-06T10:23:57.829607Z",
     "shell.execute_reply": "2025-08-06T10:23:57.828483Z"
    },
    "papermill": {
     "duration": 0.021662,
     "end_time": "2025-08-06T10:23:57.831067",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.809405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target column 'G3' not found. Skipping Mutual Information filtering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Mutual Information Filtering ---\n",
    "\n",
    "# For demonstration, let's assume 'G3' (final grade) is the target variable\n",
    "# If 'G3' is not present, or if it's a regression task, adjust accordingly.\n",
    "# For classification, use mutual_info_classif; for regression, use mutual_info_regression.\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = df_original.select_dtypes(include=np.number).columns.tolist()\n",
    "categorical_cols = df_original.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "# The 'G3' column is typically numerical in this dataset, so it can be a target for regression.\n",
    "# If you want to treat it as classification (e.g., pass/fail), you might need to discretize it.\n",
    "target_column = 'G3'\n",
    "\n",
    "if target_column in df_original.columns:\n",
    "    X = df_original.drop(columns=[target_column])\n",
    "    y = df_original[target_column]\n",
    "\n",
    "    # Encode categorical features for mutual information calculation\n",
    "    X_encoded = X.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in X_encoded.columns:\n",
    "            le = LabelEncoder()\n",
    "            X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "\n",
    "    # Calculate mutual information. Use mutual_info_regression for numerical target.\n",
    "    # If your target is categorical, use mutual_info_classif.\n",
    "    mi_scores = mutual_info_regression(X_encoded, y)\n",
    "    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n",
    "    mi_scores = mi_scores.sort_values(ascending=False)\n",
    "\n",
    "    print(\"Mutual Information Scores:\")\n",
    "    print(mi_scores)\n",
    "\n",
    "    # You can set a threshold to select features, e.g., keep features with MI score > 0.05\n",
    "    mi_threshold = 0.05\n",
    "    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n",
    "    df_mi_filtered = df_original[selected_features_mi + [target_column]] # Keep target column\n",
    "    print(f\"\\nFeatures selected by Mutual Information (threshold={mi_threshold}): {selected_features_mi}\")\n",
    "    print(f\"Dataset shape after Mutual Information filtering: {df_mi_filtered.shape}\\n\")\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' not found. Skipping Mutual Information filtering.\\n\")\n",
    "    df_mi_filtered = df_original.copy() # No filtering applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e378a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:57.921820Z",
     "iopub.status.busy": "2025-08-06T10:23:57.921523Z",
     "iopub.status.idle": "2025-08-06T10:23:57.967629Z",
     "shell.execute_reply": "2025-08-06T10:23:57.966449Z"
    },
    "papermill": {
     "duration": 0.060787,
     "end_time": "2025-08-06T10:23:57.969389",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.908602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 395\n",
      "Number of rows after removing duplicate rows: 395\n",
      "Rows removed: 0\n",
      "\n",
      "Columns removed by Duplicate Columns Method: []\n",
      "Dataset shape after Duplicate Columns filtering: (395, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Duplicate Methods Filtering ---\n",
    "\n",
    "df_no_duplicate_rows = df_original.drop_duplicates()\n",
    "print(f\"Original number of rows: {df_original.shape[0]}\")\n",
    "print(f\"Number of rows after removing duplicate rows: {df_no_duplicate_rows.shape[0]}\")\n",
    "print(f\"Rows removed: {df_original.shape[0] - df_no_duplicate_rows.shape[0]}\\n\")\n",
    "\n",
    "# Remove duplicate columns\n",
    "# Transpose the DataFrame to treat columns as rows, then drop duplicates, then transpose back.\n",
    "df_transposed = df_original.T\n",
    "df_no_duplicate_cols_transposed = df_transposed.drop_duplicates()\n",
    "df_duplicate_filtered = df_no_duplicate_cols_transposed.T\n",
    "\n",
    "# Identify columns that were duplicates and removed\n",
    "original_cols = set(df_original.columns)\n",
    "filtered_cols = set(df_duplicate_filtered.columns)\n",
    "removed_duplicate_columns = list(original_cols - filtered_cols)\n",
    "\n",
    "print(f\"Columns removed by Duplicate Columns Method: {removed_duplicate_columns}\")\n",
    "print(f\"Dataset shape after Duplicate Columns filtering: {df_duplicate_filtered.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d5be06",
   "metadata": {
    "papermill": {
     "duration": 0.010524,
     "end_time": "2025-08-06T10:23:57.989827",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.979303",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **DATASET 2 FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0706bad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.011614Z",
     "iopub.status.busy": "2025-08-06T10:23:58.011287Z",
     "iopub.status.idle": "2025-08-06T10:23:58.016113Z",
     "shell.execute_reply": "2025-08-06T10:23:58.015145Z"
    },
    "papermill": {
     "duration": 0.018224,
     "end_time": "2025-08-06T10:23:58.017948",
     "exception": false,
     "start_time": "2025-08-06T10:23:57.999724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbc20387",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.039278Z",
     "iopub.status.busy": "2025-08-06T10:23:58.038910Z",
     "iopub.status.idle": "2025-08-06T10:23:58.075679Z",
     "shell.execute_reply": "2025-08-06T10:23:58.074613Z"
    },
    "papermill": {
     "duration": 0.049792,
     "end_time": "2025-08-06T10:23:58.077750",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.027958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (477, 198)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Dataset ---\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/drug-induced-autoimmunity-prediction/DIA_trainingset_RDKit_descriptors.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\\n\")\n",
    "\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab8f6c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.100460Z",
     "iopub.status.busy": "2025-08-06T10:23:58.100003Z",
     "iopub.status.idle": "2025-08-06T10:23:58.126653Z",
     "shell.execute_reply": "2025-08-06T10:23:58.125697Z"
    },
    "papermill": {
     "duration": 0.039502,
     "end_time": "2025-08-06T10:23:58.128356",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.088854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape after Constant Method filtering: (477, 181)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Constant Method Filtering ---\n",
    "\n",
    "constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_columns)\n",
    "print(f\"Dataset shape after Constant Method filtering: {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d784834",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.149885Z",
     "iopub.status.busy": "2025-08-06T10:23:58.149570Z",
     "iopub.status.idle": "2025-08-06T10:23:58.231813Z",
     "shell.execute_reply": "2025-08-06T10:23:58.230767Z"
    },
    "papermill": {
     "duration": 0.094667,
     "end_time": "2025-08-06T10:23:58.233628",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.138961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Quasi-Constant Method (>99.0% same value): ['fr_C_S', 'fr_aldehyde', 'fr_amidine', 'fr_azo', 'fr_hdrzone', 'fr_nitro_arom_nonortho', 'fr_nitroso', 'fr_oxazole', 'fr_term_acetylene', 'fr_tetrazole']\n",
      "Dataset shape after Quasi-Constant Method filtering: (477, 171)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Quasi-Constant Method Filtering ---\n",
    "\n",
    "threshold = 0.99\n",
    "quasi_constant_columns = [col for col in df.columns \n",
    "                          if df[col].value_counts(normalize=True).max() > threshold]\n",
    "df = df.drop(columns=quasi_constant_columns)\n",
    "print(f\"Columns removed by Quasi-Constant Method (>{threshold*100}% same value): {quasi_constant_columns}\")\n",
    "print(f\"Dataset shape after Quasi-Constant Method filtering: {df.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a08abc3a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.254917Z",
     "iopub.status.busy": "2025-08-06T10:23:58.254611Z",
     "iopub.status.idle": "2025-08-06T10:23:58.353519Z",
     "shell.execute_reply": "2025-08-06T10:23:58.352353Z"
    },
    "papermill": {
     "duration": 0.111731,
     "end_time": "2025-08-06T10:23:58.355277",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.243546",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns removed by Correlation Method (>0.9): ['Chi0n', 'Chi0v', 'Chi1', 'Chi1n', 'Chi1v', 'Chi2n', 'Chi2v', 'Chi3n', 'Chi3v', 'Chi4n', 'Chi4v', 'ExactMolWt', 'HeavyAtomCount', 'HeavyAtomMolWt', 'Kappa1', 'Kappa2', 'LabuteASA', 'MaxEStateIndex', 'MinAbsPartialCharge', 'MinPartialCharge', 'MolMR', 'MolWt', 'NumHAcceptors', 'NumHDonors', 'NumHeteroatoms', 'NumSaturatedCarbocycles', 'NumSaturatedRings', 'NumValenceElectrons', 'SlogP_VSA6', 'TPSA', 'fr_Al_OH_noTert', 'fr_COO', 'fr_COO2', 'fr_C_O_noCOO', 'fr_Nhpyrrole', 'fr_benzene', 'fr_nitrile', 'fr_phenol', 'fr_phenol_noOrthoHbond']\n",
      "Dataset shape after Correlation Method filtering: (477, 132)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Correlation Filtering ---\n",
    "\n",
    "numerical_df = df.select_dtypes(include=np.number)\n",
    "correlation_matrix = numerical_df.corr().abs()\n",
    "upper_tri = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "correlation_threshold = 0.9\n",
    "to_drop_high_corr = [\n",
    "    column for column in upper_tri.columns\n",
    "    if any(upper_tri[column].fillna(0) > correlation_threshold)  # Avoid NaN warning\n",
    "]\n",
    "df = df.drop(columns=to_drop_high_corr)\n",
    "\n",
    "print(f\"Columns removed by Correlation Method (>{correlation_threshold}): {to_drop_high_corr}\")\n",
    "print(f\"Dataset shape after Correlation Method filtering: {df.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1645077f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.377777Z",
     "iopub.status.busy": "2025-08-06T10:23:58.377462Z",
     "iopub.status.idle": "2025-08-06T10:23:58.795108Z",
     "shell.execute_reply": "2025-08-06T10:23:58.793892Z"
    },
    "papermill": {
     "duration": 0.431229,
     "end_time": "2025-08-06T10:23:58.797091",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.365862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores:\n",
      "SMR_VSA10            0.102580\n",
      "MaxPartialCharge     0.085374\n",
      "PEOE_VSA7            0.080258\n",
      "SMR_VSA6             0.076873\n",
      "PEOE_VSA9            0.071520\n",
      "                       ...   \n",
      "fr_ketone            0.000000\n",
      "fr_ketone_Topliss    0.000000\n",
      "SlogP_VSA7           0.000000\n",
      "fr_lactone           0.000000\n",
      "SlogP_VSA4           0.000000\n",
      "Name: MI Scores, Length: 131, dtype: float64\n",
      "\n",
      "Features selected by Mutual Information (>0.05): ['SMR_VSA10', 'MaxPartialCharge', 'PEOE_VSA7', 'SMR_VSA6', 'PEOE_VSA9', 'fr_bicyclic', 'SMR_VSA1', 'EState_VSA8']\n",
      "Dataset shape after Mutual Information filtering: (477, 9)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Mutual Information Filtering ---\n",
    "\n",
    "# Replace 'Target' with your actual target column name (e.g., 'label', 'class', etc.)\n",
    "target_column = 'Label'  # UPDATE THIS LINE if your column has a different name\n",
    "\n",
    "if target_column in df.columns:\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    categorical_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "    X_encoded = X.copy()\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "\n",
    "    # Use classification or regression based on data type of target\n",
    "    if y.nunique() <= 10 and y.dtype in ['int', 'object']:  # assume classification\n",
    "        from sklearn.feature_selection import mutual_info_classif\n",
    "        mi_scores = mutual_info_classif(X_encoded, y, discrete_features='auto')\n",
    "    else:  # assume regression\n",
    "        mi_scores = mutual_info_regression(X_encoded, y)\n",
    "\n",
    "    mi_scores = pd.Series(mi_scores, index=X.columns, name=\"MI Scores\").sort_values(ascending=False)\n",
    "    print(\"Mutual Information Scores:\")\n",
    "    print(mi_scores)\n",
    "\n",
    "    mi_threshold = 0.05\n",
    "    selected_features_mi = mi_scores[mi_scores > mi_threshold].index.tolist()\n",
    "    df = df[selected_features_mi + [target_column]]\n",
    "\n",
    "    print(f\"\\nFeatures selected by Mutual Information (>{mi_threshold}): {selected_features_mi}\")\n",
    "    print(f\"Dataset shape after Mutual Information filtering: {df.shape}\\n\")\n",
    "else:\n",
    "    print(f\"Target column '{target_column}' not found. Skipping MI filtering.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2d4c9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.818943Z",
     "iopub.status.busy": "2025-08-06T10:23:58.818587Z",
     "iopub.status.idle": "2025-08-06T10:23:58.864401Z",
     "shell.execute_reply": "2025-08-06T10:23:58.862907Z"
    },
    "papermill": {
     "duration": 0.058559,
     "end_time": "2025-08-06T10:23:58.865979",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.807420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 6 duplicate rows.\n",
      "Removed 0 duplicate columns.\n",
      "Final dataset shape: (471, 9)\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Duplicate Filtering ---\n",
    "\n",
    "rows_before = df.shape[0]\n",
    "df = df.drop_duplicates()\n",
    "rows_after = df.shape[0]\n",
    "\n",
    "print(f\"Removed {rows_before - rows_after} duplicate rows.\")\n",
    "\n",
    "cols_before = df.shape[1]\n",
    "df = df.T.drop_duplicates().T\n",
    "cols_after = df.shape[1]\n",
    "\n",
    "print(f\"Removed {cols_before - cols_after} duplicate columns.\")\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa31b65",
   "metadata": {
    "papermill": {
     "duration": 0.009766,
     "end_time": "2025-08-06T10:23:58.885929",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.876163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Titanic Dataset Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08064b8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.907917Z",
     "iopub.status.busy": "2025-08-06T10:23:58.907042Z",
     "iopub.status.idle": "2025-08-06T10:23:58.911968Z",
     "shell.execute_reply": "2025-08-06T10:23:58.910828Z"
    },
    "papermill": {
     "duration": 0.018637,
     "end_time": "2025-08-06T10:23:58.914453",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.895816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4617a045",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.937094Z",
     "iopub.status.busy": "2025-08-06T10:23:58.936810Z",
     "iopub.status.idle": "2025-08-06T10:23:58.971564Z",
     "shell.execute_reply": "2025-08-06T10:23:58.970060Z"
    },
    "papermill": {
     "duration": 0.047461,
     "end_time": "2025-08-06T10:23:58.973231",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.925770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (183, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 183 entries, 1 to 889\n",
      "Data columns (total 12 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   PassengerId  183 non-null    int64  \n",
      " 1   Survived     183 non-null    int64  \n",
      " 2   Pclass       183 non-null    int64  \n",
      " 3   Name         183 non-null    int64  \n",
      " 4   Sex          183 non-null    int64  \n",
      " 5   Age          183 non-null    float64\n",
      " 6   SibSp        183 non-null    int64  \n",
      " 7   Parch        183 non-null    int64  \n",
      " 8   Ticket       183 non-null    int64  \n",
      " 9   Fare         183 non-null    float64\n",
      " 10  Cabin        183 non-null    int64  \n",
      " 11  Embarked     183 non-null    int64  \n",
      "dtypes: float64(2), int64(10)\n",
      "memory usage: 18.6 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('/kaggle/input/titanic/titanic_train.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "# Encode categorical\n",
    "for col in df.select_dtypes(include='object').columns:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col])\n",
    "\n",
    "print(\"Initial shape:\", df.shape)\n",
    "\n",
    "target = 'Survived'\n",
    "print(df.info())  # Shows column names, data types, and non-null counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "245ecb8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:58.995296Z",
     "iopub.status.busy": "2025-08-06T10:23:58.994950Z",
     "iopub.status.idle": "2025-08-06T10:23:59.004488Z",
     "shell.execute_reply": "2025-08-06T10:23:59.003039Z"
    },
    "papermill": {
     "duration": 0.022783,
     "end_time": "2025-08-06T10:23:59.006539",
     "exception": false,
     "start_time": "2025-08-06T10:23:58.983756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing constant features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 1. Constant Features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_features)\n",
    "print(\"After removing constant features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8017525e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.030332Z",
     "iopub.status.busy": "2025-08-06T10:23:59.029945Z",
     "iopub.status.idle": "2025-08-06T10:23:59.044709Z",
     "shell.execute_reply": "2025-08-06T10:23:59.043607Z"
    },
    "papermill": {
     "duration": 0.028455,
     "end_time": "2025-08-06T10:23:59.046758",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.018303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing quasi-constant features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 2. Quasi-Constant Features\n",
    "quasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\n",
    "df = df.drop(columns=quasi_constant)\n",
    "print(\"After removing quasi-constant features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e19e8e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.068986Z",
     "iopub.status.busy": "2025-08-06T10:23:59.068647Z",
     "iopub.status.idle": "2025-08-06T10:23:59.099036Z",
     "shell.execute_reply": "2025-08-06T10:23:59.097797Z"
    },
    "papermill": {
     "duration": 0.043541,
     "end_time": "2025-08-06T10:23:59.100776",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.057235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing duplicate features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 3. Duplicate Features\n",
    "df = df.T.drop_duplicates().T\n",
    "print(\"After removing duplicate features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05430dd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.123288Z",
     "iopub.status.busy": "2025-08-06T10:23:59.122949Z",
     "iopub.status.idle": "2025-08-06T10:23:59.135236Z",
     "shell.execute_reply": "2025-08-06T10:23:59.134112Z"
    },
    "papermill": {
     "duration": 0.02518,
     "end_time": "2025-08-06T10:23:59.136882",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.111702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing highly correlated features: (183, 12)\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(\"After removing highly correlated features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a9067f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.160042Z",
     "iopub.status.busy": "2025-08-06T10:23:59.159707Z",
     "iopub.status.idle": "2025-08-06T10:23:59.196525Z",
     "shell.execute_reply": "2025-08-06T10:23:59.195333Z"
    },
    "papermill": {
     "duration": 0.050652,
     "end_time": "2025-08-06T10:23:59.198632",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.147980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top MI Features:\n",
      " Sex            0.109410\n",
      "Age            0.094564\n",
      "Fare           0.044378\n",
      "Ticket         0.043420\n",
      "SibSp          0.014893\n",
      "Parch          0.007853\n",
      "PassengerId    0.000000\n",
      "Pclass         0.000000\n",
      "Name           0.000000\n",
      "Cabin          0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 5. Mutual Information (Classification)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "print(\"\\nTop MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0116159",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.222983Z",
     "iopub.status.busy": "2025-08-06T10:23:59.222646Z",
     "iopub.status.idle": "2025-08-06T10:23:59.264320Z",
     "shell.execute_reply": "2025-08-06T10:23:59.263000Z"
    },
    "papermill": {
     "duration": 0.056091,
     "end_time": "2025-08-06T10:23:59.266008",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.209917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Chi2 Features:\n",
      " Sex            24.945263\n",
      "Age             1.046261\n",
      "Name            0.690274\n",
      "Embarked        0.664543\n",
      "SibSp           0.612932\n",
      "PassengerId     0.608432\n",
      "Fare            0.474251\n",
      "Pclass          0.150679\n",
      "Parch           0.030309\n",
      "Ticket          0.015566\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Chi-Square\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi2_scores = chi2(X_chi, y)[0]\n",
    "print(\"\\nTop Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3490261e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.288816Z",
     "iopub.status.busy": "2025-08-06T10:23:59.288491Z",
     "iopub.status.idle": "2025-08-06T10:23:59.298777Z",
     "shell.execute_reply": "2025-08-06T10:23:59.297593Z"
    },
    "papermill": {
     "duration": 0.023815,
     "end_time": "2025-08-06T10:23:59.300419",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.276604",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top ANOVA Features:\n",
      " Sex            71.605923\n",
      "Age            12.491639\n",
      "Name            4.144636\n",
      "PassengerId     4.081191\n",
      "Fare            3.321597\n",
      "SibSp           2.070439\n",
      "Embarked        1.863299\n",
      "Pclass          0.216220\n",
      "Parch           0.100716\n",
      "Ticket          0.093873\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. ANOVA (Classification)\n",
    "anova = f_classif(X, y)\n",
    "print(\"\\nTop ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee16d55",
   "metadata": {
    "papermill": {
     "duration": 0.009942,
     "end_time": "2025-08-06T10:23:59.321435",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.311493",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **House Price Dataset Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b43c3633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.344478Z",
     "iopub.status.busy": "2025-08-06T10:23:59.344104Z",
     "iopub.status.idle": "2025-08-06T10:23:59.349123Z",
     "shell.execute_reply": "2025-08-06T10:23:59.348140Z"
    },
    "papermill": {
     "duration": 0.018362,
     "end_time": "2025-08-06T10:23:59.350849",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.332487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import mutual_info_regression, SelectKBest, f_regression, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e62379df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.375888Z",
     "iopub.status.busy": "2025-08-06T10:23:59.375591Z",
     "iopub.status.idle": "2025-08-06T10:23:59.421392Z",
     "shell.execute_reply": "2025-08-06T10:23:59.420059Z"
    },
    "papermill": {
     "duration": 0.060597,
     "end_time": "2025-08-06T10:23:59.423569",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.362972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing features: (1460, 35)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/house-price-prediction/house_price_train.csv')\n",
    "\n",
    "df = df.dropna(axis=1, how='any')\n",
    "\n",
    "df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "target = 'SalePrice' if 'SalePrice' in df.columns else df.columns[-1]  \n",
    "print(\"Before removing features:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9c20352",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.448354Z",
     "iopub.status.busy": "2025-08-06T10:23:59.447977Z",
     "iopub.status.idle": "2025-08-06T10:23:59.459608Z",
     "shell.execute_reply": "2025-08-06T10:23:59.458659Z"
    },
    "papermill": {
     "duration": 0.024755,
     "end_time": "2025-08-06T10:23:59.461128",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.436373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 35)\n"
     ]
    }
   ],
   "source": [
    "# 1. Constant Features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_features)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20bb2360",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.484080Z",
     "iopub.status.busy": "2025-08-06T10:23:59.483787Z",
     "iopub.status.idle": "2025-08-06T10:23:59.507634Z",
     "shell.execute_reply": "2025-08-06T10:23:59.506304Z"
    },
    "papermill": {
     "duration": 0.037264,
     "end_time": "2025-08-06T10:23:59.509345",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.472081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 32)\n"
     ]
    }
   ],
   "source": [
    "# 2. Quasi-Constant Features\n",
    "quasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\n",
    "df = df.drop(columns=quasi_constant)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1bd02467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.534499Z",
     "iopub.status.busy": "2025-08-06T10:23:59.534093Z",
     "iopub.status.idle": "2025-08-06T10:23:59.690479Z",
     "shell.execute_reply": "2025-08-06T10:23:59.689071Z"
    },
    "papermill": {
     "duration": 0.171565,
     "end_time": "2025-08-06T10:23:59.692519",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.520954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 32)\n"
     ]
    }
   ],
   "source": [
    "# 3. Duplicate Features\n",
    "df = df.T.drop_duplicates().T\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "903c5f4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.720581Z",
     "iopub.status.busy": "2025-08-06T10:23:59.720240Z",
     "iopub.status.idle": "2025-08-06T10:23:59.743744Z",
     "shell.execute_reply": "2025-08-06T10:23:59.742595Z"
    },
    "papermill": {
     "duration": 0.041925,
     "end_time": "2025-08-06T10:23:59.746604",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.704679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 32)\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d8d769c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:23:59.776118Z",
     "iopub.status.busy": "2025-08-06T10:23:59.775726Z",
     "iopub.status.idle": "2025-08-06T10:24:00.125565Z",
     "shell.execute_reply": "2025-08-06T10:24:00.123225Z"
    },
    "papermill": {
     "duration": 0.366234,
     "end_time": "2025-08-06T10:24:00.127626",
     "exception": false,
     "start_time": "2025-08-06T10:23:59.761392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top MI Features:\n",
      " OverallQual     0.505835\n",
      "GrLivArea       0.361999\n",
      "GarageCars      0.352018\n",
      "GarageArea      0.276845\n",
      "FullBath        0.263321\n",
      "YearBuilt       0.260867\n",
      "TotalBsmtSF     0.253215\n",
      "1stFlrSF        0.227876\n",
      "YearRemodAdd    0.208114\n",
      "TotRmsAbvGrd    0.169534\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Set target column\n",
    "target = 'SalePrice' if 'SalePrice' in df.columns else df.columns[-1]\n",
    "\n",
    "# Make sure target is still in the dataframe after cleaning\n",
    "if target not in df.columns:\n",
    "    print(f\"Target column '{target}' not found in DataFrame after preprocessing.\")\n",
    "    print(\"Available columns:\", df.columns.tolist())\n",
    "else:\n",
    "    X = df.drop(columns=[target])\n",
    "    y = df[target]\n",
    "\n",
    "    # Mutual Information\n",
    "    from sklearn.feature_selection import mutual_info_regression, f_regression\n",
    "    mi_scores = mutual_info_regression(X, y)\n",
    "    print(\"Top MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d8ae5d15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:00.151380Z",
     "iopub.status.busy": "2025-08-06T10:24:00.151021Z",
     "iopub.status.idle": "2025-08-06T10:24:00.182376Z",
     "shell.execute_reply": "2025-08-06T10:24:00.181291Z"
    },
    "papermill": {
     "duration": 0.045303,
     "end_time": "2025-08-06T10:24:00.184192",
     "exception": false,
     "start_time": "2025-08-06T10:24:00.138889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Chi2 Features:\n",
      " MiscVal          403.440783\n",
      "BsmtHalfBath     298.612323\n",
      "ScreenPorch      284.644720\n",
      "HalfBath         260.352308\n",
      "BsmtFinSF2       250.259651\n",
      "2ndFlrSF         225.125456\n",
      "Fireplaces       190.169091\n",
      "YearRemodAdd     179.213938\n",
      "EnclosedPorch    179.142328\n",
      "MSSubClass       174.897301\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Chi-Square (scaled)\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi2_scores = chi2(X_chi, y)[0]\n",
    "print(\"Top Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c67a631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:00.211979Z",
     "iopub.status.busy": "2025-08-06T10:24:00.211646Z",
     "iopub.status.idle": "2025-08-06T10:24:00.228379Z",
     "shell.execute_reply": "2025-08-06T10:24:00.227171Z"
    },
    "papermill": {
     "duration": 0.031908,
     "end_time": "2025-08-06T10:24:00.230590",
     "exception": false,
     "start_time": "2025-08-06T10:24:00.198682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ANOVA Features:\n",
      " OverallQual     2436.770591\n",
      "GrLivArea       1470.585010\n",
      "GarageCars      1013.705666\n",
      "GarageArea       926.951287\n",
      "TotalBsmtSF      880.341282\n",
      "1stFlrSF         845.524488\n",
      "FullBath         668.430296\n",
      "TotRmsAbvGrd     580.762801\n",
      "YearBuilt        548.665821\n",
      "YearRemodAdd     504.714855\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. ANOVA (Regression)\n",
    "anova = f_regression(X, y)\n",
    "print(\"Top ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58795a8",
   "metadata": {
    "papermill": {
     "duration": 0.011278,
     "end_time": "2025-08-06T10:24:00.254174",
     "exception": false,
     "start_time": "2025-08-06T10:24:00.242896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# **Santander Customer Satisfaction Dataset Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6ad8b74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:00.279537Z",
     "iopub.status.busy": "2025-08-06T10:24:00.279144Z",
     "iopub.status.idle": "2025-08-06T10:24:03.262579Z",
     "shell.execute_reply": "2025-08-06T10:24:03.261606Z"
    },
    "papermill": {
     "duration": 2.997818,
     "end_time": "2025-08-06T10:24:03.264374",
     "exception": false,
     "start_time": "2025-08-06T10:24:00.266556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, f_classif\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/kaggle/input/santander-customer-satisfaction-prediction/Santander Customer Satisfaction_train.csv')\n",
    "df = df.dropna()\n",
    "target = 'TARGET' if 'TARGET' in df.columns else df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30988171",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:03.287831Z",
     "iopub.status.busy": "2025-08-06T10:24:03.287518Z",
     "iopub.status.idle": "2025-08-06T10:24:03.602443Z",
     "shell.execute_reply": "2025-08-06T10:24:03.601293Z"
    },
    "papermill": {
     "duration": 0.328501,
     "end_time": "2025-08-06T10:24:03.604167",
     "exception": false,
     "start_time": "2025-08-06T10:24:03.275666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 337)\n"
     ]
    }
   ],
   "source": [
    "# 1. Constant Features\n",
    "constant_features = [col for col in df.columns if df[col].nunique() == 1]\n",
    "df = df.drop(columns=constant_features)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb70e520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:03.628589Z",
     "iopub.status.busy": "2025-08-06T10:24:03.628291Z",
     "iopub.status.idle": "2025-08-06T10:24:04.033829Z",
     "shell.execute_reply": "2025-08-06T10:24:04.032556Z"
    },
    "papermill": {
     "duration": 0.420642,
     "end_time": "2025-08-06T10:24:04.036008",
     "exception": false,
     "start_time": "2025-08-06T10:24:03.615366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 138)\n"
     ]
    }
   ],
   "source": [
    "# 2. Quasi-Constant Features\n",
    "quasi_constant = [col for col in df.columns if df[col].value_counts(normalize=True).max() > 0.98]\n",
    "df = df.drop(columns=quasi_constant)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd10671b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:04.072559Z",
     "iopub.status.busy": "2025-08-06T10:24:04.071998Z",
     "iopub.status.idle": "2025-08-06T10:24:17.844132Z",
     "shell.execute_reply": "2025-08-06T10:24:17.842933Z"
    },
    "papermill": {
     "duration": 13.791797,
     "end_time": "2025-08-06T10:24:17.845849",
     "exception": false,
     "start_time": "2025-08-06T10:24:04.054052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 132)\n"
     ]
    }
   ],
   "source": [
    "# 3. Duplicate Features\n",
    "df = df.T.drop_duplicates().T\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "614943ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:17.870076Z",
     "iopub.status.busy": "2025-08-06T10:24:17.869755Z",
     "iopub.status.idle": "2025-08-06T10:24:21.573478Z",
     "shell.execute_reply": "2025-08-06T10:24:21.572042Z"
    },
    "papermill": {
     "duration": 3.717937,
     "end_time": "2025-08-06T10:24:21.575516",
     "exception": false,
     "start_time": "2025-08-06T10:24:17.857579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76020, 70)\n"
     ]
    }
   ],
   "source": [
    "# 4. Correlation\n",
    "corr_matrix = df.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop_corr = [column for column in upper_tri.columns if any(upper_tri[column].fillna(0) > 0.9)]\n",
    "df = df.drop(columns=to_drop_corr)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9f8e69e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:21.598912Z",
     "iopub.status.busy": "2025-08-06T10:24:21.598627Z",
     "iopub.status.idle": "2025-08-06T10:24:51.834237Z",
     "shell.execute_reply": "2025-08-06T10:24:51.833136Z"
    },
    "papermill": {
     "duration": 30.259337,
     "end_time": "2025-08-06T10:24:51.846056",
     "exception": false,
     "start_time": "2025-08-06T10:24:21.586719",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top MI Features:\n",
      " ind_var30      0.024943\n",
      "ind_var5       0.024024\n",
      "num_var42      0.023631\n",
      "ind_var5_0     0.017282\n",
      "num_var30      0.016731\n",
      "num_var30_0    0.015891\n",
      "num_var4       0.015562\n",
      "ind_var39_0    0.014605\n",
      "var36          0.014425\n",
      "saldo_var30    0.014125\n",
      "dtype: float64\n",
      "(76020, 70)\n"
     ]
    }
   ],
   "source": [
    "# 5. Mutual Information (Classification)\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "mi_scores = mutual_info_classif(X, y)\n",
    "print(\"Top MI Features:\\n\", pd.Series(mi_scores, index=X.columns).sort_values(ascending=False).head(10))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "475fd75d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:51.874162Z",
     "iopub.status.busy": "2025-08-06T10:24:51.873363Z",
     "iopub.status.idle": "2025-08-06T10:24:51.984410Z",
     "shell.execute_reply": "2025-08-06T10:24:51.983400Z"
    },
    "papermill": {
     "duration": 0.127078,
     "end_time": "2025-08-06T10:24:51.986053",
     "exception": false,
     "start_time": "2025-08-06T10:24:51.858975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Chi2 Features:\n",
      " ind_var5       468.257507\n",
      "ind_var30      455.821225\n",
      "var36          451.071536\n",
      "ind_var8_0     160.110889\n",
      "ind_var13_0    112.246032\n",
      "ind_var12_0    103.522051\n",
      "ind_var12       85.145416\n",
      "num_var42       78.642021\n",
      "ind_var24_0     66.836311\n",
      "num_var4        53.527151\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 6. Chi-Square\n",
    "X_chi = MinMaxScaler().fit_transform(X)\n",
    "chi2_scores = chi2(X_chi, y)[0]\n",
    "print(\"Top Chi2 Features:\\n\", pd.Series(chi2_scores, index=X.columns).sort_values(ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a6af3115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T10:24:52.010885Z",
     "iopub.status.busy": "2025-08-06T10:24:52.010502Z",
     "iopub.status.idle": "2025-08-06T10:24:52.106435Z",
     "shell.execute_reply": "2025-08-06T10:24:52.105339Z"
    },
    "papermill": {
     "duration": 0.110608,
     "end_time": "2025-08-06T10:24:52.108282",
     "exception": false,
     "start_time": "2025-08-06T10:24:51.997674",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top ANOVA Features:\n",
      " ind_var30      1745.255659\n",
      "num_var30      1482.100843\n",
      "num_var42      1425.940751\n",
      "ind_var5       1418.577470\n",
      "var36           813.832521\n",
      "var15           788.508493\n",
      "num_var4        492.037964\n",
      "ind_var8_0      165.903276\n",
      "ind_var13_0     118.615826\n",
      "ind_var12_0     111.177629\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 7. ANOVA (Classification)\n",
    "anova = f_classif(X, y)\n",
    "print(\"Top ANOVA Features:\\n\", pd.Series(anova[0], index=X.columns).sort_values(ascending=False).head(10))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN0Q7oDHoKUvBtfynUFNzI3",
   "include_colab_link": true,
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 7973641,
     "sourceId": 12620520,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8014077,
     "sourceId": 12681452,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8018708,
     "sourceId": 12688949,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8019025,
     "sourceId": 12689371,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8019036,
     "sourceId": 12689384,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 65.808767,
   "end_time": "2025-08-06T10:24:52.942827",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-06T10:23:47.134060",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
